{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blogs/","title":"Blogs","text":"<p>Welcome to our blog page! Here, you'll find a collection of informative and insightful articles covering various topics related to the Microsoft technology stack. Our blog posts aim to provide valuable knowledge, tutorials, and best practices to help you navigate the world of Microsoft technologies.</p>"},{"location":"blogs/#latest-blog-posts","title":"Latest Blog Posts","text":"<p>Below are our most recent blog posts:</p> <p>Stay tuned for regular updates as we continue to share valuable content to help you stay at the forefront of Microsoft technologies!</p>"},{"location":"contact/","title":"Contact Us","text":"<p>Thank you for your interest in reaching out! If you have any questions, suggestions, or simply want to connect, we'd love to hear from you. Please feel free to use any of the following methods to get in touch:</p> <ul> <li>Email: anjkeesari@gmail.com</li> <li>Phone: +1 707-742-9606</li> <li>Website: https://anjikeesari.com</li> </ul>"},{"location":"about/about/","title":"About Me","text":"<p>Anji Keesari is a accomplished Cloud Architect with over 20 years of experience in software architecture, full-stack development, and Microsoft Azure Cloud solutions. Skilled at designing and delivering scalable, secure, and cloud-native applications, with expertise in .NET, Azure, Kubernetes, Kafka, Terraform, Helm, and Azure DevOps. Proven track record in leading enterprise transformation initiatives and scaling systems from startup to enterprise level.</p> <p>Current Role: Currently working as a Cloud Architect, leading enterprise cloud transformation initiatives. Actively contributing to one of the major project \u2013 an architectural foundation for the company's next generation of digital products. This modern cloud-native platform is designed to become the organizational standard for all future projects, driving consistency, scalability, and innovation across teams.</p> <p>Previous Experience: Spent many years in delivering large-scale software solutions across diverse domains and geographies, including the US, UK, and India (multiple states).</p> <p>Certifications: Microsoft Azure, Microsoft .NET, and  The Open Group Architecture Framework TOGAF certified.</p> <p>Publications: Author of two technical books:</p> <ul> <li>Building Microservices with Containers: A Practical Guide </li> <li>Building Scalable Kubernetes Infrastructure for Microservices</li> </ul> <p>Entrepreneurship: Founder of https://anjikeesari.com, where I share insights and tutorials on a wide range of technologies, including Azure, Kubernetes, PostgreSQL, MongoDB, Terraform, Helm, Azure DevOps, .NET, React, and more.</p> <p>Content Creation: Regular contributor to Medium.com, where I publish articles on emerging cloud-native technologies and best practices.</p> <p>Startup Mindset: Passionate about startup culture, with a proven ability to scale organizations from early-stage to enterprise level. Leverage deep technical expertise and architectural vision to create scalable, resilient, and future-ready solutions that support rapid growth and innovation.</p> <p>AI Exploration: Currently exploring the AI ecosystem to identify strategic opportunities for incorporating intelligent systems into scalable architectures, ensuring alignment with business value and emerging technologies.</p> <p>Kubernetes Expertise: Experience in deploying and managing Kubernetes clusters in production environments. Expert in ArgoCD and Helmcharts and has used these tools to deploy microservices applications on Kubernetes. </p> <p>Infrastructure as Code(IaC) Expertise: In addition to the experience with Kubernetes and related tools, I also have extensive experience with Terraform and deploy infrastructure on various cloud platforms, including Azure.</p> <p>Professional Experience: Throughout my career, I have worked with various companies in different domains such as Banking, Healthcare, and Finance, across countries like India, UK, and US. I am passionate about making a significant impact at my workplace and helping others along the way.</p> <p>Personal Interests: During free time, I finds joy in various activities such as playing soccer, going hiking, exploring new places, and most importantly, spending quality time with his loved ones.</p>"},{"location":"about/accomplishments/","title":"Technical Accomplishments","text":""},{"location":"about/accomplishments/#technical-delivery-highlights","title":"Technical Delivery Highlights","text":""},{"location":"about/accomplishments/#cloud-infrastructure-provisioning-azure-terraform","title":"Cloud Infrastructure &amp; Provisioning (Azure + Terraform)","text":"<ul> <li> <p>Designed and implemented a Terraform foundation to automate infrastructure deployment across Azure Cloud and Azure DevOps, significantly streamlining provisioning processes.</p> </li> <li> <p>Built a Hub &amp; Spoke VNet model to isolate resources and improve network security for enterprise-grade applications.</p> </li> <li> <p>Provisioned Virtual Machines, Azure Bastion Host, Private Endpoints, Private Link, DNS Resolver, and Azure Storage Accounts using Terraform for secure communication and state management.</p> </li> <li> <p>Deployed and configured Azure Container Registry (ACR) for secure storage of private Docker images.</p> </li> </ul>"},{"location":"about/accomplishments/#networking-gateway-dns","title":"Networking, Gateway &amp; DNS","text":"<ul> <li> <p>Configured Azure Application Gateway as the public entry point with reverse proxy, SSL/TLS termination, load balancing, custom listeners, and backend pools to optimize request routing.</p> </li> <li> <p>Set up Azure DNS for new domains and managed DNS records via Terraform for seamless resolution across environments.</p> </li> </ul>"},{"location":"about/accomplishments/#kubernetes-containerization-aks-helm-gitops","title":"Kubernetes &amp; Containerization (AKS + Helm + GitOps)","text":"<ul> <li> <p>Deployed containerized microservices on Azure Kubernetes Service (AKS), ensuring scalability and high availability.</p> </li> <li> <p>Configured NGINX Ingress Controller, Persistent Volumes (PV), and Persistent Volume Claims (PVC) to support application routing and storage needs.</p> </li> <li> <p>Created custom Helm charts and used Helm hooks for deploying services like databases, jobs, and Keycloak containers.</p> </li> <li> <p>Deployed pgAdmin4, MinIO, Jaeger, and KEDA on AKS using Helm and Terraform for database management, object storage, observability, and autoscaling.</p> </li> </ul>"},{"location":"about/accomplishments/#cicd-devops-automation","title":"CI/CD &amp; DevOps Automation","text":"<ul> <li> <p>Developed YAML-based Azure DevOps pipelines for .NET Core, React, and Node.js applications, automating build and release workflows.</p> </li> <li> <p>Integrated Azure DevOps with ACR, AKS, and other services through service connections and DevOps library variable groups, pulling secrets securely from Azure Key Vault.</p> </li> <li> <p>Organized Git repositories and standardized code structure for microservices to improve team collaboration and code reuse.</p> </li> <li> <p>Implemented Helm rollback strategies to quickly recover from deployment issues with minimal downtime.</p> </li> </ul>"},{"location":"about/accomplishments/#gitops-application-lifecycle-management","title":"GitOps &amp; Application Lifecycle Management","text":"<ul> <li> <p>Installed and configured ArgoCD for automated GitOps deployment to AKS clusters.</p> </li> <li> <p>Registered AKS clusters with ArgoCD, created ArgoCD Projects, and deployed Helm charts and applications with lifecycle automation and RBAC enforcement.</p> </li> </ul>"},{"location":"about/accomplishments/#security-secrets-management","title":"Security &amp; Secrets Management","text":"<ul> <li> <p>Integrated Azure Key Vault with AKS using the CSI driver to manage secrets, keys, and certificates securely.</p> </li> <li> <p>Configured RBAC across Kubernetes and ArgoCD to enable fine-grained access control, aligning with security and compliance requirements.</p> </li> </ul>"},{"location":"about/accomplishments/#messaging-eventing-observability","title":"Messaging, Eventing &amp; Observability","text":"<ul> <li> <p>Configured Azure Event Hubs and Kafka topics for producer-consumer applications, enabling seamless asynchronous communication.</p> </li> <li> <p>Integrated OpenTelemetry with Jaeger to monitor .NET microservices, providing end-to-end tracing and performance insights.</p> </li> </ul>"},{"location":"about/accomplishments/#data-analytics","title":"Data &amp; Analytics","text":"<ul> <li> <p>Deployed PostgreSQL Flexible Server for backend databases and managed access via pgAdmin in Kubernetes.</p> </li> <li> <p>Created an Azure Synapse Analytics workspace using Terraform to enable advanced analytics and big data capabilities.</p> </li> </ul>"},{"location":"about/accomplishments/#architecture-best-practices","title":"Architecture &amp; Best Practices","text":"<ul> <li> <p>Contributed to the design of a multi-tenant architecture, enabling efficient resource segmentation for multiple clients within a shared AKS environment.</p> </li> <li> <p>Authored detailed technical documentation to support onboarding, knowledge sharing, and long-term project maintainability.</p> </li> <li> <p>Implemented architecture improvements, best practices, and coding standards, resulting in measurable gains in code quality, system performance, and operational resilience.</p> </li> </ul>"},{"location":"about/accomplishments/#project-delivery-highlights","title":"Project Delivery Highlights","text":"<ul> <li> <p>Successfully developed and delivered high-quality software solutions, meeting business requirements and exceeding performance expectations.</p> </li> <li> <p>Consistently met or exceeded project deadlines, ensuring timely delivery of critical features and releases.</p> </li> <li> <p>Implemented new features and enhancements that significantly improved application performance and user experience.</p> </li> <li> <p>Resolved critical bugs and production issues, contributing to stable, reliable, and secure software operations.</p> </li> </ul>"},{"location":"about/accomplishments/#automation-deployment","title":"Automation &amp; Deployment","text":"<ul> <li> <p>Championed a culture of automation by identifying and eliminating repetitive manual tasks, reducing operational overhead and resource costs.</p> </li> <li> <p>Automated deployment pipelines using scalable and secure CI/CD practices, improving delivery speed, quality, and reliability.</p> </li> <li> <p>Collaborated with cross-functional teams to design and implement enterprise-grade features, including environment monitoring, self-healing mechanisms, and autonomous delivery capabilities.</p> </li> </ul>"},{"location":"about/accomplishments/#code-quality-mentoring-collaboration","title":"Code Quality, Mentoring &amp; Collaboration","text":"<ul> <li> <p>Participated in code reviews, providing constructive feedback and promoting code quality, best practices, and team knowledge sharing.</p> </li> <li> <p>Mentored junior engineers, offering technical guidance and supporting their career development and onboarding.</p> </li> <li> <p>Contributed to the design and implementation of scalable architectural patterns, enabling higher team productivity and feature velocity.</p> </li> </ul>"},{"location":"about/accomplishments/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li> <p>Led the implementation of static code analysis tools, proactively identifying vulnerabilities before production deployment.</p> </li> <li> <p>Defined and enforced secure coding standards and security policies, aligning development practices with industry and regulatory requirements.</p> </li> <li> <p>Provided guidance on Secure Software Development Lifecycle (SSDLC) processes and best practices to ensure ongoing application integrity.</p> </li> <li> <p>Identified and remediated security vulnerabilities, ensuring all deliverables met the highest security and compliance standards.</p> </li> </ul>"},{"location":"about/accomplishments/#testing-reliability","title":"Testing &amp; Reliability","text":"<ul> <li>Implemented and improved testing strategies, including unit, integration, and automated testing, to enhance software reliability and reduce regressions.</li> </ul>"},{"location":"about/profile/","title":"Profile Summary","text":"<p>Accomplished Cloud Architect with over 20 years of experience in software architecture, full-stack development, and Microsoft Azure Cloud solutions. Skilled at designing and delivering scalable, secure, and cloud-native applications, with expertise in .NET, Azure, Kubernetes, Kafka, Terraform, Helm, and Azure DevOps. Proven track record in leading enterprise transformation initiatives and scaling systems from startup to enterprise level.</p> <ul> <li> <p>Current Role: Currently employed at AssetMark as a Cloud Architect, leading enterprise cloud transformation initiatives. Actively contributing to the eWM 3.0 project \u2013 an architectural foundation for the company's next generation of digital products. This modern cloud-native platform is designed to become the organizational standard for all future projects, driving consistency, scalability, and innovation across teams.</p> </li> <li> <p>Previous Experience: Spent 12 years with Tata Consultancy Services (TCS), delivering large-scale software solutions across diverse domains and geographies, including the US, UK, and India (multiple states).</p> </li> <li> <p>Certifications: Microsoft Azure, .NET, and  The Open Group Architecture Framework TOGAF certified.</p> </li> <li> <p>Publications: Author of two technical books:</p> <ul> <li> <p>Building Microservices with Containers: A Practical Guide</p> </li> <li> <p>Building Scalable Kubernetes Infrastructure for Microservices</p> </li> </ul> </li> <li> <p>Entrepreneurship: Founder of https://anjikeesari.com, where I share insights and tutorials on a wide range of technologies, including Azure, Kubernetes, PostgreSQL, MongoDB, Terraform, Helm, Azure DevOps, .NET, React, and more.</p> </li> <li> <p>Content Creation: Regular contributor to Medium.com, where I publish articles on emerging cloud-native technologies and best practices.</p> </li> <li> <p>Startup Mindset: Passionate about startup culture, with a proven ability to scale organizations from early-stage to enterprise level. Leverage deep technical expertise and architectural vision to create scalable, resilient, and future-ready solutions that support rapid growth and innovation.</p> </li> <li> <p>AI Exploration: Currently exploring the AI ecosystem to identify strategic opportunities for incorporating intelligent systems into scalable architectures, ensuring alignment with business value and emerging technologies.</p> </li> <li> <p>Azure Expertise: Extensive experience provisioning and managing Azure PaaS services (App Services, API Apps, Logic Apps, API Management, Application Gateway, Azure SQL Server, Redis Cache, Azure Traffic Manager, Virtual Networks, and NSGs) using Terraform for fully automated infrastructure deployment. Implemented CI/CD pipelines to ensure continuous integration and delivery of cloud services, optimizing deployment processes.</p> </li> <li> <p>CI/CD Expertise: Proficient in designing and implementing CI/CD pipelines using Azure DevOps, supporting deployment to both on-premises and Azure Cloud for websites, APIs, and database projects.</p> </li> <li> <p>Security Focus: Led the integration of CrowdStrike with Microsoft Azure Cloud to improve threat detection and response. Worked with Microsoft Defender for Cloud to enhance security, manage vulnerabilities, and ensure compliance across hybrid environments.</p> </li> <li> <p>Ability to adapt to new technologies, learn quickly, and understand relevant cloud trends.</p> </li> </ul>"},{"location":"about/profile/#technical-expertise-current","title":"Technical Expertise (current)","text":"<p>End-to-end expertise in building and managing microservices-based architectures on the cloud. Experience spans the complete application lifecycle\u2014from infrastructure provisioning to deployment, monitoring, security, and data management for Cloud-Native Applications.</p> <ul> <li> <p>Infrastructure Provisioning: Proficient in provisioning Azure resources using Terraform, enabling automated, repeatable, and scalable infrastructure deployment.</p> </li> <li> <p>Containerization: Skilled in building microservices using Docker containers, promoting modular, portable, and isolated service development.</p> </li> <li> <p>Kubernetes Configuration: Experienced in creating Kubernetes manifest objects using Helm charts, allowing efficient templating and management of complex application deployments.</p> </li> <li> <p>GitOps Deployment: Deploys microservices to Kubernetes clusters using GitOps methodologies with tools like ArgoCD, ensuring declarative, version-controlled, and automated deployment pipelines.</p> </li> <li> <p>CI/CD with Azure DevOps: Expertise in designing and implementing robust CI/CD pipelines using Azure DevOps, supporting end-to-end automation for code integration, testing, and deployment across both on-premises and Azure environments. Proficient in managing build and release pipelines, infrastructure as code, and multi-stage deployments.</p> </li> <li> <p>Monitoring &amp; Observability: Implements observability solutions using OpenTelemetry (OTel) Collector, providing comprehensive tracing, metrics, and logging for distributed applications.</p> </li> <li> <p>Event-Driven Architecture: Designs and implements event-driven messaging patterns using Apache Kafka and Azure Event Hub, enabling scalable and decoupled communication between microservices.</p> </li> <li> <p>Database Management: Manages databases such as PostgreSQL and MongoDB, ensuring data consistency, high availability, and performance across both relational and NoSQL systems.</p> </li> <li> <p>Cloud Security: Strong focus on cloud-native security best practices, including role-based access control (RBAC), secrets management, secure API gateways, network policies, and integration with tools like Microsoft Defender for Cloud and CrowdStrike for threat detection and compliance.</p> </li> <li> <p>Architectural Leadership: Demonstrated leadership in shaping the architectural landscape of organizations by driving cloud adoption, defining best practices, mentoring engineering teams, and leading enterprise-scale modernization and digital transformation initiatives. Passionate about building future-ready platforms that are scalable, resilient, and aligned with business goals.</p> </li> </ul>"},{"location":"about/profile/#technical-expertise-earlier","title":"Technical Expertise (earlier)","text":"<ul> <li> <p>Extensive experience in designing and developing enterprise cloud-based applications using .NET Core Web APIs with Microservice clean architecture, .NET Framework 4.8, C#, React JS, jQuery, WCF, Entity Framework Core, and n-Tier and Client Server Architecture.</p> </li> <li> <p>Proficient in creating Azure DevOps build and release pipelines to host different applications like Websites, APIs, and Database projects on-premises and in Azure cloud.</p> </li> <li> <p>Skilled in provisioning Azure Cloud PaaS and IaaS services, including Azure App Services Web App, API App, Logic App, App Service Environment (ILB ASEv2), App Service Plan, API Management, Application Gateway, Azure SQL Server, Redis Cache, Azure Traffic Manager, Virtual Network, and Network Security Group (NSG).</p> </li> <li> <p>Experience in implementing ASP.NET Identity Management for user Authentication and Authorization in web-based applications.</p> </li> <li> <p>Successful migration of Microsoft .NET-based applications from On-Premises to Azure Cloud PaaS model.</p> </li> <li> <p>Established effective DevOps development processes and proficient in Continuous Integration (CI), Continuous Deliver (CD), and build/release using Microsoft release pipelines for ARM templates, PowerShell scripts, Terraform, and .NET applications.</p> </li> <li> <p>Skilled in Terraform configuration, IaC for provisioning Azure IaaS and PaaS services and automating deployment using Azure DevOps CI/CD Pipeline.</p> </li> <li> <p>Proficient in Azure Active Directory (AAD) and application configuration for Authentication, managed Identity, and service principles.</p> </li> <li> <p>Set up Azure B2C for OAuth-based Single Sign-On (SSO) with Microsoft MFA users, experience with Okta</p> </li> <li> <p>Experience in purchasing and configuring DNS custom domains and SSL certifications.</p> </li> <li> <p>Proficient in Azure identities and governance, including creating users and groups, managing role-based access control (RBAC), and providing access to Azure resources by assigning roles at different scopes.</p> </li> <li> <p>Configuration of Azure policies, resource locks, application of tags on resources, cost management, and invoice generation and analysis.</p> </li> <li> <p>Expertise in configuring VMs, managing VM sizes, adding data disks, configuring networking, configuring VM scale sets, and configuring availability sets.</p> </li> <li> <p>Automated deployment of virtual machines (VMs, Windows/Linux) using ARM templates and Terraform.</p> </li> <li> <p>Created ARM templates, PowerShell scripts, and Terraform scripts for Azure resource provisioning.</p> </li> <li> <p>Experience in creating and configuring Azure App Service, App Service plans, and App Service Environments (ASE), configuring scaling settings, securing App Services, configuring custom domain names, and deploying to deployment slots.</p> </li> <li> <p>Configured Application Insights for diagnostic logs and real-time application monitoring.</p> </li> <li> <p>Proficient in creating and configuring virtual networks, configuring private and public IP addresses, implementing subnets, configuring endpoints, and configuring private endpoints.</p> </li> <li> <p>Ensured secure access to virtual networks by creating security rules, associating network security groups (NSG) to subnets or network interfaces, and implementing Azure Firewall and Azure Bastion.</p> </li> <li> <p>Configured Azure Application Gateway, backend pools, installed certificates, and automated Application Gateway using ARM templates through CI/CD pipeline.</p> </li> <li> <p>Experience in monitoring Azure resources using Azure Monitor, configuring Azure Monitor logs, querying and analyzing logs, setting up alerts and actions, and configuring Application Insights.</p> </li> <li> <p>Proficient in creating and configuring storage accounts, configuring network access, managing access keys, and using Azure Storage Explorer.</p> </li> <li> <p>Expertise in writing cross-cutting re-usable components such as Caching, Exception Handling, Logging, and Validation.</p> </li> <li> <p>Experience in database design using SQL Server, Oracle 11g, and hands-on experience with SSIS and SSRS.</p> </li> <li> <p>Proficient in Microservices Architecture, Service Fabric, and Docker containers.</p> </li> <li> <p>Experience in setting up SSO with SAML or OAuth and collaborating closely with vendors during integration.</p> </li> </ul>"},{"location":"about/profile/#education","title":"Education","text":"<ul> <li>Master of Science (M.Sc Computer Science) \u2013 University of Mumbai, Mumbai, India</li> <li>Bachelor of Science (B.Sc Maths, Physics, Chemestry) \u2013 Osmania University, Hyderabad, India.</li> </ul>"},{"location":"articles/1-article-collection/","title":"Articles","text":"<p>Below is a list of all published articles, including details like publication date, estimated reading time, and Medium website links to read the full content:</p> # Article Title Published Date Reading Time Medium Link 68 Getting Started with Kubernetes CronJob Mar 25, 2024 5 min read Read 67 Connecting to Azure Cache for Redis with redis-cli and stunnel Mar 24, 2024 3 min read Read 66 Install Minio in Azure Kubernetes Services (AKS) Mar 24, 2024 7 min read Read 65 Install Redis Helmchart in Azure Kubernetes Services (AKS) Mar 24, 2024 7 min read Read 64 Install Grafana Loki-Stack Helmchart in Azure Kubernetes Services (AKS) Mar 16, 2024 6 min read Read 63 Install Grafana Helmchart in Azure Kubernetes Services (AKS) Mar 10, 2024 5 min read Read 62 Setup Azure Monitor Alerts for CPU &amp; Memory exceeds for PostgreSQL\u200a\u2014\u200ausing Terraform Mar 2, 2024 5 min read Read 61 Running Azure DevOps Private Agents in AKS Feb 23, 2024 11 min read Read 60 Install pgadmin4 in Azure Kubernetes Services (AKS) with Helmchart using Terraform Feb 17, 2024 11 min read Read 59 Setting up PostgreSQL database in a Docker Container Feb 10, 2024 11 min read Read 58 Setting up SQL Server database in a Docker Container Feb 10, 2024 8 min read Read 57 Dockerfile cheat sheet Feb 10, 2024 3 min read Read 56 Setting up Drupal in a Docker Container Feb 4, 2024 7 min read Read 55 Setup Keycloak in a Docker Container Feb 3, 2024 9 min read Read 54 Containerize Your First Microservice with React.js Feb 3, 2024 8 min read Read 53 Containerize Your First Website using .NET Core MVC Feb 3, 2024 11 min read Read 52 Containerize Your First Microservice with Node.js Feb 3, 2024 8 min read Read 51 Containerize Your First Microservice with .NET Core Web API Feb 3, 2024 7 min read Read 50 Create a Website Using Material for MkDocs: A Step-by-Step Guide Jan 28, 2024 12 min read Read 49 Setting Up Windows Terminal with Oh-My-Posh Jan 28, 2024 6 min read Read 48 Local Development Setup with Dev Containers Jan 20, 2024 7 min read Read 47 Getting Started with Docker Jan 18, 2024 5 min read Read 46 Send Alerts When Website is Down using Azure Application Insights Availability Test Jan 10, 2024 7 min read Read 45 What are Development Containers? Jan 8, 2024 5 min read Read 44 Exploring Docker Fundamentals Jan 5, 2024 6 min read Read 43 Getting Started with Drupal: A Beginner's Guide Dec 30, 2023 7 min read Read 42 Getting Started with Keycloak Dec 28, 2023 9 min read Read 41 Setting Up Mac Terminal with Oh-My-Zsh Dec 25, 2023 6 min read Read 40 Setup Azure Logs Alerts &amp; Notifications for Application Exceptions Dec 15, 2023 8 min read Read 39 Docker Commands Cheat Sheet Dec 10, 2023 5 min read Read 38 Getting Started with ArgoCD Jan 2, 2024 9 min read Read 37 Create Azure Event Hubs for Apache Kafka using Terraform Part-2 Jan 1, 2024 12 min read Read 36 Azure Event Hubs for Apache Kafka Introduction Part-1 Jan 1, 2024 16 min read Read 35 Create Azure Storage Account using Terraform Dec 31, 2023 11 min read Read 34 Create Azure Cache for Redis using Terraform Dec 31, 2023 11 min read Read 33 Create Azure Key Vault using Terraform Dec 30, 2023 13 min read Read 32 Create Azure PostgreSQL\u200a\u2014\u200aFlexible Server using terraform Dec 30, 2023 12 min read Read 31 Create Azure Application Gateway using terraform Dec 30, 2023 9 min read Read 30 Single Sign-On\u200a\u2014\u200aOAuth 2.0 flows Dec 30, 2023 7 min read Read 29 Single Sign-On\u200a\u2014\u200aIntroduction Dec 30, 2023 10 min read Read 28 dig commands Cheat Sheet Dec 27, 2023 4 min read Read 27 A Kubernetes Namespace Stuck in the Terminating State Dec 22, 2023 2 min read Read 26 Interacting with Azure App Service using a publishing profile Dec 22, 2023 2 min read Read 25 Hiding the full file path in VSCode Terminal Dec 9, 2023 2 min read Read 24 Reset a branch to a specific tag in Git Nov 22, 2023 1 min read Read 23 Create a new user node pool in AKS using terraform Sep 10, 2023 8 min read Read 22 Create new service connections &amp; variable groups in Azure DevOps Sep 6, 2023 8 min read Read 21 Azure DevOps CI/CD Strategy for Microservices on Kubernetes Sep 5, 2023 7 min read Read 20 Create Azure DevOps pipeline\u200a\u2014\u200afor .NET Core Web API Sep 5, 2023 8 min read Read 19 Install &amp; Interact with ArgoCD CLI Sep 5, 2023 6 min read Read 18 Install ArgoCD in AKS with helm chart using terraform Sep 5, 2023 11 min read Read 17 Setup Cert-Manager in AKS using Terraform Sep 4, 2023 13 min read Read 16 Setup NGINX ingress controller in AKS using Terraform Sep 4, 2023 12 min read Read 15 Deploying an application to Azure Kubernetes Service (AKS) Sep 4, 2023 7 min read Read 14 Prepare an application for Azure Kubernetes Service (AKS) Sep 4, 2023 10 min read Read 13 Kubernetes Pod troubleshooting Sep 4, 2023 4 min read Read 12 Create Azure Kubernetes Service (AKS) using terraform Sep 3, 2023 18 min read Read 11 Create Azure Container Registry (ACR) using terraform Sep 3, 2023 12 min read Read 10 Create Azure Virtual Network using terraform Sep 3, 2023 11 min read Read 9 Create Azure Log Analytics Workspace using terraform Sep 3, 2023 7 min read Read 8 Setup Terraform Foundation Part-2 Sep 3, 2023 12 min read Read 7 Setup Terraform Foundation Part-1 Sep 3, 2023 15 min read Read 6 Download &amp; install Software in Mac OS Sep 3, 2023 4 min read Read 6 Download &amp; install Software in Windows OS Sep 3, 2023 5 min read Read 5 Terraform Commands\u200a\u2014\u200aCheat Sheet Sep 3, 2023 4 min read Read 4 Helm Commands\u200a\u2014\u200aCheat Sheet Sep 3, 2023 8 min read Read 3 ArgoCD Commands\u200a\u2014\u200aCheat Sheet Sep 3, 2023 8 min read Read 2 Kubectl Commands Cheat Sheet Sep 3, 2023 12 min read Read 1 Git Commands Cheat Sheet Sep 3, 2023 7 min read Read"},{"location":"articles/20240218.1-build-agent/","title":"Running Azure DevOps Private Agents in AKS","text":""},{"location":"articles/20240218.1-build-agent/#setting-up-azure-devops-private-agents-in-azure-kubernetes-service-aks","title":"Setting up Azure DevOps Private Agents in Azure Kubernetes Service (AKS)","text":""},{"location":"articles/20240218.1-build-agent/#introduction","title":"Introduction","text":"<p>The conventional method for configuring Azure DevOps private agents involves deploying them on virtual machines (VMs). However, an interesting alternative is to utilize Azure Kubernetes Service (AKS) for this purpose.</p> <p>This article serves as a guide for setting up Azure DevOps private agents within an AKS cluster using Helm charts, providing step-by-step instructions for the process.</p> <p>Opting to deploy Azure DevOps private agents in AKS instead of provisioning new VMs can significantly impact cost-efficiency, scalability, and manageability of CI/CD pipelines. While VMs are easy to set up, they incur higher costs and may suffer from resource underutilization due to each VM requiring a full operating system and dedicated resources regardless of workload. This setup introduces complexity in scaling and maintenance, alongside increased compute and storage expenses.</p> <p>In contrast, hosting Azure DevOps private agents in AKS offers a more dynamic and cost-effective solution. AKS abstracts much of the complexity in managing containerized applications and leverages Kubernetes' orchestration capabilities. By deploying DevOps agents as containers within AKS, organizations benefit from automatic scaling based on workload, faster startup times compared to VMs, and simplified management of agent fleets. This article aims to explore the advantages of using AKS over VM-based approaches and provide a comprehensive guide to efficiently implementing Azure DevOps private agents within AKS, enhancing DevOps workflows for improved performance, reliability, and cost-effectiveness.</p>"},{"location":"articles/20240218.1-build-agent/#what-is-an-azure-devops-private-agent","title":"What is an Azure DevOps Private Agent?","text":"<p>At its core, an Azure DevOps private agent is a software agent that runs on a user-provided infrastructure, whether that's on-premises servers, virtual machines, or within containerized environments like Azure Kubernetes Service (AKS). These agents are responsible for executing the tasks defined in Azure DevOps pipelines, such as compiling code, running tests, and deploying applications. Each private agent is registered to a specific Azure DevOps organization and can be further associated with one or more projects within that organization.</p> <p>Here are some advantages of Azure DevOps private Agents:</p> <ul> <li> <p>Self-Hosted: Unlike the hosted agents provided by Azure DevOps, which are managed by Microsoft and run in a shared environment, private agents are self-hosted by you or your organization. You install, configure, and manage these agents on your own infrastructure, giving you more control over the execution environment.</p> </li> <li> <p>Customization: Private agents can be customized to match the specific requirements of your build and release tasks. You can install additional software, configure environment variables, and modify the agent to your project's needs.</p> </li> <li> <p>Security and Isolation: Private agents are often used when you need to execute CI/CD jobs in an environment that is not accessible from the public internet or when you want to maintain greater control over security. Private agents can operate within your private network, enhancing security and isolation.</p> </li> <li> <p>Access to Private Resources: If your CI/CD processes need access to private resources, databases, or services that are not publicly accessible, private agents can be configured to reach these resources within your network.</p> </li> </ul>"},{"location":"articles/20240218.1-build-agent/#technical-scenario","title":"Technical Scenario","text":"<p>When you're operating Azure services within a private network that is not accessible via the public internet, and you need connectivity from an Azure DevOps pipeline located in a public network, the solution is to deploy a private build machine within the same virtual network as your private services. This private build machine acts as a bridge, facilitating seamless connections to resources within the private network from Azure DevOps pipelines.</p> <p>For instance, if you've established a private AKS cluster to run your applications, you'll need a dedicated private self-hosted agent within the same virtual network to facilitate the deployment process. Similarly, consider a scenario where you've configured a PostgreSQL server with a private DNS zone, and your Azure DevOps pipeline resides in a public network. To establish a connection to this database, it is necessary to deploy your own private agent within the virtual network housing the PostgreSQL server. This solution enables secure and efficient communication with your private resources.</p>"},{"location":"articles/20240218.1-build-agent/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Installing Private Agent Helmchart in Kubernetes</li> <li>Step-2: Verify private agent resources in AKS</li> <li>Step-3: Create new agent pool in Azure DevOps</li> <li>Step-4: Create new Personal Access Token (PAT)</li> <li>Step-5: Register the Self-Hosted Agent</li> <li>Step-6: Update Build pipeline with Private Agent.</li> <li>Step-7: Test the new Private Agent</li> </ul>"},{"location":"articles/20240218.1-build-agent/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>Azure Subscription: You must have an active Azure subscription to create and manage an AKS cluster.</p> </li> <li> <p>Azure Kubernetes Service (AKS) Cluster: An operational AKS cluster where you will deploy the Azure Pipelines agent. If you do not have one, you must set it up beforehand.</p> </li> <li> <p>Azure DevOps Organization: Access to an Azure DevOps organization is required for creating agent pools and pipelines. </p> </li> <li> <p>Azure CLI: The Azure Command Line Interface (CLI) installed on your workstation for managing Azure resources, including AKS.</p> </li> <li> <p>kubectl: The Kubernetes command-line tool, <code>kubectl</code>, must be installed and configured to communicate with your AKS cluster. This tool is essential for deploying and managing applications on Kubernetes.</p> </li> <li> <p>Helm: Helm, the package manager for Kubernetes, is required for installing and managing the private agent Helm chart. Ensure Helm is installed and ready to use on your system.</p> </li> <li> <p>Permissions in Azure DevOps: Adequate permissions within your Azure DevOps organization to create and manage agent pools, personal access tokens (PATs), and pipelines.</p> </li> </ol>"},{"location":"articles/20240218.1-build-agent/#implementation-details","title":"Implementation Details","text":"<p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>To interact with your Azure Kubernetes Service (AKS) cluster, you need to establish a connection. Depending on your role, you can use either the User or Admin credentials:</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# verify the aks connection by running following commands\nkubectl get no\nkubectl get namespace -A\n</code></pre> <p>Deployment Methods</p> <p>Deploying Azure DevOps private agents within an Azure Kubernetes Service (AKS) cluster offers a modern approach to managing build and deployment infrastructure, providing scalability and cost efficiency. There are two primary methods to achieve this deployment, each with its own set of procedures and advantages.</p> <p>Method 1: Creating and Deploying Your Own Docker Container to AKS</p> <p>This method involves packaging the Azure DevOps agent into a custom Docker container and then deploying that container to your AKS cluster. It allows for maximum customization and control over the agent environment.</p> <ol> <li>Prepare the Azure DevOps Agent Dockerfile</li> <li>Build the Docker Image</li> <li>Push the Image to a Container Registry</li> <li>Deploy the Container to AKS</li> <li>Configure and Run the Azure DevOps Agent</li> </ol> <p>Advantages:</p> <ul> <li>Customizability: This method offers the most flexibility, allowing you to include any tools, scripts, or configurations needed for your builds.</li> <li>Control: You have complete control over the agent's environment, updates, and lifecycle management.</li> </ul> <p>Option 2: Deploying Private Agent Using Helm Charts to AKS</p> <p>Helm charts offer a simplified and declarative way to deploy and manage applications in Kubernetes, including Azure DevOps agents.</p> <p>Advantages:</p> <ul> <li>Simplicity: Helm charts abstract away much of the complexity associated with Kubernetes deployments, making it easier to deploy and manage Azure DevOps agents.</li> <li>Scalability: Easily scale the number of agents up or down by adjusting the Helm release.</li> <li>Reusability: Helm charts can be shared and reused across different environments or projects, promoting consistency and saving time.</li> </ul> <p>Both methods provide a way to implement in AKS for Azure DevOps agent deployment, with the choice depending on your specific needs, expertise, and preferences for customization and management.</p> <p>In this article, we will be utilizing Method 2 to install the private agent, which involves the installation of Private agent Helm charts in AKS, However in case if you want to test docker container locally before using helmchart, here are commands:</p> <pre><code>docker run -d -e AZP_AGENT_NAME=\"&lt;agent name&gt;\" -e AZP_URL=\"https://dev.azure.com/&lt;your org.&gt;\" -e AZP_POOL=\"&lt;agent pool&gt;\" -e AZP_TOKEN=\"&lt;PAT&gt;\" emberstack/azure-pipelines-agent\n</code></pre> <p>verify docker image and container</p> <pre><code>docker ps\ndocker image ls\ndocker container ls\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-1-installing-private-agent-helmchart-in-kubernetes","title":"Step-1: Installing Private Agent Helmchart in Kubernetes","text":"<p>Find the <code>azure-pipelines-agent</code> Helmchart from <code>emberstack</code> on the ArtifactHUB website:</p> <p>Click on the \"Install\" button to retrieve the necessary details for this Helmchart installation</p> <p>For this installation, I am going to use the following Helmchart. You can find detailed information about this Helm chart on the following website:</p> <p>azure-pipelines-agent Helm chart</p> <p>Add helm repo</p> <pre><code>$ helm repo add emberstack https://emberstack.github.io/helm-charts\n</code></pre> <p>Update helm repo</p> <pre><code>$ helm repo update\n</code></pre> <p>Now that you have a Helm chart for your Azure Pipelines agent, it's time to deploy it to your AKS cluster.</p> <p>Install helmchart</p> <pre><code>helm install azure-pipelines-agent emberstack/azure-pipelines-agent --namespace \"build-agent\"\n</code></pre> <p>output</p> <pre><code>NAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:01:21 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre> <p>List helm chart</p> <pre><code>helm list --namespace build-agent\n</code></pre> <p>output</p> <pre><code>NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\nazure-pipelines-agent   build-agent     2               2024-02-05 09:02:09.946719 -0800 PST    deployed        azure-pipelines-agent-2.2.26    2.2.26\n</code></pre> <p>Lsit helm chart history </p> <pre><code>helm history azure-pipelines-agent -n build-agent\n</code></pre> <p>output</p> <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Mon Feb  5 09:01:21 2024        superseded      azure-pipelines-agent-2.2.26    2.2.26          Install complete\n2               Mon Feb  5 09:02:09 2024        deployed        azure-pipelines-agent-2.2.26    2.2.26          Upgrade complete\n</code></pre> <p>Show helm chart status</p> <pre><code>helm status azure-pipelines-agent --namespace build-agent\n</code></pre> <p>output</p> <pre><code>NAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:02:09 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-2-verify-private-agent-resources-in-aks","title":"Step 2. Verify private agent resources in AKS","text":"<p>Validate to make sure all the deployments / services created and running as expected. </p> <p>Run the following <code>kubectl</code> commands to verify the installation in the AKS cluster.</p> <pre><code>kubectl get all -n build-agent\n# or\nkubectl get all,configmaps,secrets -n build-agent\n</code></pre> <p>or</p> <pre><code>kubectl get namespace build-agent\nkubectl get deployments -n build-agent\nkubectl get pods -n build-agent\nkubectl get services -n build-agent\n</code></pre> <p>expected output</p> <pre><code>\n</code></pre> <pre><code>kubectl get configmaps -n build-agent\n</code></pre> <pre><code>NAME               DATA   AGE\nkube-root-ca.crt   1      6h48m\n</code></pre> <pre><code>kubectl get secrets -n build-agent\n</code></pre> <pre><code>\n</code></pre> <pre><code>kubectl get ingress -n build-agent\n</code></pre> <pre><code>No resources found in argocd namespace.\n</code></pre> <p>Ensure that the agent deployment in AKS is successful and verify its connectivity to Azure DevOps. You can also configure auto-scaling rules for the agent pool as needed.</p> <p>Verify the pod deployment by checking pod logs.</p> <pre><code>kubectl logs pods/azure-pipelines-agent-0 -n 'build-agent'\n</code></pre> <p>output</p> <p> </p>"},{"location":"articles/20240218.1-build-agent/#step-3-create-new-agent-pool-in-azure-devops","title":"Step-3: Create new agent pool in Azure DevOps","text":"<p>To group and manage your private agents effectively, create a new agent pool in Azure DevOps. This information will be used during agent registration.</p> <p>Here are the steps to create a new agent pool in Azure DevOps :</p> <ol> <li>Sign in to Azure DevOps.</li> <li>Access your organization or Project depnding on requirement.</li> <li>Navigate to Project Settings.</li> <li>Click on \"Agent pools.\"</li> <li>Create a new agent pool by clicking \"+ New agent.\"</li> <li>Configure the agent pool with a name, description, visibility, and security settings.</li> <li>Click \"Create\" to complete the process.</li> <li>The new agent pool is now ready for use.</li> </ol> <p></p>"},{"location":"articles/20240218.1-build-agent/#step-4-create-new-personal-access-token-pat","title":"Step-4: Create new Personal Access Token (PAT)","text":"<p>Generate a new Personal Access Token (PAT) with required permissions to authenticate the agent with Azure DevOps. This token will be used during agent registration.</p> <p>here are the steps to create a new Personal Access Token (PAT) in Azure DevOps:</p> <ol> <li>Sign in to Azure DevOps.</li> <li>Access your organization.</li> <li>Go to User Settings &gt; Security.</li> <li>Click \"New token\" under Personal access tokens.</li> <li>Configure the PAT: name, expiration, access scope.</li> <li>Click \"Create\" to generate the token.</li> <li>Copy and securely store the PAT.</li> <li>Confirmation message will appear.</li> <li>Use the PAT for authentication in Azure DevOps services.</li> </ol> <p> </p>"},{"location":"articles/20240218.1-build-agent/#step-5-register-the-self-hosted-agent","title":"Step-5: Register the Self-Hosted Agent","text":"<p>After deploying the agent to your AKS cluster using the Helm chart, the next crucial step is to register the agent with Azure DevOps. This process involves configuring the agent with essential details such as your Personal Access Token (PAT), the agent pool for assignment, and any specific capabilities the agent should possess. Successfully completing this step links your self-hosted agent to Azure DevOps, enabling it to execute pipelines and tasks.</p> <p>Updating Parameters in the Helm Chart</p> <p>To register the agent, you'll need to update the Helm chart with specific parameters related to your Azure DevOps setup. These include:</p> <ul> <li><code>pipelines.url</code>: The base URL for your Azure DevOps organization.</li> <li><code>pipelines.pat.value</code>: The Personal Access Token (PAT) that the agent will use to authenticate with Azure DevOps.</li> <li><code>pipelines.pool</code>: The name of the agent pool to which the agent should be registered.</li> </ul> <p>Ensure that you replace placeholders like <code>your-organization</code>, <code>your-personal-access-token</code>, <code>your-agent-pool-name</code>, and any other specific values with the actual information relevant to your setup.</p> <p>Command to Update Helm Chart Values:</p> <p>Execute the following command to update the Helm chart values and register the agent with Azure DevOps:</p> <pre><code>helm upgrade azure-pipelines-agent emberstack/azure-pipelines-agent --set pipelines.url=https://dev.azure.com/orgname,pipelines.pat.value=lsir5gjt2djieulvmlmgv66jdrbmcaeww4oydtsxf25ap52ztpyq,pipelines.pool=azure-pipelines-agent --namespace \"build-agent\"\n</code></pre> <p>Remember to replace <code>your-organization</code>, <code>YOUR_PERSONAL_ACCESS_TOKEN</code>, and <code>your-agent-pool-name</code> with your specific details.</p> <p>Expected Output:</p> <pre><code>Release \"azure-pipelines-agent\" has been upgraded. Happy Helming!\nNAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:51:23 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 9\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre> <p>This output confirms that the Helm chart has been successfully updated and the agent is registered with Azure DevOps. You can now navigate to the Azure DevOps portal to manage and utilize your self-hosted agent within your CI/CD pipelines.</p>"},{"location":"articles/20240218.1-build-agent/#step-6-update-build-pipeline-with-private-agent","title":"Step-6: Update Build pipeline with Private Agent","text":"<p>Once your self-hosted private agent is successfully registered and operational within Azure DevOps, the subsequent step involves integrating this agent into your build pipelines. By specifying the agent pool containing your private agent in the pipeline configuration, you can direct your builds to run on this self-managed infrastructure, leveraging its benefits for your CI/CD processes.</p> <p>Modifying Azure Pipelines Configuration</p> <p>To utilize your self-hosted private agent, you need to modify the configuration of your Azure Pipelines. This adjustment involves specifying the agent pool that your private agent belongs to. By doing so, you ensure that the pipeline runs on your private infrastructure.</p> <p>How to Update Your Build Pipeline with the New Agent Pool:</p> <p>You can update the YAML file defining your build pipeline by including the <code>pool</code> property with the name of your agent pool. Below is an example of how to specify the agent pool within a job in your Azure Pipelines YAML configuration:</p> <pre><code>jobs:\n  - job: test_private_agent_job\n    pool:\n      name: azure-pipelines-agent\n    timeoutInMinutes: 5\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-7-test-the-new-private-agent","title":"Step-7: Test the new Private Agent","text":"<p>To verify that your self-hosted agent is working as expected, you can queue a build or release pipeline that targets the agent pool where your AKS agent is registered. Azure Pipelines will automatically route the job to your self-hosted agent, and you can monitor the job's progress in the Azure DevOps portal.</p>"},{"location":"articles/20240218.1-build-agent/#conclusion","title":"Conclusion","text":"<p>Setting up a self-hosted Azure Pipelines agent in an AKS cluster using Helm charts offers control, scalability, and resource efficiency for your CI/CD workflows. By following the steps outlined in this article, you can identify a Helm chart, deploy it to your AKS cluster, and register the agent with Azure DevOps seamlessly. </p>"},{"location":"articles/20240218.1-build-agent/#reference","title":"Reference","text":"<ul> <li>Run a self-hosted agent in Docker</li> <li>azure-pipelines-agent - Helm Chart</li> </ul>"},{"location":"articles/20240302.1-alerts-cpu-memory/","title":"Setup Azure Monitor Alerts for CPU &amp; Memory exceeds for PostgreSQL - using Terraform","text":""},{"location":"articles/20240302.1-alerts-cpu-memory/#introduction","title":"Introduction","text":"<p>Monitoring and managing your Azure resources are always crucial for any kind of project, and setting up alerts for CPU and memory spikes, especially in database systems like PostgreSQL or SQL Server, is very important and necessary.</p> <p>Whether you're using PostgreSQL or SQL Server databases on Azure, monitoring their performance metrics such as CPU and memory utilization is essential for ensuring optimal performance and availability. Setting up alerts for CPU and memory exceeds allows you to proactively address potential issues before they impact your applications and users.</p> <p>In this article, I will guide you through the steps to set up email, SMS, or voice notifications for your team members when CPU &amp; Memory exceeds more than 80% for PostgreSQL.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#prerequisites","title":"Prerequisites","text":"<p>Before setting up Azure Monitor alerts for CPU and memory spikes in your PostgreSQL or SQL Server databases, ensure the following prerequisites are met:</p> <ol> <li>An active Azure subscription.</li> <li>Access to Azure portal with appropriate permissions.</li> <li>PostgreSQL or SQL Server database deployed and running on Azure.</li> <li>Azure Monitor configured to monitor your database metrics.</li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#create-an-alert-when-cpu-exceeds-an-average-of-80-percent","title":"Create an alert when CPU exceeds an average of 80 percent","text":"<ol> <li> <p>Navigate to Azure Monitor: Go to the Azure portal (https://portal.azure.com) and navigate to your Azure Monitor instance.</p> </li> <li> <p>Create New Alert Rule: Select the appropriate resource (PostgreSQL or SQL Server database) from the list of monitored resources.  </p> </li> <li> <p>Set Conditions: Under the \"Alerts\" section, click on \"New alert rule\" and select the condition type as \"Metric alert.\"</p> </li> <li> <p>Define Condition: Choose the appropriate metric (e.g., \"CPU percentage\") and set the condition to \"Greater than\" and the threshold to 80 percent.  </p> </li> <li>Configure Alert Details: Provide a meaningful name and description for the alert. You can also customize the severity level and alert frequency as per your requirements.  </li> <li> <p>Set Action Group: Specify the action group to be notified when the alert is triggered. This could include sending emails, SMS notifications, or triggering automated remediation tasks.  </p> </li> <li> <p>Review and Create: Review the alert rule configuration and click on \"Create\" to finalize the setup.</p> </li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#create-an-alert-when-memory-exceeds-an-average-of-80-percent","title":"Create an alert when Memory exceeds an average of 80 percent","text":"<ol> <li>Follow the same steps as above, but this time select the memory metric instead of CPU.  </li> <li> <p>Set the condition to \"Greater than\" and the threshold to 80 percent.</p> </li> <li> <p>Configure the alert details and action group as per your preferences.</p> </li> <li> <p>Review and create the alert rule.</p> </li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#test-alert-rule","title":"Test Alert Rule","text":"<p>To ensure that the alert rules are working as expected, you can simulate a CPU or memory spike in your PostgreSQL or SQL Server database. This can be done by running resource-intensive queries or scripts against the database.</p> <p>Once the CPU or memory exceeds the defined threshold, Azure Monitor will trigger the corresponding alert, and notifications will be sent to the specified action group.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#setup-metric-alert-using-terraform","title":"Setup Metric Alert Using Terraform","text":"<p>Below are the steps to set up Azure Monitor alerts for CPU and memory exceeds using Terraform for PostgreSQL or SQL Server databases:</p> <p>Step 1: Install Terraform: If you haven't already installed Terraform, you can download it from the official website: https://www.terraform.io/downloads.html. Follow the installation instructions for your operating system.</p> <p>Step 2: Configure Azure Provider: Create a new directory for your Terraform configuration and create a file named <code>main.tf</code>. In this file, configure the Azure provider with your Azure credentials.</p> <pre><code>provider \"azurerm\" {\n  features {}\n}\n</code></pre> <p>Step 3: Define Alert Rule Resources: Add resource definitions for creating alert rules for CPU and memory exceeds. Specify the appropriate metric and condition for each alert rule.</p> <p>Memory Utilization:</p> <pre><code>// This alert will be triggered when Memory Utilization exceeds 80% for longer than 5 minutes\nresource \"azurerm_monitor_metric_alert\" \"psql_memory_usage\" {\n  name                = \"${azurerm_postgresql_flexible_server.psql.name}-memory_usage\"\n  resource_group_name = azurerm_resource_group.rg.name\n  scopes              = [azurerm_postgresql_flexible_server.psql.id]\n  description         = \"Action will be triggered when Memory utilization exceeds 80% for longer than 5 minutes\"\n  frequency           = \"PT5M\"\n  window_size         = \"PT5M\"\n  auto_mitigate = true\n  enabled = true\n  severity            = 1\n  criteria {\n    metric_namespace = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n    metric_name      = \"memory_percent\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n  }\n\n  action {\n    action_group_id = \"&lt;your-action-group-id&gt;\"\n  }\n  tags = \"default_tags goes here\"\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_postgresql_flexible_server.psql,    \n  ]\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>CPU Utilization:</p> <pre><code>// This alert will be triggered when CPU Utilization exceeds 80% for longer than 5 minutes\nresource \"azurerm_monitor_metric_alert\" \"psql_cpu_usage\" {\n  name                = \"${azurerm_postgresql_flexible_server.psql.name}-cpu_usage\"\n  resource_group_name = azurerm_resource_group.rg.name\n  scopes              = [azurerm_postgresql_flexible_server.psql.id]\n  description         = \"Action will be triggered when CPU utilization exceeds 80% for longer than 5 minutes\"\n  frequency           = \"PT5M\"\n  window_size         = \"PT5M\"\n  severity            = var.alert_severity\n  criteria {\n    metric_namespace = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n    metric_name      = \"cpu_percent\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n  }\n\n  action {\n    action_group_id = \"&lt;your-action-group-id&gt;\"\n  }\n  tags = \"default_tags goes here\"\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_postgresql_flexible_server.psql,    \n  ]\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>Replace <code>&lt;your-resource-group-name&gt;</code>, <code>&lt;your-database-resource-id&gt;</code>, and <code>&lt;your-action-group-id&gt;</code> with your actual Azure resource group name, database resource ID, and action group ID respectively.</p> <p>Step 4: Initialize and Apply Terraform Configuration: Initialize Terraform in your working directory and apply the configuration.</p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <p>By following these steps, you can set up Azure Monitor alerts for CPU and memory exceeds using Terraform for PostgreSQL or SQL Server databases.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#conclusion","title":"Conclusion","text":"<p>Setting up Azure Monitor alerts for CPU and memory exceeds is crucial for maintaining the performance and availability of your PostgreSQL or SQL Server databases on Azure. By proactively monitoring these metrics and configuring alerts, you can quickly identify and address potential issues before they escalate, ensuring a seamless experience for your applications and users.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#references","title":"References","text":"<ul> <li>Microsoft Azure Documentation:</li> <li>Azure Monitor Overview:</li> <li>Azure Monitor Alerts Documentation:</li> <li>Manages a Metric Alert within Azure Monitor:</li> </ul>"},{"location":"articles/20240323.1-redis-connection/","title":"Connecting to Azure Cache for Redis with redis-cli and stunnel","text":""},{"location":"articles/20240323.1-redis-connection/#introduction","title":"Introduction","text":"<p>Azure Cache for Redis is a fully managed, open-source, in-memory data store that is used for improving the performance of applications in Microservices architecture. In Azure, security is something always important, and as such, the Azure redis cache non-SSL port (6379) is often disabled with SSL enabled. also, when utilizing private endpoints, direct connections to Azure Cache for Redis become restricted. In this scenario, connecting to Azure Cache for Redis requires additional steps.</p> <p>In this article, we will walk through the process of connecting to Azure Cache for Redis using <code>redis-cli</code> along with <code>stunnel</code> tools. <code>stunnel</code> acts as a secure tunneling tool, enabling SSL encryption for the connection, while <code>redis-cli</code> serves as the command-line interface to interact with Redis.</p>"},{"location":"articles/20240323.1-redis-connection/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have the following prerequisites installed:</p> <ul> <li><code>stunnel</code>: Download and install <code>stunnel</code> from the official website here.</li> <li><code>redis-cli</code>: Download and install <code>redis-cli</code> from the GitHub repository here or here.</li> <li>Virtual Machine (Jumpbox) in the same network as the private endpoint of redis</li> </ul>"},{"location":"articles/20240323.1-redis-connection/#step-1-login-virtual-machine-jumpbox","title":"Step 1: Login Virtual Machine (jumpbox)","text":"<p>It's essential to have a virtual machine deployed within the same virtual network as the Azure Cache for Redis instance. This ensures that the virtual machine can establish a secure connection to the Redis cache without relying on public endpoints or traversing the internet. you need to first login into this Virtual Machine (VM) in the same network as the private endpoint of redis.</p> <p>Test Redis Connection from jumpbox before making any further steps to make sure connection is successful.</p> <p>Test-NetConnection</p> <pre><code>Test-NetConnection -Port 6380 -ComputerName redis-redis1-poc.redis.cache.windows.net\n</code></pre> <pre><code>ComputerName     : redis-redis1-poc.redis.cache.windows.net\nRemoteAddress    : 10.64.3.7\nRemotePort       : 6380\nInterfaceAlias   : Ethernet 3\nSourceAddress    : 10.24.50.78\nTcpTestSucceeded : True\n</code></pre> <p>nslookup</p> <pre><code>nslookup redis-redis1-poc.privatelink.redis.cache.windows.net\n</code></pre> <pre><code>Server:  UnKnown\nAddress:  168.63.129.14\n\nNon-authoritative answer:\nName:    redis-redis1-poc.privatelink.redis.cache.windows.net\nAddress:  10.64.3.6\n</code></pre>"},{"location":"articles/20240323.1-redis-connection/#step-2-download-and-install-stunnel","title":"Step 2: Download and install stunnel","text":"<p>Download and install <code>stunnel</code> from the provided link. Follow the installation instructions provided by the installer.</p>"},{"location":"articles/20240323.1-redis-connection/#step-3-download-and-install-redis-cli","title":"Step 3: Download and install redis-cli","text":"<p>Download and install <code>redis-cli</code> from the provided GitHub repository links. Follow the installation instructions for your respective operating system.</p>"},{"location":"articles/20240323.1-redis-connection/#step-4-configure-stunnel-to-connect-to-azure-cache","title":"Step 4: Configure stunnel to connect to Azure Cache","text":"<ol> <li> <p>Open <code>stunnel</code> and click on \"Edit Configuration\".     </p> </li> <li> <p>Update the Azure Redis Cache endpoint details in the configuration file.</p> <p> </p> </li> <li> <p>Reload the configuration in <code>stunnel</code>. You should see a \"Connection successful\" message.</p> <p> </p> </li> </ol>"},{"location":"articles/20240323.1-redis-connection/#step-5-start-stunnel-and-connect","title":"Step 5: Start stunnel and connect","text":"<ol> <li> <p>Start <code>stunnel</code>.</p> </li> <li> <p>Open a command prompt or PowerShell window and enter the <code>redis-cli</code> command along with the appropriate authentication key obtained from the Azure Cache for Redis authentication settings.</p> <p> </p> </li> <li> <p>Test the Redis connection by entering commands such as <code>info</code>.</p> <p> </p> </li> </ol> <p>You have now successfully connected to Azure Cache for Redis using the <code>stunnel</code> tool.</p>"},{"location":"articles/20240323.1-redis-connection/#conclusion","title":"Conclusion","text":"<p>Connecting to Azure Cache for Redis, especially when SSL is enforced and private endpoints are in use, requires additional steps beyond a direct connection. By utilizing <code>stunnel</code> to create a secure tunnel and <code>redis-cli</code> to interact with Redis, you can securely access your Azure Cache for Redis instance.</p>"},{"location":"articles/20240323.1-redis-connection/#references","title":"References","text":"<ul> <li>stunnel Downloads</li> <li>Microsoft Archive Redis Releases</li> <li>Redis Hashes Releases</li> </ul>"},{"location":"articles/20240325.1-cronjob/","title":"Getting Started with Kubernetes CronJob","text":""},{"location":"articles/20240325.1-cronjob/#introduction","title":"Introduction","text":"<p>In the container orchestration &amp; Kubernetes technology, managing recurring tasks efficiently is crucial for maintaining a healthy and automated system. One powerful tool in the Kubernetes for handling scheduled tasks is the CronJob. </p> <p>In this article, I will explain what the CronJobs are, their utility in Kubernetes clusters, explore some common use cases, and walk through the process of creating couple of CronJob examples.</p>"},{"location":"articles/20240325.1-cronjob/#what-is-cronjob","title":"What is CronJob?","text":"<p>In Kubernetes <code>CronJob</code> is a resource type used in Kubernetes to automate the execution of tasks on a recurring schedule. It is similar to the traditional cron utility used in Unix-like operating systems, but it operates within the Kubernetes ecosystem. </p> <p>CronJobs allow users to define jobs, which are tasks or pods that run to completion, and specify a schedule in Cron format (minute, hour, day of month, month, day of week) for when these jobs should be executed. Kubernetes CronJobs ensure that these jobs are run at the specified intervals, providing a convenient way to automate repetitive tasks within Kubernetes clusters.</p> <p>Kubernetes CronJobs simplify the management of scheduled tasks within Kubernetes clusters, enabling users to automate operations, backups, data processing, perform routine maintenance, and execute batch processes efficiently.</p>"},{"location":"articles/20240325.1-cronjob/#use-cases","title":"Use Cases","text":"<p>1. Running Scheduled PostgreSQL Queries</p> <p>Imagine you have a PostgreSQL database running for your system, and you need to run specific queries at regular intervals to generate reports or perform data cleanup. CronJobs can be configured to execute psql queries against the database periodically, automating this process.</p> <p>2. Microservices Scenarios</p> <p>In a microservices architecture, various components may require periodic tasks such as log rotation, database backups, or cache refreshing. CronJobs can be employed to schedule these tasks across different microservices, ensuring smooth operation of the entire system.</p>"},{"location":"articles/20240325.1-cronjob/#creating-your-first-cronjob","title":"Creating your First CronJob","text":"<p>Let's walk through the process of creating a simple CronJob using Kubernetes YAML configuration.</p> <p>Step 1: Define the CronJob</p> <p>Create a YAML file (e.g., <code>cronjob.yaml</code>) with the following content:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\n  namespace: sample # update your namespace here$$\nspec:\n  schedule: \"*/1 * * * *\"  # Runs every minute\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: my-container\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo \"Hello, Kubernetes!\"\n          restartPolicy: OnFailure\n</code></pre> <p>Step 2: Apply the Configuration</p> <p>Apply the YAML configuration using </p> <pre><code># kubectl apply -f cronjob-1.yaml\n</code></pre> <p>Step 3: Verify CronJob</p> <p>Check the status of the CronJob using <code>kubectl get cronjobs</code> and <code>kubectl get jobs</code>.</p> <pre><code>kubectl get cronjobs -n sample\n</code></pre> <p>output</p> <pre><code>NAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nmy-cronjob   */1 * * * *   False     0        &lt;none&gt;          38s\n</code></pre> <pre><code>kubectl get jobs -n sample\n</code></pre> <p><pre><code>NAME                  COMPLETIONS   DURATION   AGE\nmy-cronjob-28523782   1/1           4s         11s\n</code></pre> Check the logs</p> <p><pre><code>kubectl logs jobs/my-cronjob-28523783 -n sample\n</code></pre> output</p> <p><pre><code>Tue Mar 26 04:23:01 UTC 2024\nHello, Kubernetes!\n</code></pre> Explanation of key fields in the CronJob YAML</p> <ul> <li> <p>schedule: Specifies the schedule in Cron format (minute, hour, day of month, month, day of week) when the job should run.</p> </li> <li> <p>jobTemplate: Defines the template for the Job created by the CronJob, including pod specifications like containers, volumes, and restart policies.</p> </li> </ul>"},{"location":"articles/20240325.1-cronjob/#creating-your-second-cronjob","title":"Creating your Second CronJob","text":"<p>Let's create a CronJob that demonstrates a real-world use case: performing daily backups of a PostgreSQL database running in a Kubernetes cluster.</p> <p>Here's the YAML configuration for the CronJob:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: postgres-backup\nspec:\n  schedule: \"0 0 * * *\"  # Run at midnight every day\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: postgres-backup\n            image: postgres:latest  # You can use a custom image with backup tools installed\n            command: [\"sh\", \"-c\"]\n            args:\n            - pg_dump -U &lt;username&gt; -h &lt;host&gt; &lt;database_name&gt; &gt; /backup/$(date +\"%Y%m%d\").sql\n            volumeMounts:\n            - name: backup-volume\n              mountPath: /backup\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-volume\n            persistentVolumeClaim:\n              claimName: postgres-pvc  # Name of the PersistentVolumeClaim for PostgreSQL data\n</code></pre> <p>Make sure to replace <code>&lt;username&gt;</code>, <code>&lt;host&gt;</code>, and <code>&lt;database_name&gt;</code> with appropriate values for your PostgreSQL database. also, ensure that you have a PersistentVolumeClaim named <code>postgres-pvc</code> associated with your PostgreSQL deployment.</p> <p>With this CronJob configuration, Kubernetes will automatically execute the backup command at midnight every day, ensuring that your PostgreSQL database is backed up regularly.</p> <p>Explanation of key fields in the CronJob YAML</p> <ul> <li> <p>concurrencyPolicy: Determines how to handle multiple executions of the job concurrently. Options include <code>Allow</code> (default), <code>Forbid</code>, and <code>Replace</code>. Here's a breakdown of the possible values for the concurrencyPolicy field:</p> <ul> <li> <p>Allow: Allows concurrent executions of the job. This means that if a new job is scheduled to run while a previous instance of the job is still running, both jobs will run concurrently.</p> </li> <li> <p>Forbid: Disallows concurrent executions of the job. If a new job is scheduled to run while a previous instance of the job is still running, the new job will not start until the previous one completes.</p> </li> <li> <p>Replace: Replaces the existing job with the new one if a new job is scheduled to run while the previous instance of the job is still running. This effectively terminates the running job and starts the new one.</p> </li> </ul> </li> <li> <p>successfulJobsHistoryLimit:  The successfulJobsHistoryLimit field specifies the number of successfully completed jobs that should be retained in the history of the CronJob. In this case, successfulJobsHistoryLimit: 1 indicates that only the latest successful job will be kept in the history.</p> </li> </ul>"},{"location":"articles/20240325.1-cronjob/#creating-your-third-cronjob","title":"Creating your Third CronJob","text":"<p>Below is an example of a CronJob YAML configuration that schedules the execution of a <code>kubectl</code> command:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: kubectl-command\nspec:\n  schedule: \"*/5 * * * *\"  # Run every 5 minutes\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: kubectl\n            image: bitnami/kubectl:latest\n            command:\n            - kubectl\n            args:\n            - &lt;kubectl_command&gt;  # Replace &lt;kubectl_command&gt; with your desired kubectl command and arguments\n          restartPolicy: OnFailure\n</code></pre> <p>Example:</p> <p>Also, keep in mind that you will need to set up proper Role-Based Access Control (RBAC) permissions, as you may encounter errors such as 'Error from server (Forbidden): services is forbidden' if your service account lacks the necessary permissions.</p> <pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  namespace: jp-test\n  name: jp-runner\nrules:\n- apiGroups:\n  - extensions\n  - apps\n  resources:\n  - pods\n  verbs:\n  - 'get'\n\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: jp-runner\n  namespace: jp-test\nsubjects:\n- kind: ServiceAccount\n  name: sa-jp-runner\n  namespace: jp-test\nroleRef:\n  kind: Role\n  name: jp-runner\n  apiGroup: \"\"\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa-jp-runner\n  namespace: jp-test\n\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: sa-jp-runner\n          containers:\n          - name: hello\n            image: bitnami/kubectl:latest\n            command:\n            - /bin/sh\n            - -c\n            - kubectl patch deployment runners -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"jp-runner\",\"env\":[{\"name\":\"START_TIME\",\"value\":\"'$(date +%s)'\"}]}]}}}}' -n jp-test\n          restartPolicy: OnFailure\n\n# kubectl apply -f cronjob-3.yaml\n</code></pre>"},{"location":"articles/20240325.1-cronjob/#conclusion","title":"Conclusion","text":"<p>In this guide, we've explored the fundamentals of Kubernetes CronJobs, their significance in scheduling recurring tasks within Kubernetes clusters, and provided practical insights into creating and managing CronJobs. By leveraging CronJobs effectively, you can automate routine tasks, streamline operations, and enhance the efficiency of your Kubernetes environment. </p>"},{"location":"articles/20240325.1-cronjob/#references","title":"References","text":"<ul> <li>Running Automated Tasks with a CronJob</li> </ul>"},{"location":"articles/app-availability-alert/","title":"Send Alerts When Website is Down - Azure Application Insights Availability Test","text":""},{"location":"articles/app-availability-alert/#introduction","title":"Introduction","text":"<p>Website downtime can be a critical issue for companies in industries such as Insurance, Healthcare, Finance, and Banking, especially for applications that are mission critical. It can lead to inconvenience for users and potentially result in significant financial losses. To proactively address this challenge, Azure Application Insights offers a powerful feature called availability tests. These tests enable you to monitor your website's availability and receive timely alerts if the site goes down.</p> <p>In this article, I will guide you through the steps to set up email, SMS, or voice notifications for your team members using Azure Application Insights.</p>"},{"location":"articles/app-availability-alert/#prerequisites","title":"Prerequisites","text":"<p>Before we look into the steps, make sure you have the following prerequisites are in place:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure</li> <li>A Application Insights associated to the web application.</li> </ul> <p>Now, let's get started!</p>"},{"location":"articles/app-availability-alert/#step-1-create-action-groups","title":"Step 1: Create Action Groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p> <p>look into my other article for more details on Create Action Groups</p> <p>Action Group &gt; Overview</p> <p></p>"},{"location":"articles/app-availability-alert/#step-2-add-standard-test-in-application-insights","title":"Step 2: Add Standard Test in Application Insights","text":"<p>here are the steps to \"Add Standard Test in Application Insights\" under the \"Availability\" left navigation:</p> <ol> <li> <p>Access Azure Portal</p> </li> <li> <p>Select your application insights</p> </li> <li>Navigate to availability from left nav</li> <li>Click on Add Standard Test</li> <li> <p>Configure Application URL</p> <ul> <li>Enter the complete URL of your website, including the protocol (e.g., https://www.example.com).</li> </ul> </li> <li> <p>Select test locations</p> <ul> <li> <p>In the \"Locations\" section, you can choose the geographic locations from where you want to run the availability test. Azure offers a variety of locations worldwide to ensure comprehensive coverage.</p> </li> <li> <p>Click on \"Add location\" to select additional testing locations if needed.</p> </li> </ul> </li> <li> <p>Configure HTTP response codes:</p> <ul> <li> <p>Under the \"Availability\" tab, you can specify the expected HTTP response codes that indicate your website is running correctly. These codes are used to determine whether the test passes or fails. - in this scenario select 200 status code</p> </li> <li> <p>You can also set up more advanced scenarios by configuring content checks in the \"Content\" tab.</p> </li> </ul> </li> <li> <p>Save and create test:</p> <ul> <li>After configuring the test settings, click the \"Create\" or \"Save\" button to create the availability test. Azure Application Insights will now regularly run this test to check the availability of your website from the selected locations.</li> </ul> </li> <li> <p>Verification:</p> <ul> <li>To verify that the test is working as expected, you can check the test results in the \"Availability\" section. It will display the status of the test and any detected availability issues.</li> </ul> </li> </ol> <p>Application Insights &gt; Availability</p> <p></p> <p>Availability &gt; Add Standard Test</p> <p></p> <p>Add Standard Test &gt; Results</p> <p></p>"},{"location":"articles/app-availability-alert/#step-3-create-alert-rule","title":"Step 3: Create Alert Rule","text":"<p>Now that your availability test is set up, it's time to create an alert rule to trigger notifications when issues arise. by default it already creates a Rule for this.</p> <p>Click on \"Open Rules Alerts page\"</p> <p>Update Alert Rule details like description, severity, automatically resolve alerts checkbox etc.. </p> <p>Configure Actions group</p> <ol> <li>Click on \"Select Action group\" section of the alert rule and choose \"Action groups.</li> <li>Associate the alert rule with the Action Group you created in Step 1.</li> </ol> <p>Alert Rule</p> <p></p> <p>Alert Rule &gt; Details</p> <p></p> <p>Alert Rule &gt; Scope, Action, Condition</p> <p></p>"},{"location":"articles/app-availability-alert/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can set up proactive monitoring and alerting for your website using Azure Application Insights availability tests. This ensures that your team members added in action groups are immediately notified when your website is downtime, allowing for quicker response times and minimizing user disruption.</p>"},{"location":"articles/app-availability-alert/#references","title":"References","text":"<p>Here are some helpful references for further information:</p> <ul> <li>Azure Application Insights Documentation</li> <li>Azure Action Groups Documentation</li> <li>Azure Monitor Alerts Documentation</li> </ul>"},{"location":"articles/app-service-publishing-profile/","title":"Interacting with Azure App Service using a publishing profile","text":""},{"location":"articles/app-service-publishing-profile/#introduction","title":"Introduction","text":"<p>Interacting with Azure App Service using a publishing profile is something you might need in certain scenarios, developers may need to automate tasks such as file management within their App Service.</p> <p>This article provides a step-by-step explanation of a PowerShell script designed to delete files from an Azure App Service using the Azure CLI and REST API.</p>"},{"location":"articles/app-service-publishing-profile/#the-scenario","title":"The Scenario","text":"<p>Consider a situation where you have files within a specific directory on your Azure App Service, and you want to automate the process of deleting these files. This could be part of a larger automation or maintenance workflow.</p>"},{"location":"articles/app-service-publishing-profile/#the-powershell-script","title":"The PowerShell Script","text":"<p>Let's break down the PowerShell script that accomplishes this task:</p> <pre><code># Get publishing profile for web application ( get credentials)\n\n$publishingProfile = az webapp deployment list-publishing-credentials -n $WebAppServiceName -g $ResourceGroupName `\n                        --query '{name:name, publishingUserName:publishingUserName, publishingPassword:publishingPassword}' | Out-String\n\n$publishingProfileObject = ConvertFrom-JSON -InputObject $publishingProfile\n\n# Create Base64 authorization header \n$username = $publishingProfileObject.publishingUserName\n$password = $publishingProfileObject.publishingPassword \n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes((\"{0}:{1}\" -f $username,$password)))\n\n# Define Request Body\n$bodyToPOST = @{   \n    command = \"find . -mindepth 1 -delete\"\n    dir = \"/home/site/wwwroot/App_Data/jobs/triggered/job1\"   \n}   \n\n# Splat all parameters together in $param   \n$param = @{   \n    Uri = \"https://$WebAppServiceName.scm.azurewebsites.net/api/command\"   \n    Headers = @{Authorization=(\"Basic {0}\" -f $base64AuthInfo)}   \n    Method = \"POST\"   \n    Body = (ConvertTo-Json $bodyToPOST)   \n    ContentType = \"application/json\"   \n}   \n\n# Invoke REST call   \nInvoke-RestMethod @param\n</code></pre> <p>Script Explanation</p> <ul> <li>Get Publishing Profile:</li> <li> <p>The script uses the <code>az</code> command-line interface to obtain publishing credentials for the specified Azure App Service. These credentials include a username and password required for authentication.</p> </li> <li> <p>Create Base64 Authorization Header:</p> </li> <li> <p>The script extracts the publishing username and password from the obtained publishing profile and creates a Base64-encoded authorization header. This header is used for authentication when making the REST API call.</p> </li> <li> <p>Define Request Body:</p> </li> <li> <p>The script defines a request body in the form of a hash table (<code>$bodyToPOST</code>). In this example, the command is set to \"find . -mindepth 1 -delete,\" which is a command to delete files in a specified directory (<code>dir</code>).</p> </li> <li> <p>Invoke REST Call:</p> </li> <li>Using splatting, the script combines parameters into the <code>$param</code> hash table for the <code>Invoke-RestMethod</code> cmdlet. Parameters include the URI (REST API endpoint), headers (including the authorization header), HTTP method (POST), request body (converted to JSON), and content type.</li> <li>The <code>Invoke-RestMethod</code> cmdlet is then used to make the REST API call to the Azure App Service's SCM (Site Control Manager) endpoint to execute the specified command.</li> </ul>"},{"location":"articles/app-service-publishing-profile/#conclusion","title":"Conclusion","text":"<p>This PowerShell script provides a practical example of automating file deletion within an Azure App Service. Developers can customize the script for their specific scenarios, incorporating it into larger automation workflows or maintenance tasks. By leveraging Azure CLI and REST APIs, this script showcases the flexibility and extensibility of Azure App Service for managing web applications in the cloud.</p>"},{"location":"articles/application-availability-alerts/","title":"How to Configure Alerts in Azure Application Insights?","text":"<p>Proactive monitoring and alerting are critical for maintaining the reliability and performance of web applications hosted on Azure. Azure Application Insights provides monitoring and alerting capabilities, allowing you to gain insights into your application's behavior and health. </p> <p>In this article, I will provide step by step instructions for configuring monitoring alerts in Azure Application Insights specifically for exceptions that occur in any web application.</p>"},{"location":"articles/application-availability-alerts/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following prerequisites:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure </li> <li>A Application Insights associated to the web application.</li> <li>A web application configured to send logs data to Azure Application Insights.</li> </ul>"},{"location":"articles/application-availability-alerts/#step-1-create-action-groups","title":"Step 1: Create action groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p> <p>Here are the steps to create an Action Group:</p> <ol> <li>Login into azure portal, select Monitoring or \"Application Insights\" -&gt; Alerts - &gt; Action Group - This will take you to the Action Groups page.</li> <li>Select Action groups &gt; Create.</li> <li>Select values for Subscription, Resource group, and Region.</li> <li>Enter a name for Action group name and Display name.</li> <li>In the \"Notifications\" tab, you can set up one or more notification methods for the action group. These can include email, SMS, voice, or other options. To configure a notification, do the following:</li> <li>In the \"Actions\" tab, you can define automated actions that should be taken when the action group is triggered. These actions can include running a Logic App, invoking a webhook, or sending an email.</li> <li>Add Tags (Optional)</li> <li>Review and Create</li> <li>Confirmation</li> </ol> <p>Azure will begin creating the action group. Once the creation process is complete, you will receive a confirmation message.</p> <p>This action group can be associated with alert rules from various Azure services to trigger notifications and automated responses based on specific conditions.</p>"},{"location":"articles/application-availability-alerts/#step-2-create-a-new-alert-rule","title":"Step 2: Create a new alert rule","text":"<ol> <li> <p>Application Insights &gt; Alerts</p> </li> <li> <p>Open the + Create menu, and select Alert rule.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-3-configure-alert-rule-details","title":"Step 3: Configure Alert Rule Details","text":"<ol> <li> <p>In the \"Basics\" tab of the alert rule creation wizard, provide a Name and Description for your alert rule.</p> </li> <li> <p>Under Resource, select the web application associated with the Application Insights instance you're monitoring.</p> </li> <li> <p>For the Condition, click on the \"Add condition\" button.</p> </li> <li> <p>In the condition configuration:</p> </li> <li> <p>Choose \"Custom log search\" as the signal type.</p> </li> <li> <p>Configure the query to filter exceptions. For example, you can use the following query to detect exceptions:      <pre><code>exceptions\n| where type == \"Microsoft.ApplicationInsights.Web.Exceptions.HandledException\"\n</code></pre></p> </li> <li> <p>Set the Aggregation type to \"Count.\"</p> </li> <li> <p>Define the Threshold value that will trigger the alert. For example, if you want to be alerted when there are more than 5 exceptions in a 5-minute window, set the threshold to 5.</p> </li> <li> <p>Set the Operator to \"Greater than.\"</p> </li> <li> <p>Configure the Severity and Alert logic according to your requirements.</p> </li> <li> <p>Click \"Done\" to save the condition.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-5-define-alert-details","title":"Step 5: Define Alert Details","text":"<ol> <li> <p>In the \"Alert details\" section, specify a Name for your alert instance.</p> </li> <li> <p>Set the Severity based on the criticality of the alert.</p> </li> <li> <p>Configure the Action group by selecting the action group you created earlier.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-6-review-and-create","title":"Step 6: Review and Create","text":"<ol> <li> <p>Review all the configurations to ensure they are accurate.</p> </li> <li> <p>Click \"Create alert rule\" to create the alert rule.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-7-testing-and-validation","title":"Step 7: Testing and Validation","text":"<p>Test your alert rule ensure it triggers as expected. To do this, you can deliberately introduce exceptions into your web application and monitor the Azure Application Insights interface for alerts.</p>"},{"location":"articles/application-availability-alerts/#conclusion","title":"Conclusion","text":"<p>Configuring monitoring alerts in Azure Application Insights for exceptions in web applications is a critical to ensure the reliability and performance of your applications. </p>"},{"location":"articles/application-availability-alerts/#references","title":"References","text":"<ul> <li>Create or edit a metric alert rule</li> </ul>"},{"location":"articles/azure-log-alerts/","title":"Setup Azure Logs Alerts &amp; Notifications for Application Exceptions","text":""},{"location":"articles/azure-log-alerts/#introduction","title":"Introduction","text":"<p>Monitoring and detecting application exceptions is crucial for maintaining the reliability and performance of your applications hosted on Azure. Azure Log Analytics and Azure Monitor provide powerful tools to help you achieve this. </p> <p>In this article, I will provide step by step instructions for setting up Azure Logs Alerts and Notifications specifically for application exceptions.</p>"},{"location":"articles/azure-log-alerts/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure </li> <li>A Application Insights associated to the web application.</li> <li>A web application configured to send logs data to Azure Application Insights.</li> <li>An Azure Log Analytics workspace set up to collect logs from your application.</li> <li>Diagnostics Settings configured for your application's logs to send exceptions to the Log Analytics workspace.</li> </ul>"},{"location":"articles/azure-log-alerts/#step-1-create-action-groups","title":"Step 1: Create Action Groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p>"},{"location":"articles/azure-log-alerts/#step-11-action-groups-structure","title":"Step 1.1: Action Groups Structure","text":"<p>Before creating action groups, let me provide some valuable tips here.</p> <p>The number and structure of action groups you need to create can vary based on your application team size and the nature of your project. It's important to modify your approach to meet your specific requirements and needs.</p> <p>Here are a few suggestions:</p> <ol> <li> <p>Single Action Group for All Environments: You can opt for a single action group that covers all your environments. This approach simplifies management and is suitable for smaller teams.</p> </li> <li> <p>Separate Action Groups for Each Environment: You can set up separate action groups for individual environments. This provides more granularity and allows you to modify notifications and responses to each environment's unique characteristics.</p> </li> <li> <p>One for Non-Production and One for Production: For smaller teams, it's often practical to create two action groups\u2014one dedicated to non-production environments and the other focused on the production environment. This way, you can prioritize alerts and responses accordingly.</p> </li> </ol> <p>For larger applications with complex microservices architectures, consider creating separate action groups for each domain or team group. This approach ensures that alerts are directed to the right teams, facilitating efficient issue resolution.</p> <p>The structure and number of action groups you create should align with your team's size, project complexity, and operational requirements. </p>"},{"location":"articles/azure-log-alerts/#step-11-create-action-groups","title":"Step 1.1: Create Action Groups","text":"<p>Here is the list of steps to create action group:</p> <ol> <li> <p>Sign in to the Azure Portal.</p> </li> <li> <p>In the left navigation pane, click on \"Monitor.\"</p> </li> <li> <p>Under \"Alerts,\" click on \"Action groups.\"</p> </li> <li> <p>Click on the \"+ New action group\" button.</p> </li> <li> <p>Under Resource group, select the appropriate resource group.</p> </li> <li> <p>In the \"Basics\" tab, provide a unique Name and Short Name for the action group.</p> </li> <li> <p>In the \"Notifications\" section, configure the action to notify relevant stakeholders for non-production environments. You can set up email notifications, SMS, or other preferred communication channels.</p> </li> <li> <p>Click \"OK\" to create the action group.</p> </li> </ol> <p>Action Group</p> <p></p> <p>Action Group &gt; Basic</p> <p></p> <p>Action Group &gt; Notification</p> <p></p> <p>Action Group &gt; Tags</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-12-test-action-group","title":"Step 1.2: Test Action Group","text":"<p>Before proceeding to the alert rule setup, test the action group by simulating an alert. This will ensure that notifications are properly configured and reaching the intended recipients.</p> <p>Action Group &gt; Test</p> <p>Test Alerts</p> <p></p> <p>Email from mail box</p> <p></p> <p>Email from mail box</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-2-create-a-new-alert-rule","title":"Step 2: Create a new alert rule","text":"<ol> <li> <p>In the Azure Portal, under \"Monitor,\" click on \"Alert rules.\"</p> </li> <li> <p>Click the \"+ New alert rule\" button.</p> </li> </ol>"},{"location":"articles/azure-log-alerts/#step-21-setup-conditions","title":"Step 2.1: Setup Conditions","text":"<ol> <li> <p>In the \"Basics\" tab, choose the appropriate Resource type and Resource for your application.</p> </li> <li> <p>Under the \"Conditions\" section, click on \"+ Add condition.\"</p> </li> <li> <p>In the condition configuration: Choose \"Custom log search\" as the signal type.</p> </li> <li> <p>Click \"Done\" to save the condition.</p> </li> </ol> <p>Monitoring &gt; Alert Rule </p> <p></p> <p>or </p> <p>Web App or Api App &gt; Alert Rule </p> <p></p> <p>Alert Rule &gt; Condition</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-22-write-log-query","title":"Step 2.2: Write Log Query","text":"<p>The log query written should accurately filter and select the exceptions you want to monitor. Make sure it targets the relevant log data and time frame.</p> <p>Write a log query that selects application exceptions. For example:</p> <pre><code>exceptions\n| where timestamp &gt; ago(1h)\n</code></pre> <pre><code>AppServiceAppLogs\n| where TimeGenerated &gt; ago(30m)\n| order by TimeGenerated desc\n| take 100\n</code></pre> <p>Adjust the time range and query as per your requirements.</p> <p>Alert Rule &gt; Log Query</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-23-associate-action-group","title":"Step 2.3: Associate Action Group","text":"<ol> <li> <p>In the \"Actions\" section, click on \"+ Add action group.\"</p> </li> <li> <p>Choose the action group created for the specific environment (non-production or production) where the alert will be triggered.</p> </li> <li> <p>Configure the alert action settings, including the severity and threshold.</p> </li> <li> <p>Click \"OK\" to associate the action group with the alert rule.</p> </li> </ol> <p>Alert Rule &gt; Action Group</p> <p></p> <p>Alert Rule &gt; Details</p> <p></p> <p>Alert Rule &gt; Overvie</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-24-test-alert-rule","title":"Step 2.4: Test Alert Rule","text":"<p>Before finalizing the alert rule, it's essential to test it by creating a simulated exception or by adjusting the log query to match actual exceptions. This ensures that alerts trigger as expected.</p> <p></p> <p></p>"},{"location":"articles/azure-log-alerts/#conclusion","title":"Conclusion","text":"<p>Setting up Azure Logs Alerts &amp; Notifications for Application Exceptions is a proactive approach to identifying and addressing issues in your applications. By following the steps outlined in this guide, you can ensure that relevant teams are promptly notified when exceptions occur, allowing for timely troubleshooting and maintenance.</p>"},{"location":"articles/azure-log-alerts/#references","title":"References","text":"<ul> <li>Azure Monitor Documentation</li> <li>Azure Log Analytics Documentation</li> <li>Azure Action Groups Documentation</li> <li>Create or edit a log alert rule</li> </ul>"},{"location":"articles/dev-containers-introduction/","title":"What are Development Containers?","text":""},{"location":"articles/dev-containers-introduction/#overview","title":"Overview","text":"<p>Modern software development often involves complex setups, dependencies, and configurations. Ensuring that every team member's development environment matches and keeping it consistent can be challenging. That's where Development Containers come into play.</p> <p>In this article, we will explore the fundamentals of <code>Dev Containers</code>. We'll try to understand what they are, why the <code>.devcontainer</code> folder is crucial, how Dev Containers work, and ultimately, we'll learn how to develop applications inside a Dev Container.</p>"},{"location":"articles/dev-containers-introduction/#prerequisites","title":"Prerequisites","text":"<p>Before look into Development Containers, it's helpful to have a basic understanding of application development, containerization, and version control systems like Git. also, you should have Docker and VS code installed on your local machine, as Development Containers often rely on Docker to create isolated development environments.</p>"},{"location":"articles/dev-containers-introduction/#what-is-dev-containers","title":"What is Dev Containers?","text":"<p><code>Development containers</code>, or <code>dev containers</code> also known as <code>Remote Container</code>, are a standardized approach to defining and managing development environments within containers. They encapsulate all the necessary tools, libraries, and configurations required for a specific development project. Dev Containers enable developers to work in a consistent environment, regardless of their local setup, operating system, or development machine.</p> <p>Using Dev Containers can significantly enhance the development experience by eliminating the setup overhead, ensuring consistency, and simplifying collaboration across teams. </p>"},{"location":"articles/dev-containers-introduction/#what-is-devcontainer-folder","title":"What is <code>.devcontainer</code> folder ?","text":"<p>The <code>.devcontainer</code> folder is a special directory in a project that is often used with the Visual Studio Code (VS Code) Dev Containers extension. This folder contains configuration files that define how the development container should be set up when a developer opens the project in VS Code. The configuration details include settings for the container image, runtime, extensions, environment variables, and more.</p> <p>The purpose of the <code>.devcontainer</code> folder is to encapsulate the development environment configuration as code. This enables developers to define and version their development environment settings alongside the project code. When someone opens the project in VS Code, the Dev Containers extension reads the configuration in the .devcontainer folder and automatically configures the development container accordingly.</p> <p><code>.devcontainer</code> folder typically include:</p> <ol> <li> <p>devcontainer.json:  The primary configuration file is <code>devcontainer.json.</code> This JSON file outlines the settings for the development container, specifying the Docker image, runtime, environment variables, user settings, and VS Code extensions to be installed.</p> </li> <li> <p>Dockerfile: Optionally, you may include a Dockerfile in the .devcontainer folder if you need to customize the base Docker image further. This file is used to build the image when the container is created.</p> </li> <li> <p>docker-compose.yml (Optional): If your project requires additional services or multiple containers, you can include a docker-compose.yml file to define the multi-container configuration.</p> </li> </ol>"},{"location":"articles/dev-containers-introduction/#how-dev-containers-works","title":"How Dev Containers works?","text":"<p>Dev Containers, often associated with Visual Studio Code's <code>Remote - Containers extension</code>, works by enabling developers to create and use containerized development environments. These environments are defined within a container and provide a consistent, reproducible setup for coding, building, and running applications. </p> <p>Here's a step-by-step explanation of how Dev Containers work:</p> <ul> <li> <p>Project Configuration: Developers create a special folder in their project named as <code>.devcontainer</code>. Inside this folder, configuration files are added to define the development environment.</p> </li> <li> <p>Configuration Files:  The primary configuration file is <code>devcontainer.json</code>. This JSON file specifies details about the development container, such as the Docker image to use, runtime settings, environment variables, and Visual Studio Code settings.</p> </li> <li> <p>Optional Dockerfile:  Optionally, a <code>Dockerfile</code> can be included in the <code>.devcontainer</code> folder. This file allows developers to customize the base Docker image further. It is used to build the container image when the development environment is created.</p> </li> <li> <p>Opening the Project in Visual Studio Code: Developers open the project in Visual Studio Code, and the presence of the <code>.devcontainer</code> folder is detected by the \"Remote - Containers\" extension.</p> </li> <li> <p>Extension Activation:  The \"Remote - Containers\" extension automatically recognizes the project as a Dev Container project and suggests reopening it in a containerized environment.</p> </li> <li> <p>Container Creation: When developers choose to reopen the project in a container, Visual Studio Code uses the information from the <code>devcontainer.json</code> file to create a Docker container that encapsulates the development environment.</p> </li> <li> <p>Mounting Project Files: Project files are mounted from the local file system into the container, allowing developers to work with their source code seamlessly.</p> </li> <li> <p>Extensions Installation: Visual Studio Code extensions specified in <code>devcontainer.json</code> are installed and run inside the container. This ensures that developers have the necessary tools and extensions for their development tasks.</p> </li> <li> <p>Running and Debugging:  Developers can run and debug their applications within the container. This allows them to test and iterate in an environment that mirrors production closely.</p> </li> </ul>"},{"location":"articles/dev-containers-introduction/#benefits-of-developing-applications-inside-a-dev-container","title":"Benefits of developing applications inside a Dev Container","text":"<p>Setup local development environment that leverages containerization through Microsoft's DevContainer mechanism allows developers to create and run their development environments within containers, providing a consistent and reproducible setup for coding, testing, and debugging. </p> <p>Here are some key benefits of a local development setup using Microsoft's DevContainer:</p> <ul> <li> <p>Container-Based Development: Developers use containers to encapsulate their development environments, ensuring consistency and reproducibility across different machines.</p> </li> <li> <p>Run Services and Databases together: All the necessary services, databases, and supporting components for the application are containerized. This includes running databases like PostgreSQL, MySQL, or services like Redis or RabbitMQ in separate containers.</p> </li> <li> <p>Consistent Development Environments: Developers benefit from a consistent development environment, minimizing the \"it works on my machine\" problem. Everyone working on the project uses the same containerized setup.</p> </li> <li> <p>Isolation and Portability:  Containerization provides isolation for services and dependencies, preventing conflicts between different projects. It also ensures portability, allowing developers to easily share their container configurations.</p> </li> <li> <p>Ease of Onboarding: New developers can quickly get started by cloning the repository and using the predefined Dev Container configuration. This streamlines the onboarding process, as developers don't need to spend time setting up dependencies manually.</p> </li> <li> <p>Integrated Development Environment (IDE) Support: Integrated Development Environments (IDEs) like Visual Studio Code support the \"Remote - Containers\" extension, allowing developers to seamlessly work with containerized environments.</p> </li> <li> <p>Version Control for Development Environments::  The <code>devcontainer.json</code> file, along with other configuration files like Dockerfiles, can be version controlled. This allows teams to track changes to the development environment settings and ensures a versioned and documented setup.</p> </li> <li> <p>Local Testing and Debugging: Developers can locally test and debug their applications within the containerized environment. This includes running and debugging services, APIs, and other components.</p> </li> <li> <p>Facilitates Microservices Development: Container-based development aligns well with microservices architecture. Each microservice can have its own containerized development environment, simplifying the overall development process for microservices-based applications.</p> </li> <li> <p>Docker Compose Integration: Docker Compose may be utilized to define and manage multi-container environments locally. It simplifies the orchestration of multiple containers needed for the complete development setup.</p> </li> </ul>"},{"location":"articles/dev-containers-introduction/#conclusion","title":"Conclusion","text":"<p>In conclusion, Development Containers are adds huge value in modern software development. They bring consistency, portability, isolation, and version control to development environments, making collaboration and project management more efficient. By adopting Dev Containers, development teams can streamline their workflows, reduce setup time, and ensure that everyone is on the same page when it comes to building and testing applications locally.</p>"},{"location":"articles/dev-containers-introduction/#references","title":"References","text":"<ul> <li>Development Containers</li> <li>Use a Docker container</li> <li>Beginner's Series to: Dev Containers</li> <li>Visual Studio Code Remote - Containers</li> </ul>"},{"location":"articles/dev-containers-local-setup/","title":"Local Development Setup with Dev Containers","text":""},{"location":"articles/dev-containers-local-setup/#overview","title":"Overview","text":"<p>In this lab, I will walk you through the process of setting up a local development environment using Dev Containers. <code>Dev Containers</code> provide a consistent and isolated environment for your development projects, ensuring a seamless and uniform experience across team members.</p> <p>If you're new to Dev Containers and want to learn more about the concept, please visit our website to explore - What are Development Containers </p>"},{"location":"articles/dev-containers-local-setup/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ol> <li>Step 1: Install Docker</li> <li>Step 2: Install the remote - containers extension</li> <li>Step 3: Create a new project</li> <li>Step 4: Add Dev Container configuration to a project</li> <li>Step 5: Running a project in a dev container</li> <li>Step 6: Verify the Setup</li> </ol>"},{"location":"articles/dev-containers-local-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure that you have the following prerequisites installed on your machine:</p> <ul> <li>Visual Studio Code</li> <li>(Optional) A Project in Azure DevOps</li> <li>(Optional) Git client tool</li> <li>Docker (If not installed, refer to \"Step 1: Install Docker\" below)</li> </ul>"},{"location":"articles/dev-containers-local-setup/#step-1-install-docker","title":"Step 1: Install Docker","text":"<p>If Docker is not already installed on your machine, follow these steps:</p> <ol> <li>Visit the Docker official website  to download and install Docker for your operating system.</li> <li>Complete the installation process by following the on-screen instructions.</li> <li>Verify the installation by running <code>docker --version</code> in your terminal or command prompt.</li> </ol> <p>Verify the docker installation by running following commands:</p> <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre>"},{"location":"articles/dev-containers-local-setup/#step-2-install-the-remote-containers-extension","title":"Step 2: Install the remote containers extension","text":"<p>To work effectively with Dev Containers in Visual Studio Code, you'll need the \"Remote - Containers\" extension:</p> <p>Open Visual Studio Code and navigate to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window (or press <code>Ctrl+Shift+X</code>). Search for \"Remote - Containers\" and install the extension provided by Microsoft.</p> <p></p>"},{"location":"articles/dev-containers-local-setup/#step-3-create-a-new-project","title":"Step 3: Create a new project","text":"<p>Now, you can start working on your project. You have the option to create a new project or open an existing one in Visual Studio Code.</p> <p>In this step, we will set up a new Node.js API project using a basic Express application as our example. To expedite the process, we'll utilize Express's scaffolding tool to generate the necessary directory structure and essential files.</p> <p>Open your terminal and execute the following commands:</p> <pre><code>$ npx express-generator --no-view src\n$ cd src\n$ npm install\n</code></pre> <p>npx express-generator:</p> <p>The npx express-generator command initializes the project, creating a structure that includes directories like 'bin' and 'routes'.</p> <p></p> <p>npm install:</p> <p>Ensure you run npm install to set up and configure all required Node.js modules.</p> <p>This step ensures that your project is equipped with the necessary dependencies, allowing seamless integration with Docker and efficient containerization of your Node.js application.</p> <p></p> <p>folder structure</p> <p>you've established the foundation for your Node.js API project, complete with a standardized directory structure and essential files.</p> <p></p> <p>This should have created a number of files in your directory, including bin and routes directories. Make sure to run npm install so that npm can get all of your Node.js modules set up and ready to use.</p>"},{"location":"articles/dev-containers-local-setup/#step-4-add-dev-container-configuration-to-a-project","title":"Step 4: Add Dev Container configuration to a project","text":"<p>In the root of your project, create a folder named <code>.devcontainer</code> if it doesn't already exist. Inside this folder, create a file named <code>devcontainer.json</code>. This file will contain the configuration for your Dev Container.</p> <p>Here is a basic example for a Node.js project:</p> <pre><code>// .devcontainer/devcontainer.json\n{\n  \"name\": \"Node.js Dev Container\",\n  \"image\": \"node:14\",\n  \"extensions\": [\"dbaeumer.vscode-eslint\"],\n  \"forwardPorts\": [3000],\n  \"settings\": {\n    \"terminal.integrated.shell.linux\": \"/bin/bash\"\n  }\n}\n</code></pre> <p>Adjust the configuration according to your project's requirements and dependencies.</p> <p>Sample <code>docker-compose.yml</code> for Dev Containers</p> <p>Here is an example of a Docker Compose file defining a multi-container application with three services. The application consists of ASP.NET Core API services that depend on a SQL Server database. Docker Compose is utilized to orchestrate the deployment of these three containers.</p> <pre><code>version: '3'\n\nservices:\n  aspnet-api:\n    build:\n      context: ../aspnet-api\n      dockerfile: ../aspnet-api/Dockerfile\n      args:\n        - ARG1=value1\n        - ARG2=value2\n    container_name: aspnet-api-container\n    ports:\n      - \"80:80\"\n    networks:\n      - default\n    environment:\n      ASPNETCORE_ENVIRONMENT: Production\n      API_VERSION: v1\n    depends_on:\n      - sqlserver-db\n\n  aspnet-app:\n    build:\n      context: ../aspnet-app\n      dockerfile: ../aspnet-app/Dockerfile\n      args:\n        - ARG1=value3\n        - ARG2=value4\n    container_name: aspnet-app-container\n    ports:\n      - \"5000:5000\"\n    networks:\n      - default\n    environment:\n      ASPNETCORE_ENVIRONMENT: Development\n      APP_NAME: MyApp\n    depends_on:\n      - sqlserver-db\n\n  sqlserver-db:\n    build:\n      context: ../sqlserver-db\n      dockerfile: ../sqlserver-db/Dockerfile\n    container_name: sqlserver-db-container\n    environment:\n      SA_PASSWORD: YourStrongPassword\n    ports:\n      - \"1433:1433\"\n    networks:\n      - default\n    command: sh -c \"sleep 20 &amp;&amp; /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P YourStrongPassword -Q 'CREATE DATABASE YourDatabase'\"\n</code></pre> <p>Explanation:</p> <ul> <li>The <code>docker-compose.yml</code> file describes a multi-container application with three services: <code>aspnet-api</code>, <code>aspnet-app</code>, and <code>sqlserver-db</code>.</li> <li><code>aspnet-api</code> and <code>aspnet-app</code> are ASP.NET Core applications, each with its own Dockerfile and build arguments.</li> <li><code>sqlserver-db</code> is a SQL Server container with a specific command to initialize a database after starting.</li> <li>Services are connected to the default network for communication.</li> <li>Dependencies are specified using the <code>depends_on</code> key, ensuring that services wait for others to start before launching.</li> </ul>"},{"location":"articles/dev-containers-local-setup/#step-5-running-a-project-in-a-dev-container","title":"Step 5: Running a project in a dev container","text":"<p>Reopen a project in a container</p> <p>When you choose to \"Reopen in Container\" in Visual Studio Code, it triggers the Remote - Containers extension to rebuild and reopen your project within a containerized environment. </p> <p>Open the Command Palette (<code>Ctrl+Shift+P</code>), type \"Reopen in Container,\" and select the option to rebuild the project inside the Dev Container.</p>"},{"location":"articles/dev-containers-local-setup/#step-6-verify-the-setup","title":"Step 6: Verify the Setup","text":"<p>Once the container is built and the project is reopened, verify that your development environment is running smoothly inside the Dev Container.</p> <p>Now that your local development environment is containerized, you can start coding with the confidence that everyone on your team will have a consistent setup.</p>"},{"location":"articles/dev-containers-local-setup/#docker-compose-commands","title":"Docker compose commands","text":"<p>For more comprehensive details on Docker commands and some commonly used Docker Compose commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"articles/dev-containers-local-setup/#conclusion","title":"Conclusion","text":"<p>Developing applications inside a Dev Container offers a consistent, isolated, and reproducible development environment, streamlining your development workflow. With Docker and VS Code's Remote - Containers extension, you can quickly set up and manage Dev Containers, ensuring that your team works in agreement with the same development environment. This approach simplifies onboarding, enhances collaboration, and minimizes environment-related issues, allowing you to focus on building application development.</p>"},{"location":"articles/dev-containers-local-setup/#references","title":"References","text":"<ul> <li>Development Containers</li> <li>Use a Docker container</li> <li>Beginner's Series to: Dev Containers</li> </ul>"},{"location":"articles/docker-fundamentals/","title":"Exploring Docker Fundamentals","text":""},{"location":"articles/docker-fundamentals/#overview","title":"Overview","text":"<p>In this article, we'll explore the basics of Docker, which are like building blocks for understanding how containers work. Whether you're an experienced coder or just starting out, grasping these basics is essential for easily deploying applications in containers. These core concepts will come in handy as you continue your learning journey with docker.</p>"},{"location":"articles/docker-fundamentals/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a powerful platform that simplifies the process of developing, shipping, and running applications. Docker uses a technology known as containerization to encapsulate an application and its dependencies into a self-contained unit called a <code>container</code>. These containers are lightweight, portable, and consistent across different environments.</p>"},{"location":"articles/docker-fundamentals/#why-use-docker","title":"Why use Docker?","text":"<p>Docker simplifies the development, deployment, and management of applications, offering an adaptable solution for modern software development practices. Its popularity comes from from its ability to address challenges related to consistency, scalability, and efficiency in the software development lifecycle.</p> <p>Docker has become increasingly popular in the software development and IT industry due to its numerous advantages. Here are some key benefits of using Docker:</p> <ol> <li> <p>Portability:    Docker containers encapsulate applications and their dependencies, ensuring consistency across different environments. This portability eliminates the common problem of \"it works on my machine\" and facilitates seamless deployment across various systems.</p> </li> <li> <p>Isolation:    Containers provide a lightweight and isolated environment for applications. Each container runs independently, preventing conflicts between dependencies and ensuring that changes made in one container do not affect others.</p> </li> <li> <p>Efficiency:    Docker's containerization technology enables efficient resource utilization. Containers share the host OS kernel, making them lightweight compared to traditional virtual machines. This results in faster startup times and improved performance.</p> </li> <li> <p>Scalability:    Docker makes it easy to scale applications horizontally by running multiple instances of containers. This scalability allows developers to change the workloads and ensures optimal resource utilization.</p> </li> <li> <p>Microservices architecture:    Docker is integral to the microservices architecture, where applications are composed of small, independently deployable services. Containers facilitate the development, deployment, and scaling of microservices, enabling agility and ease of management.</p> </li> <li> <p>DevOps integration:    Docker aligns well with DevOps practices by promoting collaboration between development and operations teams. Containers can be easily integrated into continuous integration and continuous deployment (CI/CD) pipelines, streamlining the software delivery process.</p> </li> <li> <p>Community support:    Docker's community offers lot of pre-made tools and solutions, helping developers work faster and learn from others.</p> </li> <li> <p>Security:     Docker provides built-in security features, such as isolation and resource constraints, to enhance application security. </p> </li> <li> <p>Cross-platform compatibility:     Docker containers can run on various operating systems, including Linux, Windows, and macOS. This cross-platform compatibility is beneficial for teams working in heterogeneous environments.</p> </li> </ol>"},{"location":"articles/docker-fundamentals/#docker-concepts","title":"Docker concepts","text":"<p>Understanding these basic concepts is essential for effectively working with Docker and leveraging its advantages in terms of portability, scalability, and consistency across different environments.  Here are basic concepts of Docker:</p> <ul> <li> <p>Containerization Containerization is a technology that allows you to package an application and its dependencies, including libraries and configuration files, into a single container image.</p> </li> <li> <p>Images An image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Docker images are used to create containers. They are built from a set of instructions called a Dockerfile.</p> </li> <li> <p>Dockerfile A Dockerfile is a text file that contains a set of instructions for building a Docker image. It specifies the base image, adds dependencies, copies files, and defines other settings necessary for the application to run.</p> </li> <li> <p>Containers Containers are instances of Docker images. They run in isolated environments, ensuring that the application behaves consistently across different environments. Containers share the host OS kernel but have their own file system, process space, and network interfaces.</p> </li> <li> <p>Registries Docker images can be stored and shared through registries. The default registry is Docker Hub, but private registries can also be used. Registries allow versioning, distribution, and collaboration on Docker images.</p> </li> <li> <p>Docker compose Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to define a multi-container application in a single file, specifying services, networks, and volumes.</p> </li> <li> <p>Docker engine Docker Engine is the core component that manages Docker containers. It includes a server, REST API, and a command-line interface (CLI). The Docker daemon runs on the host machine, and the Docker CLI communicates with it to build, run, and manage containers.</p> </li> <li> <p>Volumes Volumes provide a way for containers to persist data outside their lifecycle. They can be used to share data between containers or to persist data even if a container is stopped or removed.</p> </li> <li> <p>Networking Docker provides networking capabilities that allow containers to communicate with each other or with the external world. Containers can be connected to different networks, and ports can be mapped between the host and the containers.</p> </li> </ul>"},{"location":"articles/docker-fundamentals/#container-orchestration","title":"Container orchestration","text":"<p>Whether managing a small cluster or a large-scale production environment, adopting container orchestration is crucial for containerized applications. Here are some container orchestrations:</p> <ul> <li> <p>Kubernetes: Kubernetes is the most widely adopted container orchestration platform. It automates the deployment, scaling, and management of containerized applications, providing a robust and extensible framework.</p> </li> <li> <p>Docker Swarm: Docker Swarm is a native clustering and orchestration solution provided by Docker. While it may not be as feature-rich as Kubernetes, it offers simplicity and seamless integration with Docker.</p> </li> <li> <p>Amazon ECS: Amazon Elastic Container Service (ECS) is a fully managed container orchestration service provided by AWS. It integrates with other AWS services and is suitable for users already utilizing the AWS ecosystem.</p> </li> <li> <p>Azure Kubernetes Service (AKS): AKS is a managed Kubernetes service offered by Microsoft Azure. It simplifies the deployment and management of Kubernetes clusters in the Azure cloud.</p> </li> </ul>"},{"location":"articles/docker-fundamentals/#docker-desktop","title":"Docker Desktop","text":"<p>Docker Desktop is a powerful tool that provides a user-friendly interface and environment for developing, building, and testing applications using Docker containers on local machine. </p> <p>Docker Desktop provides a convenient environment for developers to work with containers on their personal machines.</p>"},{"location":"articles/docker-fundamentals/#install-docker","title":"Install Docker","text":"<p>Here are the steps to install Docker on a different operating systems:</p> <p>Windows:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Windows page.</li> <li>Click on the \"Download for Windows\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Follow the installation wizard, accepting the default options.</li> <li>The installer may require you to restart your computer.</li> </ul> <p>Enable Hyper-V (Windows 10 Pro/Enterprise):</p> <ul> <li>If you're running Windows 10 Pro or Enterprise, Docker Desktop will use Hyper-V for virtualization. Ensure that Hyper-V is enabled in the Windows Features.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, start Docker Desktop from the Start Menu.</li> <li>The Docker icon will appear in the system tray when Docker Desktop is running.</li> </ul> <p>macOS:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Mac page.</li> <li>Click on the \"Download for Mac\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Drag the Docker icon to the Applications folder.</li> <li>Launch Docker from Applications.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, Docker Desktop should start automatically.</li> <li>The Docker icon will appear in the menu bar when Docker Desktop is running.</li> </ul> <p>Verify Docker install:</p> <p>To verify that Docker is installed correctly, open a terminal and run the following command:</p> <pre><code>docker --version\n\n# or\ndocker version\n</code></pre> <p>If you notice this, it indicates that your Docker is not in a running status.</p> <pre><code>error during connect: this error may indicate that the docker daemon is not running: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version\": open //./pipe/docker_engine: The system cannot find the file specified.\nClient:\n Cloud integration: v1.0.35\n Version:           24.0.2\n API version:       1.43\n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n</code></pre> <p>After Docker desktop is started and if everything is set up correctly, you should see following message indicating that your Docker installation is working.</p> <pre><code>Client:\n Cloud integration: v1.0.35 \n Version:           24.0.2  \n API version:       1.43    \n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n\nServer: Docker Desktop 4.21.1 (114176)\n Engine:\n  Version:          24.0.2\n  API version:      1.43 (minimum version 1.12)\n  Go version:       go1.20.4\n  Git commit:       659604f\n  Built:            Thu May 25 21:52:17 2023\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.21\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\n runc:\n  Version:          1.1.7\n  GitCommit:        v1.1.7-0-g860f061\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>Docker is now installed on your machine, and you can start using it to containerize your applications.</p>"},{"location":"articles/docker-fundamentals/#docker-commands","title":"Docker Commands","text":"<p>For more comprehensive details on Docker commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"articles/docker-fundamentals/#conclusion","title":"Conclusion","text":"<p>Docker and containerization have changed the way we build and use application development. Now that you understand the basics of Docker, you're ready to dive deeper. Docker is straightforward and flexible, making it a great tool for developers. It ensures that your application works the same way in different situations, keeps things separate, and easily grows with your needs. So, go ahead and start your journey with containers.</p>"},{"location":"articles/docker-fundamentals/#references","title":"References","text":"<ul> <li>Getting started guide</li> <li>Docker images</li> <li>Docker Documentation</li> <li>Docker Hub</li> </ul>"},{"location":"articles/drupal/","title":"Getting Started with Drupal: A Beginner's Guide","text":"<p>Drupal is a free open-source powerful and flexible content management system (CMS) that allows you to create and manage websites. Whether you're a beginner or an experienced web developer, Drupal can be an excellent choice for building websites, from personal blogs to large enterprise-level applications. </p> <p>In this article, we'll walk you through the basics of getting started with Drupal. exploring what it is, its key features, and why it's essential to consider using it for your website development.</p>"},{"location":"articles/drupal/#what-is-drupal","title":"What is Drupal?","text":"<p>Drupal is a free open-source content management system (CMS) written in PHP and supported by global community of developers. Drupal offers a robust platform for creating dynamic and feature-rich websites. Drupal is suitable for a wide range of web projects, from personal blogs to enterprise-level websites and applications. Drupal allows you to create, organize, and manage content, making it a flexible tool for a wide range of web projects.</p>"},{"location":"articles/drupal/#why-choose-drupal","title":"Why Choose Drupal?","text":"<p>Here are some reasons why Drupal is a good choice for web development:</p> <ul> <li> <p>Flexibility: Drupal's modular architecture allows you to create custom content types, design layouts, and extend functionality through a vast library of contributed modules.</p> </li> <li> <p>Community: Drupal has community of developers, designers, and users who actively contribute to its growth. You'll find extensive documentation, forums, and support resources.</p> </li> <li> <p>Scalability: It's easy to scale your Drupal website as your needs grow. Whether you're running a small blog or a high-traffic e-commerce site, Drupal can handle it.</p> </li> <li> <p>Security: Drupal prioritizes security, offering robust features and frequent security updates to keep your website safe from vulnerabilities.</p> </li> <li> <p>Multilingual Support: Drupal provides multilingual capabilities out of the box, making it accessible for global audiences.</p> </li> </ul>"},{"location":"articles/drupal/#uses-cases-of-drupal","title":"Uses cases of Drupal","text":"<p>Drupal is a flexible content management system (CMS) with a wide range of applications across various industries. Its flexibility and scalability make it suitable for different use cases. </p> <p>Here are some common uses for Drupal:</p> <ul> <li> <p>Corporate Websites: Many businesses choose Drupal to build their corporate websites. Drupal's robust user management, content organization, and scalability make it an ideal choice for showcasing products, services, and company information.</p> </li> <li> <p>E-commerce: Drupal can power e-commerce websites with the help of modules like Drupal Commerce. It enables businesses to set up online stores, manage products, process payments, and provide a seamless shopping experience to customers.</p> </li> <li> <p>Blogs and News Portals: Drupal offers excellent support for blogging and news websites. Content authors can create, categorize, and publish articles easily. Features like tagging, commenting, and social media integration enhance reader engagement.</p> </li> <li> <p>Government Websites: Many government agencies and organizations rely on Drupal for their websites. Drupal's security features, accessibility compliance, and multilingual support meet government requirements for online services.</p> </li> <li> <p>Educational Institutions: Schools, colleges, and universities use Drupal to create websites for academic institutions. Drupal allows for the management of course content, student information, events, and alumni networks.</p> </li> <li> <p>Media and Entertainment: Drupal serves as a foundation for media and entertainment websites, including those for music, movies, and publications. It can handle multimedia content and provide content recommendations.</p> </li> <li> <p>Healthcare: The healthcare industry uses Drupal to create websites for medical practices, hospitals, and health information portals. Drupal's privacy features ensure compliance with healthcare data regulations.</p> </li> </ul>"},{"location":"articles/drupal/#key-features","title":"Key Features","text":"<p>Here are some of the key features of Drupal:</p> <ul> <li> <p>Content Management: Drupal allows users to easily create, manage, and organize content. You can define custom content types, such as articles, blog posts, events, and more, to suit your specific needs.</p> </li> <li> <p>Modular Architecture: Drupal's modular architecture allows you to extend its functionality by adding modules. There are thousands of contributed modules available in the Drupal community, enabling you to add features like e-commerce, SEO optimization, social media integration, and more.</p> </li> <li> <p>Themes and Design: Drupal offers a theming system that allows you to change the look and feel of your website. You can use pre-built themes or create custom themes to match your brand's identity.</p> </li> <li> <p>Multilingual Support: Drupal provides built-in multilingual support, making it easy to create websites in multiple languages. You can translate content, configure language-specific settings, and deliver a global user experience.</p> </li> <li> <p>User Management: Drupal allows you to define user roles and set permissions, ensuring that only authorized users can access and edit specific content or perform certain actions on the site.</p> </li> <li> <p>Scalability: Whether you're running a small blog or a large enterprise website, Drupal can scale to meet your needs. It can handle high-traffic websites and complex web applications.</p> </li> <li> <p>Security: Drupal prioritizes security and offers frequent security updates to protect your website from vulnerabilities. It has a dedicated security team and follows best practices for secure web development.</p> </li> <li> <p>SEO-Friendly: Drupal is SEO-friendly out of the box. It provides clean URLs, customizable meta tags, XML sitemaps, and other SEO features to help your website rank well in search engines.</p> </li> <li> <p>Community Support: Drupal has huge community of developers, designers, and users who contribute to its growth. You can find extensive documentation, forums, and support resources to assist you with any Drupal-related questions or issues.</p> </li> <li> <p>Customization: Drupal's flexibility allows you to create custom content types, modules, and themes to customize your website to your unique requirements. You can build highly customized solutions with Drupal.</p> </li> </ul>"},{"location":"articles/drupal/#drupals-versioning","title":"Drupal's Versioning","text":"<p>Understanding Drupal's versioning is importent for users and developers to know which version of Drupal they are using and what to expect in terms of features, support, and compatibility.</p> <p>Here's an overview of Drupal's versioning system:</p> <p>Major Versions</p> <ol> <li> <p>Drupal 7: Drupal 7 was a significant release known for its stability and contributed module ecosystem. It was widely used for many years. Drupal 7 reached its end of life on November 28, 2022, which means it no longer receives official support and updates from the Drupal community.</p> </li> <li> <p>Drupal 8: Drupal 8 introduced a modern architecture and significant changes, including the use of Symfony components. It focused on improving developer experience and site building capabilities. Drupal 8 reached its end of life on November 2, 2021.</p> </li> <li> <p>Drupal 9: Drupal 9 is an evolution of Drupal 8 rather than a completely new codebase. It removed deprecated code and introduced updated dependencies. Migrating from Drupal 8 to Drupal 9 is typically smoother than a major version transition in the past.</p> </li> <li> <p>Drupal 10: Drupal 10 improves content modeling, block management, menu and taxonomy organization, and permission administration.</p> </li> </ol> <p>Release Cycle</p> <p>Drupal follows a regular release cycle, with minor and patch releases to address bugs, security issues, and add minor improvements. The release cycle for major versions is as follows:</p> <ul> <li> <p>Major Version: Major versions introduce significant changes and new features. They are released approximately every five years.</p> </li> <li> <p>Minor Version: Minor versions are released every six months and include new features and improvements. These updates are backward-compatible within the same major version.</p> </li> <li> <p>Patch Version: Patch versions are released more frequently, often monthly, to address security vulnerabilities and critical bugs. They maintain backward compatibility within the minor version.</p> </li> </ul> <p>Versioning Example</p> <p>Let's take an example to understand Drupal's versioning:</p> <ul> <li>Drupal 8.9.3: In this version, \"8\" represents the major version (Drupal 8), \"9\" represents the minor version (8.9), and \"3\" represents the patch version (8.9.3). This release is part of the Drupal 8 series and includes bug fixes and security updates.</li> </ul> <p>Upgrade Paths</p> <p>To keep your Drupal website secure and up-to-date, it's important to plan for version upgrades. When major versions reach their end of life, it's advisable to upgrade to the next major version, as that's where active development and support are concentrated.</p> <p>For example, Drupal 7 users were encouraged to migrate to Drupal 9 once Drupal 7 reached end of life. Drupal provides tools and documentation to facilitate the migration process.</p>"},{"location":"articles/drupal/#drupals-high-level-components","title":"Drupal's high-level components","text":"<p>Drupal's high-level architecture consists of several key components that work together to deliver its functionality. Here are the high-level architecture components of Drupal:</p> <ul> <li> <p>Core: Drupal's core is the foundational software that includes essential features and functionalities. It provides the basic structure for building websites and web applications. Core components include the content management system, user management, access control, and API support.</p> </li> <li> <p>Modules: Modules are extensions that add specific features and functionality to Drupal. They can be contributed modules developed by the Drupal community or custom modules created to meet specific project requirements. Modules can extend Drupal's capabilities for content types, user roles, e-commerce, SEO, and more.</p> </li> <li> <p>Themes: Themes determine the visual appearance and layout of a Drupal website. Drupal supports theming to create custom designs and styles for your site. Themes can be customized or selected from pre-built themes available in the Drupal theme repository.</p> </li> <li> <p>Database: Drupal uses a relational database to store and manage content, configuration, and user data. It supports various database management systems like MySQL, PostgreSQL, and SQLite. The database stores content types, taxonomy, user profiles, and more.</p> </li> <li> <p>Entities: Entities are fundamental data objects in Drupal, representing items like nodes, users, and taxonomy terms. They have properties and fields, making them flexible and extensible. Entities allow you to define and manage different types of content.</p> </li> <li> <p>Fields: Fields are reusable data elements that can be attached to entities. They enable the creation of structured content with specific data types, such as text, numbers, images, and dates. Fields are used to define content types and capture data within them.</p> </li> <li> <p>Nodes: Nodes are a specific type of entity that represents content items. In Drupal, content is often organized as nodes, and each node belongs to a content type. Nodes can be articles, pages, blog posts, or any other structured content.</p> </li> <li> <p>Taxonomy: Taxonomy is a system for categorizing and organizing content. It allows you to create and manage vocabularies and terms to classify content. Taxonomy terms are used to tag and categorize nodes, making content organization and navigation easier.</p> </li> <li> <p>User Management: Drupal includes a robust user management system that allows you to define user roles, set permissions, and control user access to content and functionality. User profiles, authentication, and registration features are also included.</p> </li> <li> <p>APIs: Drupal provides APIs that enable developers to interact with and extend the platform's functionality. These APIs include the Entity API, Form API, Database API, RESTful Web Services, and more.</p> </li> <li> <p>Cache and Performance Optimization: Drupal includes caching mechanisms to improve website performance. Caching stores frequently accessed data, reducing server load and improving page load times. Performance optimization is a critical consideration in Drupal architecture.</p> </li> <li> <p>Security: Drupal prioritizes security and includes security features such as input validation, output sanitization, and protection against common web vulnerabilities. It also has a dedicated security team that releases timely security updates.</p> </li> <li> <p>Search: Drupal includes search functionality to enable users to search for content within the website. It supports search indexing and can be extended with modules like Apache Solr for more advanced search capabilities.</p> </li> <li> <p>Multilingual Support: Drupal provides built-in multilingual support, allowing you to create websites in multiple languages. It offers translation tools and configuration options for managing multilingual content.</p> </li> <li> <p>Content Workflow: Drupal offers content workflow management features, allowing users to create, review, and publish content through defined workflows. Content moderation, revision tracking, and approval processes can be configured.</p> </li> </ul>"},{"location":"articles/drupal/#php-role-in-drupal","title":"PHP Role in Drupal","text":"<p>Understanding PHP role in Drupal is importent for anyone looking to work with Drupal. Let's explore the PHP relationship with Drupal.</p> <p>Drupal is built using PHP. Here's how PHP is used in Drupal:</p> <ul> <li> <p>Themes: PHP is used to create and customize Drupal themes. Theme files often contain PHP code that generates HTML markup and dynamically renders content.</p> </li> <li> <p>Modules: Drupal modules are extensions that enhance the CMS's functionality. Modules are written in PHP and can be used to add features, create custom content types, and integrate with third-party services.</p> </li> <li> <p>Templates: PHP templates in Drupal are used to control the presentation layer of a website. They determine how content is displayed and can be customized to match the desired design.</p> </li> <li> <p>Custom Code: Developers can write custom PHP code to extend Drupal's capabilities. This includes creating custom modules, hooks, and plugins to help Drupal to specific project requirements.</p> </li> <li> <p>Database Interaction: PHP in Drupal interacts with the underlying database to store and retrieve content, configuration settings, and user data. Drupal uses the PHP Data Objects (PDO) API for secure database operations.</p> </li> <li> <p>User Authentication: PHP handles user authentication and access control in Drupal. It verifies user credentials and manages user sessions.</p> </li> <li> <p>Form Handling: Drupal relies on PHP for form generation and processing. PHP code is used to build and validate forms, handle user submissions, and process data.</p> </li> <li> <p>Dynamic Content: PHP is helpful in rendering dynamic content on Drupal websites. It allows for the generation of content based on user interactions and data from various sources. ich websites and web applications using this flexible CMS.</p> </li> </ul>"},{"location":"articles/drupal/#understanding-databases-and-drupal","title":"Understanding Databases and Drupal","text":"<p>Databases play a fundamental role in how Drupal manages, stores, and retrieves content, configuration, and user data. Understanding the relationship between databases and Drupal is key for  site builders, developers, and administrators to effectively working with the CMS.</p> <p>Whether you're setting up a small blog or a large enterprise website, a solid understanding Drupal's database structure is essential for building and maintaining successful Drupal projects.</p> <p>Here are some most commonly used DBMS systems with Drupal:</p> <ul> <li> <p>MySQL: MySQL is a popular open-source relational database system known for its speed and reliability. </p> </li> <li> <p>PostgreSQL: PostgreSQL is another robust open-source relational database system known for its advanced features and data integrity. Drupal has strong support for PostgreSQL.</p> </li> <li> <p>SQLite: SQLite is a self-contained, serverless, and lightweight relational database engine. It is often used for smaller Drupal installations and development environments.</p> </li> </ul> <p>Key components of Drupal's database architecture include:</p> <ul> <li> <p>Nodes: Content in Drupal, such as articles, pages, and custom content types, is stored as nodes in the database.</p> </li> <li> <p>Fields: Fields define the types of data that can be associated with nodes. They can include text, images, dates, and more.</p> </li> <li> <p>Taxonomy: Taxonomy vocabularies and terms are used to categorize and tag content. They are stored in the database and help organize content.</p> </li> <li> <p>Users and Permissions: User accounts, roles, and permissions are stored in the database, allowing Drupal to manage access control.</p> </li> <li> <p>Configuration Settings: Drupal's configuration settings are stored in the database, enabling site administrators to customize the CMS without code changes.</p> </li> </ul>"},{"location":"articles/drupal/#understanding-drush","title":"Understanding Drush","text":"<p>Drush (short for \"Drupal Shell\") is a command-line tool that enhances Drupal development, administration, and site management. It's a powerful utility that simplifies many tasks, making Drupal development more efficient and convenient. </p> <p>Understanding how to leverage Drush effectively can significantly boost your productivity when working with Drupal. Whether you're a developer streamlining your workflow or a site administrator managing multiple Drupal installations, Drush is a valuable tool for simplifying common tasks and enhancing your Drupal experience.</p> <p>Here's an overview of how Drush integrates with Drupal:</p> <p>What is Drush?</p> <p>Drush is a command-line interface (CLI) tool that provides a wide range of commands for managing Drupal websites. It allows you to perform various tasks, such as site installation, module management, database updates, and more, directly from the command line. Drush is especially popular among Drupal developers and administrators for its speed and flexibility.</p> <p>Installing Drush</p> <p>To use Drush, you need to install it on your local development environment or server. Drush can be installed globally or locally within a Drupal project. The installation method may vary depending on your operating system and preferences. Once installed, you can run Drush commands from your terminal or command prompt.</p> <p>Common Drush Commands</p> <p>Drush offers a vast array of commands, but here are some common tasks you can perform with Drush:</p> <ul> <li> <p>Site Installation: Drush can automate the process of installing Drupal. You can specify installation parameters, such as database credentials, site name, and admin user details, in a command.</p> </li> <li> <p>Module Management: You can use Drush to enable, disable, and update modules on your Drupal site. Drush can also download and install modules directly from Drupal.org or other sources.</p> </li> <li> <p>Theme Management: Drush simplifies theme management by allowing you to enable and set themes as the default from the command line.</p> </li> <li> <p>Database Updates: When Drupal core or contributed modules release updates that require database schema changes, Drush can run these updates quickly and efficiently.</p> </li> <li> <p>Configuration Export and Import: Drush facilitates the export and import of Drupal configuration settings, making it easier to manage configuration changes across different environments.</p> </li> <li> <p>Clearing Caches: Drush provides commands to clear various caches within Drupal, improving site performance and ensuring that changes take effect immediately.</p> </li> <li> <p>User and Role Management: You can create and manage users and roles using Drush commands, streamlining administrative tasks.</p> </li> <li> <p>Batch Processing: Drush can execute commands in batch mode, allowing you to perform tasks on large datasets efficiently.</p> </li> </ul> <p>Drush and Drupal Console</p> <p>Drupal Console is another CLI tool for Drupal, and while it shares some similarities with Drush, it focuses more on code generation and scaffolding for Drupal 8 and later versions. Depending on your project's requirements, you may choose to use either Drush or Drupal Console, or both, to enhance your Drupal development workflow.</p> <p>Scripting and Automation</p> <p>Drush's scripting capabilities allow developers to automate repetitive tasks and build custom scripts for managing Drupal sites. This is particularly useful for deployment processes and site maintenance.</p> <p>Community Support</p> <p>Drush has a robust and active community that regularly contributes to its development. You can find extensive documentation, tutorials, and community support to help you make the most of Drush's capabilities.</p>"},{"location":"articles/drupal/#understanding-multi-tenant-architecture-with-drupal","title":"Understanding Multi-tenant Architecture with Drupal","text":"<p>Multi-tenant architecture is a design approach that allows multiple independent clients, often referred to as \"tenants,\" to share a common software application while maintaining their isolation and customization. This architectural concept is particularly valuable for software platforms like Drupal when serving multiple clients, organizations, or websites from a single codebase and infrastructure. </p> <p>Understanding multi-tenant architecture with Drupal is importent for organizations that serve multiple clients or maintain various websites on a shared platform. Whether you're a SaaS provider, a managed hosting company, or a large organization, multi-tenancy can streamline management, reduce costs, and provide modified solutions to your tenants while maintaining the advantages of Drupal as a robust content management system.</p> <p>Here's an overview of multi-tenant architecture in the context of Drupal:</p> <p>What is Multi-tenant Architecture?</p> <p>Multi-tenant architecture, sometimes called \"Software as a Service\" (SaaS) architecture, is a model where a single instance of an application serves multiple tenants. In the case of Drupal, tenants can represent separate websites, organizations, or clients, each with distinct requirements, data, and customization needs. Despite sharing the same codebase and infrastructure, each tenant remains isolated and can have its configuration, content, and appearance.</p> <p>Key Components</p> <p>To understand multi-tenant architecture with Drupal, let's explore its key components:</p> <ul> <li> <p>Shared Codebase: All tenants share the same Drupal core code, contributed modules, and themes. This centralizes maintenance and ensures consistency across the platform.</p> </li> <li> <p>Tenant-Specific Configuration: Each tenant can have its configuration settings, including database tables, settings.php files, and even custom modules or themes change as per its needs.</p> </li> <li> <p>Shared Database or Separate Databases: Multi-tenancy can employ a shared database approach where all tenants share a single database or separate databases for each tenant, ensuring data isolation.</p> </li> <li> <p>Domain or Subdomain Mapping: Multi-tenant setups often involve domain or subdomain mapping, allowing each tenant to have its unique domain name while running on the same Drupal installation.</p> </li> <li> <p>Tenant-Specific Content and Users: While the codebase and shared resources are common, each tenant can have its content, users, and content structure.</p> </li> </ul> <p>Use Cases</p> <p>Multi-tenant architecture is applicable in various scenarios:</p> <ul> <li> <p>SaaS Applications: Organizations offering software solutions to multiple clients can use multi-tenancy to provide individual instances of their application with customized branding and configurations.</p> </li> <li> <p>Managed Hosting Providers: Hosting providers can offer multi-tenant Drupal hosting services, allowing customers to create and manage their websites while sharing underlying infrastructure.</p> </li> </ul> <p>Advantages</p> <p>Multi-tenant architecture with Drupal offers several advantages:</p> <ul> <li> <p>Cost-Efficiency: Sharing a common codebase and infrastructure reduces maintenance and hosting costs compared to managing separate instances.</p> </li> <li> <p>Centralized Updates: Drupal core updates, module updates, and security patches can be applied once to the shared codebase, ensuring consistency and security across all tenants.</p> </li> <li> <p>Scalability: Adding new tenants is straightforward, making multi-tenant Drupal platforms scalable as the client base grows.</p> </li> <li> <p>Customization: Each tenant can customize its appearance, content, and functionality within the defined constraints, providing flexibility while maintaining consistency.</p> </li> </ul> <p>Considerations</p> <p>When implementing multi-tenant architecture in Drupal, consider these important factors:</p> <ul> <li> <p>Security: Robust access control mechanisms and data isolation are crucial to maintaining security and preventing data leaks between tenants.</p> </li> <li> <p>Resource Management: Monitor resource usage to prevent one tenant's activities from affecting the performance of others.</p> </li> <li> <p>Backup and Recovery: Implement comprehensive backup and recovery strategies to safeguard data across all tenants.</p> </li> <li> <p>Governance: Define governance policies and procedures to manage tenant onboarding, customization, and support efficiently.</p> </li> </ul>"},{"location":"articles/drupal/#drupal-vs-sitecore","title":"Drupal vs Sitecore","text":"<p>Drupal and Sitecore are both content management systems (CMS), but they have significant differences in terms of their target audience, features, licensing, and use cases. </p> <p>Here's a comparison of Drupal and Sitecore:</p> <p>Target Audience:</p> <ul> <li>Drupal: Drupal is an open-source CMS that caters to a wide range of users, from small businesses and individual bloggers to large enterprises and government organizations. It's known for its flexibility and scalability, making it suitable for various types of websites and applications.</li> <li>Sitecore: Sitecore is primarily targeted at larger enterprises and organizations that require a robust and feature-rich CMS. It's often used by businesses with complex digital marketing needs.</li> </ul> <p>Licensing:</p> <ul> <li>Drupal: Drupal is open-source software. This means it's free to download, use, and modify. You only need to pay for hosting and any premium modules or themes you choose to use.</li> <li>Sitecore: Sitecore is a proprietary CMS, which means it requires a paid license to use. The cost of a Sitecore license can be substantial and is typically based on factors like the number of users and the scale of the implementation.</li> </ul> <p>Flexibility and Customization:</p> <ul> <li>Drupal: Drupal is renowned for its flexibility and extensibility. It offers a vast library of contributed modules and themes that allow you to customize your website extensively. Developers can create custom modules and themes to customize Drupal to specific needs.</li> <li>Sitecore: Sitecore is known for its robust digital experience platform (DXP) capabilities. It provides extensive out-of-the-box features for personalization, analytics, and marketing automation. While it's customizable, it's often considered less flexible than Drupal due to its emphasis on specific enterprise-level features.</li> </ul> <p>Scalability:</p> <ul> <li>Drupal: Drupal is highly scalable and can handle websites of all sizes. It's used for everything from small blogs to large, high-traffic sites. Its scalability depends on the hosting infrastructure and optimization.</li> <li>Sitecore: Sitecore is designed for enterprise-level scalability. It can manage large volumes of content, handle high traffic, and provide advanced analytics and personalization features that are essential for big organizations.</li> </ul> <p>Content Management Features:</p> <ul> <li>Drupal: Drupal provides essential content management features like content creation, editing, and versioning. While it can handle content workflows, it may require additional modules for more advanced content management needs.</li> <li>Sitecore: Sitecore offers advanced content management capabilities, including sophisticated content personalization, A/B testing, and marketing automation. It excels in managing complex content strategies.</li> </ul> <p>Learning curve:</p> <ul> <li>Drupal: Drupal can have a steeper learning curve, especially for beginners. Its flexibility and extensive features may require some time to master.</li> <li>Sitecore: Sitecore is known for its user-friendly interface and comprehensive documentation, making it relatively easier to learn for those with the right budget and resources.</li> </ul> <p>In summary, Drupal is a flexible and open-source CMS suitable for a wide range of users and projects, while Sitecore is an enterprise-level CMS with a focus on advanced digital marketing and personalization features. The choice between Drupal and Sitecore depends on your organization's specific requirements, budget, and expertise in managing and customizing these platforms.</p>"},{"location":"articles/drupal/#conclusion","title":"Conclusion","text":"<p>Drupal is a open-source content management system (CMS) that helps you to create websites that stand out in functionality and design. Whether you're a beginner or an experienced developer, Drupal's flexibility and community support make it an excellent choice for building dynamic and engaging websites. </p>"},{"location":"articles/drupal/#references","title":"References","text":"<ul> <li>Drupal Official Website</li> <li>Drupal Documentation</li> <li>Drupal Community Events</li> </ul>"},{"location":"articles/git-reset/","title":"Reset a branch to a specific tag in Git","text":""},{"location":"articles/git-reset/#introduction","title":"Introduction","text":"<p><code>git reset</code> is a powerful command in Git that allows you to undo changes in your working directory and reset the repository to a specific commit. This is useful when you want to discard commits and get back to a particular state in your project's history.</p>"},{"location":"articles/git-reset/#scenario","title":"Scenario","text":"<p>Let's consider a scenario where you have two branches: <code>main</code> and <code>develop</code>. The <code>main</code> branch was initially in a good state, but due to a mistake, some commits were accidentally merged into it instead of the <code>develop</code> branch. Now, you need to reset the <code>main</code> branch to its original state.</p>"},{"location":"articles/git-reset/#steps-to-reset-main-branch","title":"Steps to Reset <code>main</code> Branch","text":"<p>Follow these steps to reset the <code>main</code> branch:</p> <ol> <li> <p>Check the Status:    <pre><code>git status\n</code></pre></p> </li> <li> <p>Switch to <code>main</code> Branch:    <pre><code>git checkout main\n</code></pre></p> </li> <li> <p>View Commit History:    <pre><code>git log --oneline\n</code></pre></p> </li> <li> <p>Perform the Reset:    <pre><code>git reset --hard 592ac92\n</code></pre>    Replace <code>592ac92</code> with the commit hash you want to reset to.</p> </li> <li> <p>Force Push to Remote:    <pre><code>git push -f origin main\n</code></pre>    This forcefully overwrites the existing history in the upstream repository.</p> </li> </ol>"},{"location":"articles/git-reset/#conclusion","title":"Conclusion","text":"<p>Using <code>git reset</code> can be a lifesaver when you need to undo changes and get back to a specific commit. However, use it with caution, especially when force-pushing, as it can rewrite history.</p>"},{"location":"articles/hide-full-path/","title":"Hiding the full file path in VSCode Terminal","text":"<p>In this guide, we'll walk through the steps to hide the full file path and only show the current folder name in your PowerShell prompt.</p> <p>The customization involves modifying the PowerShell profile.</p>"},{"location":"articles/hide-full-path/#step-1-open-the-powershell-terminal-in-vscode","title":"Step 1: Open the PowerShell Terminal in VSCode","text":"<p>Launch Visual Studio Code and open the integrated terminal by selecting  <code>Terminal</code></p>"},{"location":"articles/hide-full-path/#step-2-open-the-powershell-profile","title":"Step 2: Open the PowerShell Profile","text":"<p>Type the following command into the terminal and press Enter:</p> <pre><code>code $PROFILE\n</code></pre> <p>or </p> <p>you can open the profile in <code>Notepad</code> by using the following command:</p> <pre><code>notepad $PROFILE\n</code></pre>"},{"location":"articles/hide-full-path/#step-3-add-custom-prompt-function","title":"Step 3: Add Custom Prompt Function","text":"<pre><code>Function Prompt { \"$( ( get-item $pwd).Name )&gt; \" }\n</code></pre> <p>or</p> <pre><code>function prompt {\n  $p = Split-Path -leaf -path (Get-Location)\n  \"$p&gt; \"\n}\n</code></pre> <p>This modification ensures that only the current folder name is displayed in the PowerShell prompt.</p>"},{"location":"articles/hide-full-path/#step-4-save-the-profile","title":"Step 4: Save the Profile","text":"<p>Save the changes to the profile file and close the editor.</p>"},{"location":"articles/hide-full-path/#step-5-restart-powershell","title":"Step 5: Restart PowerShell","text":"<p>To apply the changes, restart your PowerShell session.</p> <p>Now, when you open a new PowerShell terminal in Visual Studio Code, you will notice that the full file path is hidden, and only the current folder name is displayed in the prompt.</p> <p>before </p> <p></p> <p>after </p> <p></p>"},{"location":"articles/hide-full-path/#conclusion","title":"Conclusion","text":"<p>Customizing the PowerShell prompt in Visual Studio Code allows you to hide the full file path and displaying only the current folder name.</p>"},{"location":"articles/keycloak/","title":"Getting Started with Keycloak: A Beginner\u2019s Guide","text":""},{"location":"articles/keycloak/#introduction","title":"Introduction","text":"<p>In a microservices architecture, ensuring the security of applications and services (APIs) is importent. Unauthorized access to protected data can potentially cost millions of dollars in banking and financial IT companies. User authentication, authorization, and identity management are critical aspects of securing web and mobile applications for these companies. This is where Keycloak comes into play.</p> <p>This in this article I will walk you through the fundamentals of Keycloak, what is keycloak is, Key features, why do we need to use it, where do we use it, By the end of this guide, you'll have a solid understanding of how to leverage Keycloak to enhance the security and usability of your microservices applications.</p>"},{"location":"articles/keycloak/#what-is-keycloak","title":"What is Keycloak?","text":"<p><code>Keycloak</code> is an open-source identity and access management (IAM) solution developed by <code>Red Hat</code>. It offers a robust and flexible framework for securing your applications and services with modern authentication and authorization mechanisms, particularly beneficial in microservices architecture.</p> <p>Keycloak simplifies the process of managing user identities, ensuring only authorized users can access your resources. Whether you're developing a single-page web application, a mobile app, or a complex microservices architecture, Keycloak offers a centralized and highly customizable solution for handling authentication and access control.</p>"},{"location":"articles/keycloak/#key-features","title":"Key Features","text":"<p>Here are the Keycloak's main features in the context of modern web and mobile applications and microservices architectures.</p> <ol> <li> <p>Single Sign-On (SSO): Keycloak enables users to sign in once and gain access to multiple applications without the need to re-enter their credentials. </p> </li> <li> <p>Open source: Keycloak is an open-source project, making it accessible and customizable for various use cases.</p> </li> <li> <p>User federation: Keycloak can integrate with external identity providers, such as LDAP, Active Directory, or social media platforms, allowing you to leverage existing user databases and credentials.</p> </li> <li> <p>Role-Based Access Control: You can define fine-grained access control by assigning roles and permissions to users and applications, ensuring that users only access the resources they are authorized to.</p> </li> <li> <p>Multi-Factor authentication (MFA): Enhance security by implementing MFA, requiring users to provide multiple forms of verification before gaining access.</p> </li> <li> <p>OAuth 2.0 and openID connect: Keycloak supports modern identity and authorization protocols, making it suitable for both traditional and modern web application development.</p> </li> <li> <p>multi-tenant support: which allows you to efficiently manage and secure multiple tenants or organizations within a single Keycloak instance, providing isolation and customized access control for each tenant.</p> </li> <li> <p>Scalability and high availability: Keycloak can be deployed in a scalable and highly available configuration, making it suitable for enterprise-level applications.</p> </li> </ol>"},{"location":"articles/keycloak/#installing-keycloak","title":"Installing Keycloak","text":"<p>Setting up Keycloak can be done in various ways depending on your specific requirements and preferences. Here are some different methods and deployment options for setting up Keycloak:</p> <ol> <li> <p>Running as a container on Docker:    Keycloak is available as an official Docker image on Docker Hub. You can run Keycloak in a Docker container, making it easy to deploy and manage. Docker Compose can also be used for multi-container setups. This approach will also provides full control over the configuration and allows you to customize Keycloak to your needs.</p> </li> <li> <p>Running Keycloak on Kubernetes:    Deploying Keycloak on Kubernetes is another popular choice for container orchestration. Kubernetes provides scalability, high availability, and ease of management. Helm charts are available to simplify deployment on Kubernetes.</p> </li> <li> <p>Installing and running Keycloak locally:    Keycloak provides a standalone server distribution that's easy for developers to set up on their local development machines. This is useful for testing and development purposes.</p> </li> </ol>"},{"location":"articles/keycloak/#when-to-use-and-when-not-to-use-keycloak","title":"When to use and when not to use Keycloak?","text":"<p>Keycloak is a powerful identity and access management (IAM) solution, but it may not be the best fit for every situation. Here's when you should consider using Keycloak and when you might want to explore other options:</p> <p>When to Use Keycloak:</p> <ol> <li> <p>Multi-application environments: Use Keycloak when you have multiple applications or microservices that need centralized authentication and authorization. Keycloak allows you to manage user access across various applications from a single point.</p> </li> <li> <p>Single Sign-On (SSO) requirements: Keycloak is a good choice when you want to implement single sign-on (SSO) across multiple applications, allowing users to log in once and access multiple services without re-entering credentials.</p> </li> <li> <p>Open standards: If you prefer using open standards like OAuth 2.0, OpenID Connect, and SAML for identity and access management, Keycloak supports these protocols, making it suitable for integrating with a wide range of platforms and applications.</p> </li> <li> <p>Customization: When you need a flexible IAM solution that you can customize extensively to meet your organization's specific requirements, Keycloak offers a high degree of customization through its configuration options and extension points.</p> </li> <li> <p>Open source preference: If you prefer open-source solutions with no licensing costs, Keycloak is open-source and can be a cost-effective option.</p> </li> <li> <p>Enterprise needs: Red Hat Single Sign-On (RH-SSO), based on Keycloak, is a suitable choice for enterprises requiring commercial support, certification, and integration with other Red Hat products.</p> </li> <li> <p>Security and compliance: When security and compliance are paramount, Keycloak provides features for securing applications, auditing user activities, and enforcing access controls, helping you meet regulatory requirements.</p> </li> </ol> <p>When Not to Use Keycloak:</p> <ol> <li> <p>Simple authentication: For simple applications or websites that require basic authentication but don't need extensive identity management features, implementing a custom authentication solution or using a lightweight framework might be more straightforward and efficient.</p> </li> <li> <p>Lightweight requirements: If your project has minimal authentication and authorization needs and you don't require features like SSO, fine-grained access control, or user federation, using a full-fledged IAM solution like Keycloak might be overkill. You could opt for simpler authentication methods or third-party authentication providers.</p> </li> <li> <p>Legacy systems: In situations where integrating modern IAM solutions is challenging due to legacy systems or complex infrastructure, you may need to consider alternatives that can work more seamlessly with your existing setup.</p> </li> </ol>"},{"location":"articles/keycloak/#high-level-architecture-of-keycloak","title":"High-level architecture of Keycloak*","text":"<p>Here are the key architectural components of Keycloak:</p> <ul> <li> <p>Keycloak server:    The core component of Keycloak is the Keycloak Server itself. It is responsible for managing user identities, enforcing authentication and authorization policies, and serving as the central hub for all identity-related operations.</p> </li> <li> <p>Realms:    Realms are isolated security domains within Keycloak. Each realm defines its own set of users, roles, policies, and authentication settings. Realms are used to separate and organize different applications or services.</p> </li> <li> <p>Clients:    Clients represent applications or services that interact with Keycloak for authentication and authorization. Each client is associated with a specific realm and defines how the application interacts with Keycloak, including configuration details and security settings.</p> </li> <li> <p>Users and identity providers:    Keycloak allows you to manage user accounts directly within realms. You can also integrate external identity providers (IdPs), such as LDAP, Active Directory, or social media logins, to federate user identities.</p> </li> <li> <p>Authentication flows:    Authentication flows define the steps and mechanisms used to authenticate users. Keycloak supports various authentication methods and allows you to customize the flows to meet your security requirements.</p> </li> <li> <p>Authorization policies:    Authorization policies specify access control rules for protecting resources. Keycloak enables you to define policies based on roles, attributes, or custom logic, ensuring that only authorized users can access specific resources.</p> </li> <li> <p>Tokens:    Keycloak issues tokens, such as access tokens and ID tokens, to represent user authentication and authorization. These tokens are used by clients to access protected resources and services.</p> </li> <li> <p>Single Sign-On (SSO):    Keycloak provides single sign-on capabilities, allowing users to log in once and gain access to multiple applications without the need to re-enter credentials. SSO enhances user experience and security.</p> </li> <li> <p>Adapters and libraries:    Keycloak offers adapters and libraries for various programming languages and frameworks (e.g., Java, Node.js, Spring Boot). These components facilitate integration with Keycloak, enabling your applications to participate in the authentication and authorization process.</p> </li> <li> <p>Administration console:     The Keycloak Administration Console is a web-based interface for administrators to configure and manage realms, clients, users, roles, authentication flows, and other Keycloak settings.</p> </li> </ul>"},{"location":"articles/keycloak/#what-is-keycloak-admin-console","title":"What is Keycloak Admin Console?","text":"<p>The Keycloak Admin Console is a web-based administrative interface provided by Keycloak for managing and configuring the identity and access management (IAM) system. It serves as the primary graphical user interface (GUI) for administrators, allowing them to perform various tasks related to the setup and maintenance of Keycloak realms, clients, users, roles, authentication flows, and other IAM components for securing applications and services.</p> <p>Key features and functions of the Keycloak Admin Console include:</p> <ul> <li> <p>Realm management: Administrators can create, configure, and manage realms. Realms are isolated security domains that define user stores, authentication settings, and authorization policies for different applications or services.</p> </li> <li> <p>Client configuration: Within each realm, administrators can define client applications and configure their settings, including authentication methods, redirect URIs, and fine-grained security policies.</p> </li> <li> <p>User management: Users and their attributes can be managed within realms. Administrators can create, modify, or delete user accounts, as well as perform actions like resetting passwords and enabling or disabling users.</p> </li> <li> <p>Role management: Keycloak allows the creation of roles and role-based access control (RBAC) policies. Administrators can assign roles to users and specify which roles have access to specific resources.</p> </li> <li> <p>Authentication Flows: Administrators can customize and configure authentication flows for realms, defining how users authenticate and which authentication methods are used during login.</p> </li> <li> <p>Token configuration: Settings related to access tokens, refresh tokens, and identity tokens can be adjusted to meet security and performance requirements.</p> </li> <li> <p>Client scopes: Administrators can create and manage client scopes, which define the attributes and permissions associated with clients. Client scopes can be assigned to clients to influence their behavior.</p> </li> <li> <p>Sessions and Single Sign-On (SSO): Administrators can monitor user sessions, including active sessions, and manage single sign-on settings to enhance user experience and security.</p> </li> <li> <p>Logs and auditing: Keycloak provides logs and audit records within the Admin Console for administrators to track system events, user activities, and authentication events.</p> </li> <li> <p>Security configuration: Security settings, such as password policies, brute-force protection, and token validation, can be configured to ensure the IAM system's security.</p> </li> <li> <p>Themes and customization: The Admin Console's look and feel can be customized through themes and branding to match an organization's visual identity.</p> </li> <li> <p>User federation: Administrators can configure user federation with external identity providers (IdPs) within realms, enabling the integration of LDAP, Active Directory, social media logins, and more.</p> </li> <li> <p>Realm export and import: Keycloak allows administrators to export realm configurations and import them into other Keycloak instances, simplifying deployment and migration tasks.</p> </li> </ul>"},{"location":"articles/keycloak/#references","title":"References","text":"<ul> <li>Keycloak Official Documentation</li> <li>GitHub repository</li> <li>Stack Overflow</li> </ul>"},{"location":"articles/mac-terminal-setup/","title":"Setting up Mac Terminal with Oh-My-Zsh","text":"<p>In this tutorial, I'll guide you through the process of customizing your Mac terminal using Oh-My-Zsh.  <code>Oh-My-Zsh</code> is a popular open-source framework for managing Zsh configurations, and it comes with a variety of themes and plugins to enhance your terminal experience.</p>"},{"location":"articles/mac-terminal-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the following prerequisites in place:</p> <ul> <li>A Mac computer running macOS.</li> <li>A terminal emulator. or iTerm2</li> </ul>"},{"location":"articles/mac-terminal-setup/#step-1-install-oh-my-zsh","title":"Step 1: Install Oh-My-Zsh","text":"<p>The first step is to install Oh-My-Zsh. Open your terminal and run the following command:</p> <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>This command will download and install Oh-My-Zsh on your system. Follow the on-screen prompts to complete the installation.</p>"},{"location":"articles/mac-terminal-setup/#step-2-customize-the-theme","title":"Step 2: Customize the Theme","text":"<p>Oh-My-Zsh comes with a variety of themes to choose from. To customize your terminal's appearance, open the Zsh configuration file in a text editor. You can use nano, vim, or any text editor you prefer. Here's an example using nano:</p> <pre><code>nano ~/.zshrc\n# or\nopen ~/.zshrc\n</code></pre> <p>Look for the <code>ZSH_THEME</code> variable in the configuration file and change it to your desired theme. You can find a list of available themes on the Oh-My-Zsh Themes GitHub page.</p> <p>for example:</p> <pre><code>ZSH_THEME=\"aussiegeek\"\n</code></pre> <p>Save your changes and exit the text editor. Your terminal will now display the selected theme.</p>"},{"location":"articles/mac-terminal-setup/#step-3-install-plugins","title":"Step 3: Install Plugins","text":"<p>Plugins enhance the functionality of your terminal. Oh-My-Zsh has a plugin system that allows you to easily add new features. Let's install two popular plugins: autocomplete and auto-highlighting.</p>"},{"location":"articles/mac-terminal-setup/#autocomplete-plugin","title":"Autocomplete Plugin","text":"<p>To install the autocomplete plugin, open your terminal and run:</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions\n</code></pre> <p>Next, add the plugin to your <code>~/.zshrc</code> file. Find the <code>plugins</code> line and add <code>'zsh-autosuggestions'</code> to the list of plugins. It should look like this:</p> <pre><code>plugins=(\n  # other plugins\n  zsh-autosuggestions\n)\n</code></pre> <p>Save your changes and exit.</p>"},{"location":"articles/mac-terminal-setup/#auto-highlighting-plugin","title":"Auto-Highlighting Plugin","text":"<p>To install the auto-highlighting plugin, open your terminal and run:</p> <pre><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\n</code></pre> <p>Now, add this plugin to your <code>~/.zshrc</code> file in the same way you added the autocomplete plugin:</p> <pre><code>plugins=(\n  # other plugins\n  zsh-syntax-highlighting\n)\n</code></pre> <p>Save and exit the file.</p>"},{"location":"articles/mac-terminal-setup/#step-4-enable-the-plugins","title":"Step 4: Enable the Plugins","text":"<p>To enable the newly installed plugins, open your <code>~/.zshrc</code> file once again and make sure the <code>plugins</code> section looks like the following:</p> <pre><code>plugins=(\n  # other plugins\n  git\n  zsh-autosuggestions\n  zsh-syntax-highlighting\n  zsh-completions\n)\n</code></pre> <p>Save your changes and exit the file.</p>"},{"location":"articles/mac-terminal-setup/#step-5-restart-your-terminal","title":"Step 5: Restart Your Terminal","text":"<p>To apply the changes, close and reopen your terminal, or run the following command:</p> <pre><code>source ~/.zshrc\n\n# or\nomz reload \n</code></pre> <p>Your Mac terminal is now set up with Oh-My-Zsh, and you've customized it with your chosen theme and plugins. Enjoy your enhanced terminal experience!</p> <p>Feel free to explore more themes and plugins to further enhance your productivity and style in the terminal.</p> <p>If everything is setup properly then your terminal output will look like this.</p> <p></p>"},{"location":"articles/mac-terminal-setup/#resources","title":"Resources","text":"<ul> <li>Oh-My-Zsh Official Website</li> <li>Oh-My-Zsh Themes</li> </ul>"},{"location":"articles/mkdocs-setup/","title":"Create a Website Using Material for MkDocs: A Step-by-Step Guide","text":""},{"location":"articles/mkdocs-setup/#introduction-to-material-for-mkdocs","title":"Introduction to Material for MkDocs,","text":"<p>Material for MkDocs, a static site generator written in Python, simplifies the process of creating project documentation. it helps for IT professionals, developers, and writers for creating a website. Static websites are fast, secure, and easy to maintain, making them an ideal choice for documentation, blogs, and personal websites. </p> <p>In this article, I will guide you through the steps by step instructions on how to create a static website or documentation site using Material for MkDocs, an extension of the MKDocs project that brings a modern look and additional features to your site.</p>"},{"location":"articles/mkdocs-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of Python and Markdown.</li> <li>Access to a command-line interface.</li> </ul>"},{"location":"articles/mkdocs-setup/#step-1-installation","title":"Step 1: Installation","text":"<p>Install Homebrew (macOS users):</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Install Python 3:</p> <p>MkDocs requires a recent version of <code>Python</code> and the Python package manager, <code>pip</code>, to be installed on your system.</p> <pre><code>brew install python3\n\n# verify the installation\npython3 --version\n</code></pre> <p>(or use the respective command for your operating system).</p> <p>Upgrade PIP:</p> <pre><code>pip3 install --upgrade pip\n</code></pre> <p>This ensures you have the latest version of PIP.</p> <p>Install MKDocs and Related Packages:</p> <pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-material-extensions\n</code></pre> <p>These commands install MKDocs, the Material theme, and additional extensions.</p>"},{"location":"articles/mkdocs-setup/#step-2-creating-your-project","title":"Step 2: Creating Your Project","text":"<p>Initialize Your MKDocs Project:</p> <pre><code>mkdocs new my-project\n</code></pre> <p>output <pre><code>INFO    -  Creating project directory: my-project\nINFO    -  Writing config file: my-project/mkdocs.yml\nINFO    -  Writing initial docs: my-project/docs/index.md\n</code></pre></p> <p>Replace 'my-project' with your project name. This command creates a new directory with essential configuration files.</p> <p>Navigate to Your Project Directory:</p> <pre><code>cd my-project\n</code></pre> <p>Open the project in VS code</p> <pre><code>code .\n</code></pre> <p></p>"},{"location":"articles/mkdocs-setup/#step-3-run-the-project-locally","title":"Step 3: Run the Project Locally","text":"<p>MkDocs comes with a <code>built-in dev-server</code> that lets you preview your documentation as you work on it. Make sure you're in the same directory as the <code>mkdocs.yml</code> configuration file, and then start the server by running the <code>mkdocs serve</code> command:</p> <p><pre><code>mkdocs serve\n</code></pre> </p> <p>This command starts a local server. Access your site at http://127.0.0.1:8000 and see changes in real-time as you edit your documents.</p> <p></p> <p>The server automatically rebuilds your site when you save changes, allowing you to see updates in real-time.</p>"},{"location":"articles/mkdocs-setup/#step-4-add-content-configure-the-website","title":"Step 4: Add Content &amp; Configure the Website","text":"<p>MKDocs organizes content using Markdown files in the <code>docs</code> directory. The <code>index.md</code> file is the homepage of your website.</p> <p>Create Markdown Files:</p> <p>Write your content in Markdown format. You can create multiple files and organize them into directories as needed.</p> <p>Customization and Extensions</p> <p>Material for MkDocs supports extensive customization. Edit your <code>mkdocs.yml</code> to include features like a search bar, social media links, or Google Analytics. MkDocs also supports plugins, which can add functionalities like search engine optimization, PDF export, and more.</p> <p>Configure the Website</p> <p>Editing <code>mkdocs.yml</code> customizes your website\u2019s structure. Define your site name, theme (Material), and navigation structure. </p> <p>Here\u2019s a basic example:</p> <pre><code>site_name: My Awesome Project\ntheme: \n  name: material\nnav:\n  - Home: index.md\n  - About: about.md\n</code></pre> <p>Explore the Material for MkDocs documentation for customization options and add them to your <code>mkdocs.yml</code>.</p>"},{"location":"articles/mkdocs-setup/#step-5-building-and-deploying-your-site","title":"Step 5: Building and Deploying Your Site","text":"<p>Build Your Site:</p> <p>When you're ready to publish, build a static site with:</p> <pre><code>mkdocs build\n</code></pre> <p>This command compiles your Markdown files into a static HTML website in the <code>site</code> directory.</p> <p>This creates a <code>site</code> directory with your website\u2019s static HTML files. For deployment, you can upload these files to a web server or use services like GitHub Pages for hosting. MkDocs provides a simple command for deploying to GitHub Pages:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Deploy Your Site:</p> <p>Choose a hosting solution (e.g., GitHub Pages, GitLab Pages or any other hosting solution) and follow their instructions to deploy your MKDocs site.</p>"},{"location":"articles/mkdocs-setup/#step-6-customization-home-page","title":"Step 6: Customization Home Page","text":"<p>Your home page is the first thing visitors see. Make it informative and engaging. Edit the <code>index.md</code> file in the <code>docs</code> folder to add content. </p> <p>This is the file you are looking for: <code>overrides/home.html</code>. You'll want to copy it over to your own overrides directory. Make sure you've set your custom_dir in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  custom_dir: docs/overrides\n...\n</code></pre> <p>In the front matter of your <code>index.md</code>, you need to specify the template to use:</p> <pre><code>---\ntitle: Title\ntemplate: home.html\n---\n</code></pre> <p>Note</p> <p>One important thing that took me a while to realize: you need a newline at the end of your md file. If you don't have one, the content will not display. I guess it's processed as having null content if you don't include the newline.</p> <p></p>"},{"location":"articles/mkdocs-setup/#conclusion","title":"Conclusion","text":"<p>Material for MkDocs is an excellent tool for IT professionals and tech writers to create and maintain documentation websites. With its easy-to-use features and extensive customization options, it's an ideal choice for documenting software project documentation, writing online e-Book, and more.</p>"},{"location":"articles/mkdocs-setup/#references","title":"References","text":"<ul> <li>Material for MkDocs</li> <li>MkDocs </li> <li>Getting Started with MkDocs</li> </ul>"},{"location":"articles/namespace-stuck/","title":"A Kubernetes Namespace Stuck in the Terminating State","text":""},{"location":"articles/namespace-stuck/#symptom","title":"Symptom","text":"<p>If you are experiencing issues with deleting namespaces in Kubernetes (AKS). When we execute the <code>kubectl delete ns</code> command, it becomes unresponsive and gets stuck in the terminating state. Even if we try to abort the operation, it remains stuck in this state indefinitely.</p> <p>Symptom is - A Kubernetes namespace is stuck in the Terminating state.</p>"},{"location":"articles/namespace-stuck/#root-cause","title":"Root Cause","text":"<p><code>Finalizer Issue</code></p> <p>Finalizers are mechanisms that allow resources to perform cleanup actions before they're deleted. If a resource has a finalizer that fails to complete its task, it can block the deletion of the entire namespace.</p>"},{"location":"articles/namespace-stuck/#resolving-the-problem","title":"Resolving the Problem","text":"<p>To resolve this issue, you'll need to identify and manually delete  a Terminating Namespace from finalizers.</p> <ol> <li> <p>View the namespaces that are stuck in the Terminating state:</p> <pre><code>kubectl get namespaces\n</code></pre> </li> <li> <p>Select a terminating namespace and view the contents of the namespace to find out the finalizer:</p> <pre><code>kubectl get namespace &lt;terminating-namespace&gt; -o yaml\n</code></pre> <p>Your YAML contents might resemble the following output:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: 2018-11-19T18:48:30Z\n  deletionTimestamp: 2018-11-19T18:59:36Z\n  name: &lt;terminating-namespace&gt;\n  resourceVersion: \"1385077\"\n  selfLink: /api/v1/namespaces/&lt;terminating-namespace&gt;\n  uid: b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Terminating\n</code></pre> </li> <li> <p>Create a temporary JSON file:</p> <pre><code>kubectl get namespace &lt;terminating-namespace&gt; -o json &gt;tmp.json\n</code></pre> </li> <li> <p>Edit your <code>tmp.json</code> file. Remove the <code>kubernetes</code> value from the <code>finalizers</code> field and save the file.</p> <p>Your <code>tmp.json</code> file might resemble the following output:</p> <pre><code>{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Namespace\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2018-11-19T18:48:30Z\",\n        \"deletionTimestamp\": \"2018-11-19T18:59:36Z\",\n        \"name\": \"&lt;terminating-namespace&gt;\",\n        \"resourceVersion\": \"1385077\",\n        \"selfLink\": \"/api/v1/namespaces/&lt;terminating-namespace&gt;\",\n        \"uid\": \"b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\"\n    },\n    \"spec\": {\n        \"finalizers\": \n    },\n    \"status\": {\n        \"phase\": \"Terminating\"\n    }\n}\n</code></pre> </li> <li> <p>To set a temporary proxy IP and port, run the following command. Be sure to keep your terminal window open until you delete the stuck namespace:</p> <pre><code>kubectl proxy\n</code></pre> <p>Your proxy IP and port might resemble the following output:</p> <pre><code>Starting to serve on 127.0.0.1:8001\n</code></pre> </li> <li> <p>From a new terminal window, make an API call with your temporary proxy IP and port:</p> <pre><code>curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/&lt;terminating-namespace&gt;/finalize\n</code></pre> <p>Your output might resemble the following content:</p> <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"&lt;terminating-namespace&gt;\",\n    \"selfLink\": \"/api/v1/namespaces/&lt;terminating-namespace&gt;/finalize\",\n    \"uid\": \"b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\",\n    \"resourceVersion\": \"1602981\",\n    \"creationTimestamp\": \"2018-11-19T18:48:30Z\",\n    \"deletionTimestamp\": \"2018-11-19T18:59:36Z\"\n  },\n  \"spec\": {\n\n  },\n  \"status\": {\n    \"phase\": \"Terminating\"\n  }\n}\n</code></pre> <p>Note: The finalizer parameter is removed.</p> </li> <li> <p>Verify that the terminating namespace is removed:</p> <pre><code>kubectl get namespaces\n</code></pre> </li> </ol>"},{"location":"articles/namespace-stuck/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to successfully resolve the issue of Kubernetes namespace stuck in the terminating state and delete namespace.</p>"},{"location":"articles/oatuh2.0-flows/","title":"Single Sign-On - OAuth 2.0 flows","text":""},{"location":"articles/oatuh2.0-flows/#introduction","title":"Introduction","text":"<p>OAuth 2.0 defines several authorization flows, also known as <code>grant types</code>, to enable different use cases for securing access to resources. The choice of which OAuth 2.0 flow to use depends on the specific requirements and characteristics of the client application and the desired level of security. Here are the main OAuth 2.0 flows:</p> <p>In this article, I will explain the four distinct OAuth 2.0 grant types, providing a detailed exploration complete with sequence diagrams and interactive examples.  we'll also look into the interactions between the various entities involved and understanding of each grant type in details. </p>"},{"location":"articles/oatuh2.0-flows/#authorization-code-flow-authorization-code-grant","title":"Authorization Code Flow (Authorization Code Grant)","text":"<ul> <li>Use Case: Suitable for web applications running on a server.</li> <li>Flow:<ol> <li>The client redirects the user to the authorization server's authorization endpoint.</li> <li>The user authenticates and approves the requested permissions.</li> <li>The authorization server issues an authorization code and redirects the user back to the client with the code.</li> <li>The client exchanges the authorization code for an access token and, optionally, a refresh token by making a direct request to the authorization server's token endpoint.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Authorization Code Flow:</p> <p>This sequence diagram illustrates the interactions between the client, user, authorization server, and resource server in the Authorization Code Flow. The numbers represent the sequential order of the steps.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant User\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Initiate Authorization Request\n  AuthorizationServer--&gt;&gt;User: 2. Present Login Screen\n  User-&gt;&gt;AuthorizationServer: 3. Enter Credentials\n  AuthorizationServer--&gt;&gt;User: 4. Consent Screen\n  User-&gt;&gt;AuthorizationServer: 5. Grant Permissions\n  AuthorizationServer--&gt;&gt;Client: 6. Redirect with Authorization Code\n  Client-&gt;&gt;AuthorizationServer: 7. Exchange Authorization Code for Tokens\n  AuthorizationServer--&gt;&gt;Client: 8. Access Token, Refresh Token\n  Client-&gt;&gt;ResourceServer: 9. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 10. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Initiate Authorization Request (Client to Authorization Server): The client initiates the authorization process by redirecting the user to the authorization server's authorization endpoint.</p> </li> <li> <p>Present Login Screen (Authorization Server to User): The authorization server presents a login screen to the user to enter their credentials.</p> </li> <li> <p>Enter Credentials (User to Authorization Server): The user enters their credentials.</p> </li> <li> <p>Consent Screen (Authorization Server to User): If necessary, the authorization server presents a consent screen to the user to grant permissions to the client.</p> </li> <li> <p>Grant Permissions (User to Authorization Server): The user grants the requested permissions.</p> </li> <li> <p>Redirect with Authorization Code (Authorization Server to Client): The authorization server redirects the user back to the client with an authorization code.</p> </li> <li> <p>Exchange Authorization Code for Tokens (Client to Authorization Server): The client exchanges the received authorization code for an access token and, optionally, a refresh token.</p> </li> <li> <p>Access Token, Refresh Token (Authorization Server to Client): The authorization server responds with the access token and refresh token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#implicit-flow-implicit-grant","title":"Implicit Flow (Implicit Grant)","text":"<ul> <li>Use Case: Designed for user-agent-based clients (e.g., single-page applications) that cannot keep a client secret confidential.</li> <li>Flow:<ol> <li>The client redirects the user to the authorization server's authorization endpoint.</li> <li>The user authenticates and approves the requested permissions.</li> <li>The authorization server issues an access token directly in the redirect URI fragment.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Implicit Flow:</p> <p>This sequence diagram illustrates the interactions between the client, user, authorization server, and resource server in the Implicit Flow. The numbers represent the sequential order of the steps.</p> <p>Note</p> <p>Note that in the Implicit Flow, the access token is returned directly to the client in the URL fragment.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant User\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Initiate Authorization Request\n  AuthorizationServer--&gt;&gt;User: 2. Present Login Screen\n  User-&gt;&gt;AuthorizationServer: 3. Enter Credentials\n  AuthorizationServer--&gt;&gt;User: 4. Consent Screen\n  User-&gt;&gt;AuthorizationServer: 5. Grant Permissions\n  AuthorizationServer--&gt;&gt;Client: 6. Redirect with Access Token (in URL fragment)\n  Client-&gt;&gt;ResourceServer: 7. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 8. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Initiate Authorization Request (Client to Authorization Server): The client initiates the authorization process by redirecting the user to the authorization server's authorization endpoint.</p> </li> <li> <p>Present Login Screen (Authorization Server to User): The authorization server presents a login screen to the user to enter their credentials.</p> </li> <li> <p>Enter Credentials (User to Authorization Server): The user enters their credentials.</p> </li> <li> <p>Consent Screen (Authorization Server to User): If necessary, the authorization server presents a consent screen to the user to grant permissions to the client.</p> </li> <li> <p>Grant Permissions (User to Authorization Server): The user grants the requested permissions.</p> </li> <li> <p>Redirect with Access Token (in URL fragment) (Authorization Server to Client): The authorization server redirects the user back to the client with an access token directly included in the URL fragment.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol> <p>Here is the simplified example of sending an OpenID Connect authentication request to an Active Directory Federation Services (AD FS) endpoint and obtaining both the <code>id_token</code> and <code>access token</code>.</p> <p>OpenID Connect Authentication Request:</p> <p>Assuming your AD FS server is hosted at <code>https://adfs.example.com</code> and your client application is registered with the client ID <code>your-client-id</code>.</p> <p>The OpenID Connect authentication request might look like this:</p> <pre><code>GET https://adfs.example.com/adfs/oauth2/authorize\n    ?response_type=id_token token\n    &amp;client_id=your-client-id\n    &amp;redirect_uri=https://your-app-callback-url.com\n    &amp;scope=openid\n    &amp;nonce=your-nonce-value\n    &amp;response_mode=fragment\n    &amp;state=your-state-value\n</code></pre> <p>Explanation of parameters:</p> <ul> <li><code>response_type=id_token token</code>: Request both an ID token and an access token.</li> <li><code>client_id</code>: The client identifier registered with AD FS.</li> <li><code>redirect_uri</code>: The callback URL where AD FS will redirect the user after authentication.</li> <li><code>scope=openid</code>: Request OpenID Connect authentication.</li> <li><code>nonce</code>: A unique value to mitigate replay attacks.</li> <li><code>response_mode=form_post</code>: The response is sent as a form post.</li> <li><code>state</code>: A value used to maintain state between the request and the callback.</li> </ul> <p>response_mode=<code>fragment</code> or <code>form_post</code></p> <ul> <li><code>response_mode=fragment</code>: Specify that the response should be included in the URL fragment.</li> </ul> <p>OpenID Connect Authentication Response:</p> <p>Assuming a successful authentication, AD FS will redirect the user back to your specified <code>redirect_uri</code> with the tokens. The response might look like this:</p> <pre><code>POST https://your-app-callback-url.com\nContent-Type: application/x-www-form-urlencoded\n\nid_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjIwMTYtMDYtMjNUMTY6MDY6MDMuMjA1In0.eyJzdWIiOiIxMDAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEyMzQ1Njc4OTAxMjMiLCJhdWQiOiJodHRwczovL2V4YW1wbGUuY29tIiwibm9uY2UiOiJ5b3VyLW5vbmNlLXZhbHVlIiwiaXNzIjoiaHR0cHM6Ly9hZGZzLmV4YW1wbGUuY29tIiwiaWF0IjoxNjA5OTY1NTk3LCJleHAiOjE2MDk5NjU5OTcsImF0X2hhc2giOiJiZHNEdHBJZzRGSWtIOWw5elVZOWlnIiwic2lkIjoiYmNkS2JzWlVzQzZTUGwyazd6cTZsQT09In0.yu0uIwXVCMvZaLOdNbPiPbdgI90r-IA0Iy-l6QhH1ZyDrxP9dQAn6qGmBHXJrO15sZb3asHsqj6f3_7pVl7DFDDZXzHKFEHLJfR0deZ0OHoNlgUklrxr7tmqqTw07EYsOa_9CIsZD9id0PCTDAm0ZIyakO9BCL44O0UyvjNlHtMvXV8W4N24vQGEGjw0Cx2Nm7c__HZxS_5H0rUJL2FXFjjXgDrNhEFGjGziGjbOXwxzWc_W2AM-g-buQsN8wHw5kv8vh7mjPXYkAlKJWAKHHek5XlQDljJbWz7R1w5CfQ5MQ7CzrqX62NfeXeWZsGMKdfnAVQLkkMaOqA\n\n&amp;access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjIwMTYtMDYtMjNUMTY6MDY6MDMuMjA1In0.eyJleHAiOjE2MDk5NjYxOTcsImF1ZCI6Imh0dHBzOi8vZXhhbXBsZS5jb20iLCJpc3MiOiJodHRwczovL2FkZnMuZXhhbXBsZS5jb20iLCJzdWIiOiIxMDAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEyMzQ1Njc4OTAxMjMiLCJhdWQiOiJodHRwczovL2V4YW1wbGUuY29tIiwianRpIjoiNjQyMjZkM2EtN2YyNi00ZjMxLWEzMDktZDM0Y2MxNzJhNTlkIiwiaWF0IjoxNjA5OTY1NTk3fQ.DGtIlQkyqmq6ZR0bF61F5qGvBX3XXayRvdx5tQQziP-t4_1f0Y6Vl5bkYuf6hI3PYINxlWs5cM1bhecvqSo-gegqHvz7EJzj7-03YSo-01KuTLm9VyISgo6XstGLyDOFhBr7v1agx2pM7OAz8AER8sI1AmJJ1-fE0P1o7j-jd-v1cD7I8KckACin64S8b8arIjiLiluZbj3TNg1YJf7Xa5nb5oQ96VCzw7BYBwtP9bu0l5YyQ4ILVsj_yw8OxFf0KXdLpF97QvaR-Iu3IwYXZ2sSZFzRUbxjoCJqkQzvldIBK7pFlHkTTrjZLgGhZS5WuR5kZm57pSbpmrOqCtHA\n</code></pre> <p>Explanation of parameters:</p> <ul> <li><code>id_token</code>: The ID token containing user information.</li> <li><code>access_token</code>: The access token used to access protected resources.</li> </ul> <p>Note</p> <p>The actual endpoints and parameters may vary based on your AD FS configuration and the OpenID Connect implementation.</p>"},{"location":"articles/oatuh2.0-flows/#resource-owner-password-credentials-flow","title":"Resource Owner Password Credentials Flow","text":"<ul> <li>Use Case: Suitable for trusted clients that can directly request and obtain the user's credentials.</li> <li>Flow:<ol> <li>The client directly requests the user's credentials (e.g., username and password).</li> <li>The client uses the user's credentials to request an access token directly from the authorization server.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Resource Owner Password Credentials Flow:</p> <p>This sequence diagram illustrates the interactions between the client, authorization server, and resource server in the Resource Owner Password Credentials Flow. </p> <p>Note</p> <p>It's important to note that this flow involves the client collecting and transmitting the user's credentials, so it should only be used by highly trusted clients, and it's generally not recommended for public or untrusted clients.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Request Access Token\n  AuthorizationServer--&gt;&gt;Client: 2. Request User Credentials\n  Client-&gt;&gt;AuthorizationServer: 3. User Credentials\n  AuthorizationServer--&gt;&gt;Client: 4. Access Token, Refresh Token\n  Client-&gt;&gt;ResourceServer: 5. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 6. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Request Access Token (Client to Authorization Server): The client requests an access token directly from the authorization server by providing its client credentials and the resource owner's (user's) credentials.</p> </li> <li> <p>Request User Credentials (Authorization Server to Client): The authorization server requests the user credentials (username and password) from the client.</p> </li> <li> <p>User Credentials (Client to Authorization Server): The client provides the user credentials (username and password) to the authorization server.</p> </li> <li> <p>Access Token, Refresh Token (Authorization Server to Client): Upon successful authentication, the authorization server responds with an access token and, optionally, a refresh token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#client-credentials-flow","title":"Client Credentials Flow","text":"<ul> <li>Use Case: Used when the client is the resource owner and wants to access its own resources.</li> <li>Flow:<ol> <li>The client directly requests an access token from the authorization server using its own credentials.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Client Credentials Flow:</p> <p>This sequence diagram illustrates the interactions between the client, authorization server, and resource server in the Client Credentials Flow. The Client Credentials Flow is used when the client is the resource owner and wants to access its own resources. It does not involve user authentication, making it suitable for machine-to-machine communication where the client is acting on its own behalf.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Request Access Token\n  AuthorizationServer--&gt;&gt;Client: 2. Respond with Access Token\n  Client-&gt;&gt;ResourceServer: 3. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 4. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Request Access Token (Client to Authorization Server): The client directly requests an access token from the authorization server by providing its client credentials (client ID and client secret).</p> </li> <li> <p>Respond with Access Token (Authorization Server to Client): The authorization server responds with an access token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the obtained access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#conclusion","title":"Conclusion","text":"<p>OAuth 2.0 plays a crucial role in enabling Single Sign-On and secure authorization across various applications and services. The choice of the appropriate flow depends on the specific requirements of the client application, the level of security needed, and the characteristics of the user interaction. Continuous attention to security best practices is essential for maintaining a robust and trustworthy authentication and authorization system.</p>"},{"location":"articles/oatuh2.0-flows/#references","title":"References","text":"<ul> <li>AD FS OpenID Connect/OAuth flows and Application Scenarios</li> <li>OAuth 2.0 Specification</li> <li>OpenID Connect Specifications</li> <li>Security Assertion Markup Language (SAML)</li> <li>Mermaidv10.6.1 Live Editor</li> <li></li> </ul>"},{"location":"articles/pv-pvc-stuck/","title":"Delete PV(Persistent Volume) and PVC(Persistent Volume Claim) stuck in terminating state","text":"<p>If you are experiencing issues with deleting <code>Persistent Volume</code> or <code>Persistent Volume Claim</code> in Kubernetes (AKS). When we execute the <code>kubectl delete pv</code> or <code>kubectl delete pvc</code> command, it becomes unresponsive and gets stuck in the terminating state. Even if we try to abort the operation, it remains stuck in this state indefinitely.</p> <p>In this article, we will explore the symptoms, root cause, and the proper resolution to overcome this challenge.</p>"},{"location":"articles/pv-pvc-stuck/#symptom","title":"Symptom","text":"<p>Symptom is - A Kubernetes Persistent Volume or Persistent Volume Claim is stuck in the Terminating state because of not following the order during the deletion.</p> <p>The deletion of PV(Persistent Volume) and PVC(Persistent Volume claim) needs to follow in specific order else you will get stuck in terminatiion state. When you are planning to delete the Persistent Volume as well as Persistent Volume Claim then you must follow an order -</p> <ul> <li>First delete - Persistent Volume Claim</li> <li>Second delete- Persistent Volume</li> </ul> <p>Note</p> <p>You should never delete PV(Persistent Volume) without deleting its PVC(Persistent Volume Claim))</p>"},{"location":"articles/pv-pvc-stuck/#root-cause","title":"Root Cause","text":"<p><code>Finalizer Issue</code></p> <p>Each Kubernetes resource running in the cluster has Finalizers associated with it. Finalizers prevent accidental deletion of resources(Persistent Volume, Persistent Volume Claim), If you accidentally issue kubectl delete command on Kubernetes resource and if there is a finalizer associated with that resource then it is going to put the resource in <code>Read-Only</code> mode and prevent it from deletion.</p>"},{"location":"articles/pv-pvc-stuck/#resolving-the-problem","title":"Resolving the Problem","text":"<p>If you find your Kubernetes resources, such as Persistent Volumes (PVs) or Persistent Volume Claims (PVCs), stuck in the terminating state, resolving the issue involves removing the associated <code>Finalizer</code>. Follow these steps to successfully resolve the problem:</p>"},{"location":"articles/pv-pvc-stuck/#step-1-retrieve-information-about-resources","title":"Step-1: Retrieve information about resources","text":"<p>Use the following commands to retrieve information about your PVs and PVCs:</p> <pre><code># Get Persistent Volume Claims in a specific namespace\nkubectl get pvc -n namespace1\n\n# Get Persistent Volumes in a specific namespace\nkubectl get pv -n namespace1\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#step-2-remove-the-finalizer","title":"Step-2: Remove the Finalizer","text":"<p>Execute the following commands to remove the Finalizer from both the Persistent Volume and Persistent Volume Claim:</p> <pre><code># Patch the Persistent Volume to remove the Finalizer\nkubectl patch pv sample-app-pv -p '{\"metadata\":{\"finalizers\":null}}' -n namespace1\n\n# Patch the Persistent Volume Claim to remove the Finalizer\nkubectl patch pvc sample-app-pvc -p '{\"metadata\":{\"finalizers\":null}}' -n namespace1\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#step-3-delete-resources","title":"Step-3: Delete Resources","text":"<p>Now that the Finalizer has been removed, proceed to delete the resources in the correct order:</p> <p>Delete Persistent Volume Claim</p> <pre><code># Delete Persistent Volume Claim\nkubectl delete pvc sample-app-pvc -n namespace1\n</code></pre> <p>Delete Persistent Volume</p> <p>After the successful deletion of the Persistent Volume Claim, proceed to delete the Persistent Volume:</p> <pre><code># Delete Persistent Volume\nkubectl delete pv sample-app-pv -n namespace1\n</code></pre> <p>or</p> <p>Force Delete (if needed)</p> <p>If the regular deletion commands do not work, you can use the force delete option:</p> <pre><code># Force delete Persistent Volume Claim\nkubectl delete pvc --grace-period=0 --force --namespace namespace1 sample-app-pvc\n\n# Force delete Persistent Volume\nkubectl delete pv --grace-period=0 --force --namespace namespace1 sample-app-pv\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to successfully resolve the issue of Kubernetes resources stuck in the terminating state and delete both Persistent Volumes and Persistent Volume Claims.</p>"},{"location":"articles/single-sign-on/","title":"Single Sign-On - Introduction","text":""},{"location":"articles/single-sign-on/#introduction","title":"Introduction","text":"<p>If you're involved in implementing Single Sign-On (SSO) in your project, it's crucial to understand the fundamental concepts that will help you to implement the authentication mechanism. </p> <p>In this article, we'll explain the foundational elements of Single Sign-On. By looking into these core concepts, you'll gain a clear understanding that will help to implement Single Sign-On in your own application.</p> <p>Whether you're new to SSO or seeking a refresher, this exploration will help you with the essential knowledge needed to review the complexities of Single Sign-On and make informed decisions during the implementation process. </p>"},{"location":"articles/single-sign-on/#what-is-single-sign-on","title":"What is Single Sign-On?","text":"<p><code>Single Sign-On (SSO)</code> is a user authentication process that enables a user to access multiple applications with a single set of login credentials. Instead of requiring users to remember separate usernames and passwords for each application, SSO streamlines the login process by authenticating the user once and granting access to multiple services.</p>"},{"location":"articles/single-sign-on/#waht-are-the-benefits-of-sso","title":"Waht are the benefits of SSO?","text":"<p>Single Sign-On (SSO) offers several benefits for both users and organizations. Here are some key advantages of implementing SSO:</p> <ul> <li> <p>Enhanced User Experience: Users only need to remember and enter one set of credentials to access multiple applications and services, leading to a smoother and more efficient user experience.</p> </li> <li> <p>Time and Productivity Savings: SSO reduces the need for users to repeatedly log in, saving time and minimizing disruptions to workflow. This can result in increased productivity across an organization.</p> </li> <li> <p>Minimized Password Reset Requests: With fewer passwords to manage, organizations typically experience a decrease in password-related support requests and help desk calls, leading to cost savings.</p> </li> <li> <p>Centralized Access Management: SSO allows for centralized control and management of user access. Changes in user permissions or account status can be applied uniformly across all connected applications.</p> </li> <li> <p>Enhanced Security: SSO can enhance security by enforcing consistent authentication policies. Users are less likely to resort to insecure practices, such as writing down passwords, when dealing with multiple credentials.</p> </li> </ul>"},{"location":"articles/single-sign-on/#how-single-sign-on-works","title":"How Single Sign-On works?","text":"<p>Single Sign-On (SSO) works by allowing users to authenticate once and gain access to multiple applications or services without the need to re-enter credentials for each one. The fundamental idea is to streamline the authentication process and provide a seamless user experience across various systems. Here's a high-level overview of how SSO works:</p> <ol> <li> <p>User Attempts to Access a Service: </p> <ul> <li>When a user attempts to access an application or service that is part of the SSO system, they are redirected to the SSO system for authentication.</li> </ul> </li> <li> <p>SSO Authentication Request:</p> <ul> <li>The SSO system initiates an authentication request, prompting the user to provide their credentials (username and password) or use alternative authentication methods like multi-factor authentication (MFA).</li> </ul> </li> <li> <p>User Authentication:</p> <ul> <li>The user enters their credentials or completes the required authentication steps. The SSO system verifies the user's identity.</li> </ul> </li> <li> <p>Issuance of Authentication Token:</p> <ul> <li>Upon successful authentication, the SSO system issues an authentication token. This token serves as proof of the user's identity and is often in the form of a secure token like a JSON Web Token (JWT).</li> </ul> </li> <li> <p>Token Exchange (Optional):</p> <ul> <li>In some cases, depending on the SSO protocol used (such as OAuth 2.0 or SAML), the authentication token may be exchanged for an access token or a service-specific token.</li> </ul> </li> <li> <p>SSO Session Management:</p> <ul> <li>The SSO system manages the user's session, keeping track of the user's authenticated state. This session information is used to facilitate access to other services without additional authentication.</li> </ul> </li> <li> <p>User Accesses Another Service:</p> <ul> <li>If the user decides to access another application or service within the SSO environment, the SSO system recognizes the user's existing session and provides access without requesting credentials again.</li> </ul> </li> <li> <p>Logout Handling (Optional):</p> <ul> <li>When the user logs out, the SSO system can handle the logout process by terminating the user's session across all connected services. This ensures a complete logout experience.</li> </ul> </li> <li> <p>Token Expiry and Refresh (Optional):</p> <ul> <li>Authentication tokens may have a limited validity period. If needed, the SSO system can handle token expiration by either requiring re-authentication or using mechanisms like token refresh to obtain a new token without requiring the user's credentials.</li> </ul> </li> </ol>"},{"location":"articles/single-sign-on/#what-are-the-different-types-of-sso","title":"What are the different types of SSO?","text":"<p>Single Sign-On (SSO) comes in various types, depends on different use cases. Here are some common types of SSO:</p> <ul> <li> <p>Enterprise SSO: Primarily used within an organization, Enterprise SSO allows users to access various internal systems and applications using a single set of credentials. It enhances security and simplifies user management for IT administrators.</p> </li> <li> <p>Web SSO: Web SSO extends the SSO concept to web applications. Users can log in once to access multiple web services, making it prevalent in online platforms, social media, and cloud-based applications.</p> </li> <li> <p>Federated SSO: Federated SSO enables users to access resources across multiple organizations or domains. It relies on trust relationships between identity providers, allowing for seamless authentication in a distributed environment.</p> </li> </ul>"},{"location":"articles/single-sign-on/#popular-sso-protocols","title":"Popular SSO Protocols","text":"<p>Several popular Single Sign-On (SSO) protocols are widely used to implement SSO across various applications and services. Here are some of the most common SSO protocols:</p> <ul> <li> <p>OAuth 2.0: OAuth 2.0 is a widely adopted authorization framework that allows a user to grant a third-party application limited access to their resources without exposing their credentials. While OAuth 2.0 itself is not an authentication protocol, it is often used in conjunction with OpenID Connect to achieve both authentication and authorization in an SSO scenario.</p> </li> <li> <p>OpenID Connect: OpenID Connect is an identity layer built on top of OAuth 2.0. It extends OAuth 2.0 to provide a standardized way for clients to request and receive identity information about users. OpenID Connect is specifically designed for authentication and is commonly used for SSO.</p> </li> <li> <p>SAML (Security Assertion Markup Language): SAML is an XML-based standard for exchanging authentication and authorization data between parties, particularly in a web browser environment. It allows for the secure transfer of user identity information between an identity provider (IdP) and a service provider (SP), facilitating SSO.</p> </li> </ul>"},{"location":"articles/single-sign-on/#roles-of-single-sign-on","title":"Roles of Single Sign-On","text":"<p>The Single Sign-On (SSO) process involves several roles or components working together in a coordinated fashion to provide a secure, seamless, and efficient Single Sign-On experience for users across multiple applications and services. The main roles or components include:</p> <p>User (Resource Owner):</p> <ul> <li>Role: The end-user or resource owner is the individual seeking access to various applications and services without the need for multiple logins.</li> <li>Interaction: Initiates the authentication process by attempting to access a resource or application.</li> </ul> <p>Identity Provider (IdP):</p> <ul> <li>Role: The Identity Provider is responsible for authenticating the user and asserting their identity to other applications or services.</li> <li>Interaction: Verifies user credentials and issues authentication tokens (such as SAML assertions or JWTs) to signify a successful authentication.</li> </ul> <p>Service Provider (SP) / Resource Server::</p> <ul> <li>Role: The Service Provider is the application or service that the user wants to access. It relies on the Identity Provider's assertion to grant access to the user.</li> <li>Interaction: Receives the authentication token from the user and validates it with the Identity Provider to authorize access.</li> </ul> <p>Authorization Server:</p> <ul> <li>Role: In OAuth-based SSO systems, the Authorization Server is responsible for granting access tokens to client applications.</li> <li>Interaction: Issues access tokens after authenticating the user and obtaining their consent. The client application uses these tokens to access protected resources.</li> </ul>"},{"location":"articles/single-sign-on/#tokens-used-in-sso","title":"Tokens used in SSO","text":"<p>Let's explore the two types of tokens, <code>identity tokens</code>, and <code>access tokens</code>, delivered by the Authorization Server, both of which are commonly presented in the form of JSON Web Tokens (JWTs):</p> <p>ID Tokens:</p> <ul> <li>Purpose: ID tokens are primarily used in the context of OpenID Connect. They are meant to carry information about the authentication of the user.</li> <li>Content: An ID token contains claims about the identity of the authenticated user, such as their user ID, username, and possibly other information like email or profile information.</li> <li>Usage: ID tokens are typically used by the client application to obtain information about the authenticated user. They are not used to access protected resources but rather to identify the user.</li> <li>Example Scenario: After a user logs in through an OpenID Connect provider, the provider issues an ID token, which the client application can use to get information about the authenticated user.</li> </ul> <p>Access Tokens:</p> <ul> <li>Purpose: Access tokens are used to access protected resources on behalf of a user.</li> <li>Content: An access token represents the authorization granted to a client application to access specific resources on behalf of the user. It may contain information about the scope of access, expiration time, and other details.</li> <li>Usage: Access tokens are presented by the client to the resource server to gain access to protected resources. They are used in API calls to demonstrate that the client has been authorized to access the requested resources.</li> <li>Example Scenario: A user logs in and grants permission to a third-party application to access their profile information on a social media platform. The application receives an access token that it can then use to make API requests to retrieve the user's profile data.</li> </ul> <p>Example Identity Token (JWT): <pre><code>{\n  \"iss\": \"https://openid-provider.com\",\n  \"sub\": \"1234567890\",\n  \"aud\": \"your-client-id\",\n  \"exp\": 1632969781,\n  \"iat\": 1632966181,\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"picture\": \"https://example.com/john-doe.jpg\",\n  \"nonce\": \"nonce-value\"\n}\n</code></pre></p> <p>Token data details:</p> <ul> <li><code>sub</code> (Subject): Identifies the subject of the token (e.g., user ID).</li> <li><code>iss</code> (Issuer): Specifies the issuer of the token (e.g., Authorization Server).</li> <li><code>aud</code> (Audience): Indicates the audience for which the token is intended (e.g., client application or resource server).</li> <li><code>exp</code> (Expiration Time): Specifies the expiration time of the token.</li> <li><code>iat</code> (Issued At): Indicates the time at which the token was issued.</li> </ul> <p>Access Tokens:</p> <ul> <li>Description: Access tokens are used by the client application to access protected resources on behalf of the user. They represent the authorization granted by the user.</li> <li>Contents: Access tokens often include information about the granted permissions, scope, and expiration time.</li> <li>Use Case: Access tokens are presented by the client application to the resource server when making requests for protected resources. They serve as a proof of authorization.</li> </ul> <p>Example Access Token (JWT): <pre><code>{\n  \"iss\": \"https://authorization-server.com\",\n  \"sub\": \"1234567890\",\n  \"aud\": [\"https://api.example.com\", \"https://resources.example.com\"],\n  \"exp\": 1632969781,\n  \"iat\": 1632966181,\n  \"scope\": \"read write\",\n  \"jti\": \"a1b2c3d4e5f6\"\n}\n</code></pre> simplified example of what an access token might look like:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"read write\",\n  \"refresh_token\": \"rT_5f9d2a96-7ae3-4eae-b64d-6375942b9c2a\"\n}\n</code></pre> <p>Refresh Tokens:</p> <p>In Single Sign-On (SSO) systems, refresh tokens play a crucial role in extending the validity of access tokens and maintaining a user's authenticated session without requiring repeated user authentication. </p> <p>Access tokens, which grant access to resources, typically have a limited lifespan for security reasons. Once an access token expires, the user would need to re-authenticate to obtain a new one.</p> <p>When a user initially authenticates and obtains an access token, the Identity Provider or Authorization Server may also issue a refresh token along with the access token.</p> <p>These tokens play a crucial role in the OAuth 2.0 and OpenID Connect protocols, providing a secure and standardized way to convey identity and authorization information in a distributed and interoperable manner.</p>"},{"location":"articles/single-sign-on/#oauth-20-flows","title":"OAuth 2.0 flows","text":"<p>OAuth 2.0 flows, also known as OAuth 2.0 grant types, define the mechanism through which applications obtain authorization and access tokens to interact with protected resources on behalf of a user. Each flow is designed for specific use cases and scenarios, providing a standardized way for clients (applications) to request and obtain access to resources.</p> <p>The main OAuth 2.0 flows include:</p> <p>Authorization Code Grant Flow:</p> <ul> <li>Use Case: Web applications with a server-side component.</li> <li>Description: Involves the redirection of the user to the authorization server, where the user authenticates and grants permission. The authorization server returns an authorization code to the client, which is then exchanged for an access token.</li> </ul> <p>Implicit Grant Flow:</p> <ul> <li>Use Case: Browser-based applications (JavaScript applications).</li> <li>Description: Designed for client-side applications running in the user's browser. The access token is issued directly to the client without the need for an intermediate authorization code exchange.</li> </ul> <p>Client Credentials Grant Flow:</p> <ul> <li>Use Case: Confidential clients, such as backend servers or applications that can securely store client credentials.</li> <li>Description: The client (usually a server) directly requests an access token from the authorization server using its client credentials (client ID and secret). This flow is suitable for machine-to-machine communication.</li> </ul> <p>Resource Owner Password Credentials Grant Flow:</p> <ul> <li>Use Case: Highly trusted applications, such as native mobile apps.</li> <li>Description: Involves the resource owner (user) providing their username and password directly to the client. The client then uses these credentials to obtain an access token from the authorization server.</li> </ul> <p>These flows provide flexibility and cater to different types of applications and security requirements. The choice of a specific flow depends on the characteristics of the client application, the level of trust, and the security considerations of the overall system architecture.</p>"},{"location":"articles/single-sign-on/#json-web-token-jwt","title":"JSON Web Token (JWT)","text":"<p>JWTs are often used for authentication and authorization purposes in web applications and APIs. They can be sent between parties, and since they are self-contained, the recipient can verify the information within the token without needing to contact the issuer. JWTs are widely used in various protocols and frameworks, including OAuth 2.0 and OpenID Connect.</p> <p>JWTs are defined by the RFC 7519 standard and consist of three parts:</p> <p>1. Header: The header typically consists of two parts: the type of the token (JWT) and the signing algorithm being used, such as HMAC SHA256 or RSA.</p> <p>Example:    <pre><code>{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n</code></pre></p> <p>2. Payload: The payload contains the claims. Claims are statements about an entity (typically, the user) and additional data.</p> <p>Example:    <pre><code>{\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"iat\": 1516239022\n}\n</code></pre></p> <p>3. Signature: To create the signature part, you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that.</p> <p>Example (using HMAC SHA256): <pre><code>HMACSHA256(\n    base64UrlEncode(header) + \".\" +\n    base64UrlEncode(payload),\n    secret)\n</code></pre></p> <p>The resulting JWT looks like this: <pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwMCIsIm5hbWUiOiJKb2huIERvZSIsImlhdCI6MTUxNjIzOTAyMn0.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n</code></pre></p>"},{"location":"articles/single-sign-on/#conclusion","title":"Conclusion","text":"<p>Single Sign-On plays an importent role in the realm of identity and access management, providing a user-centric and secure approach to accessing multiple applications seamlessly. With benefits ranging from enhanced user experience to improved security and streamlined administration, SSO continues to play a vital role in shaping the future of digital identity.</p>"},{"location":"articles/single-sign-on/#references","title":"References","text":"<ul> <li>OAuth 2.0 Specification</li> <li>OpenID Connect Specifications</li> <li>Security Assertion Markup Language (SAML)</li> </ul>"},{"location":"articles/windows-terminal-setup/","title":"Setting up Windows Terminal with Oh-My-Posh","text":"<p>In this tutorial, I'll guide you through the process of customizing your Windows terminal using Oh-My-Posh. </p> <p><code>Oh My Posh</code> is a highly customizable prompt engine designed to elevate your command-line experience. With full support for colors, it allow users to create visually appealing and informative prompts in various shell environments.</p>"},{"location":"articles/windows-terminal-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the following prerequisites in place:</p> <ul> <li>Windows 10 or later</li> <li>PowerShell - make sure latest version is installed</li> <li>Chocolatey</li> <li>A Windows Terminal</li> </ul>"},{"location":"articles/windows-terminal-setup/#step-1-install-windows-terminal","title":"Step 1: Install Windows Terminal","text":"<p>If you haven't already installed Windows Terminal, you can install from choco tool or get it from the Microsoft Store</p> <p>Microsoft Store:</p> <ul> <li>Open the Microsoft Store.</li> <li>Search for \"Windows Terminal\" and click on the application's page.</li> <li>Click the \"Get\" or \"Install\" button to download and install it.</li> </ul> <p>Once Windows Terminal is installed, you can open it by searching for \"Windows Terminal\" in the Windows Start menu.</p>"},{"location":"articles/windows-terminal-setup/#step-2-install-oh-my-posh","title":"Step 2: Install Oh-My-Posh","text":"<p>Open your terminal and run the following command:</p> <pre><code>choco install oh-my-posh\n</code></pre> <p></p> <p>This command will download and install Oh-My-Posh on your system. </p>"},{"location":"articles/windows-terminal-setup/#step-3-configure-oh-my-posh","title":"Step 3: Configure Oh My Posh","text":"<p>Once Oh-My-Posh is installed, you can configure your PowerShell prompt to use a custom theme. To configure your prompt, follow these steps:</p> <p>Open your PowerShell profile for editing</p> <pre><code>code $profile\n# or\nnotepad.exe $PROFILE\n</code></pre> <p>file will be empty initially, insert following</p> <pre><code>oh-my-posh.exe init pwsh | Invoke-Expression\n</code></pre> <p>Save and close your profile</p> <p>open a new Windows Terminal instance to see the updated prompt with your chosen theme.</p> <p></p>"},{"location":"articles/windows-terminal-setup/#step-4-install-nerd-font","title":"Step 4: Install Nerd Font","text":"<p>Nerd Fonts are specialized fonts that include a wide range of icons and symbols commonly used in programming and terminal applications. These fonts are popular among developers and users who want to enhance their terminal experience. Here are the steps to install a Nerd Font on your system:</p> <ul> <li> <p>Choose a Nerd Font: Before you can install a Nerd Font, you need to decide which Nerd Font variant you want to use.You can find a list of available Nerd Fonts on the Nerd Fonts GitHub repository. <code>DejaVu Sans Mono Nerd Font</code> - is my favorite</p> </li> <li> <p>Download the Nerd Font: Visit the following webstie to download your chosen of Nerd Font. https://www.nerdfonts.com/font-downloads</p> </li> <li> <p>Install the Nerd Font: Once you have downloaded the Nerd Font, click on <code>install</code> button to install it on your Windows system:      </p> </li> <li> <p>Configure Windows Terminal to Use the Nerd Font: To use the Nerd Font in Windows Terminal, you'll need to configure your terminal settings. Here's how to do it:</p> <p></p> <p></p> </li> </ul> <p>To apply the changes, close and reopen Windows Terminal. </p>"},{"location":"articles/windows-terminal-setup/#step-5-verify-the-oh-my-posh-install","title":"Step 5: Verify the Oh My Posh Install","text":"<p>To verify that Oh-My-Posh is correctly installed and configured, open Windows Terminal and ensure that you see your custom prompt with the selected theme. You should see a stylish and informative prompt that includes Git status, time, and other relevant information.</p> <p>Now, You've successfully set up Windows Terminal with Oh-My-Posh, enhancing your command-line experience on Windows.</p> <p></p>"},{"location":"articles/windows-terminal-setup/#conclusion","title":"Conclusion","text":"<p>By following these steps, you've successfully set up Windows Terminal with Oh-My-Posh and a Nerd Font. You now have a highly customizable and visually appealing terminal environment on your Windows system, perfect for coding and everyday tasks.</p> <p>Feel free to explore different Oh-My-Posh themes and Nerd Font variants to personalize your terminal even further.</p>"},{"location":"articles/windows-terminal-setup/#resources","title":"Resources","text":"<ul> <li>Oh My Posh - Official Website</li> <li>Nerd Fonts</li> <li>microsoft/terminal</li> <li>Installation</li> </ul>"},{"location":"developertools/ai-engineering/ai-concepts/","title":"AI Concepts for Beginners","text":"<p>1. Introduction to Artificial Intelligence</p> <ul> <li>1.1 What is AI?</li> <li>1.2 History and Evolution of AI</li> <li>1.3 Applications of AI in Real Life</li> <li>1.4 Myths and Misconceptions about AI</li> </ul> <p>2. Types of AI</p> <ul> <li>2.1 Narrow AI vs General AI vs Super AI</li> <li>2.2 Reactive Machines vs Limited Memory vs Theory of Mind</li> <li>2.3 Strong AI vs Weak AI</li> </ul> <p>3. Key Fields of AI</p> <ul> <li>3.1 Machine Learning</li> <li>3.2 Deep Learning</li> <li>3.3 Natural Language Processing (NLP)</li> <li>3.4 Computer Vision</li> <li>3.5 Robotics</li> <li>3.6 Expert Systems</li> </ul> <p>4. Fundamentals of Machine Learning</p> <ul> <li>4.1 What is Machine Learning?</li> <li>4.2 Types of ML: Supervised, Unsupervised, Reinforcement Learning</li> <li>4.3 Key Concepts: Features, Labels, Training, Testing</li> <li>4.4 Overfitting and Underfitting</li> <li>4.5 Model Evaluation Metrics</li> </ul> <p>5. Introduction to Neural Networks</p> <ul> <li>5.1 What is a Neural Network?</li> <li>5.2 Architecture: Input, Hidden, and Output Layers</li> <li>5.3 Activation Functions</li> <li>5.4 Training with Backpropagation and Gradient Descent</li> </ul> <p>6. Working with Data</p> <ul> <li>6.1 Importance of Data in AI</li> <li>6.2 Data Collection and Cleaning</li> <li>6.3 Feature Engineering</li> <li>6.4 Data Preprocessing Techniques</li> </ul> <p>7. Tools and Programming Languages for AI</p> <ul> <li>7.1 Why Python is Popular in AI</li> <li>7.2 Key Libraries and Frameworks (NumPy, Pandas, scikit-learn, TensorFlow, PyTorch)</li> <li>7.3 Using Jupyter Notebooks and Google Colab</li> </ul> <p>8. AI Development Workflow</p> <ul> <li>8.1 Defining the Problem</li> <li>8.2 Preparing the Dataset</li> <li>8.3 Selecting and Training a Model</li> <li>8.4 Evaluating the Model</li> <li>8.5 Hyperparameter Tuning</li> <li>8.6 Model Deployment (Basic Overview)</li> </ul> <p>9. Hands-On Beginner Projects</p> <ul> <li>9.1 Spam Email Classifier</li> <li>9.2 Handwritten Digit Recognition (MNIST)</li> <li>9.3 Sentiment Analysis of Movie Reviews</li> <li>9.4 Basic Chatbot using NLP</li> <li>9.5 Image Classifier with CNNs</li> <li>9.6 House Price Predictor</li> <li>9.7 Stock Price Trend Classifier</li> <li>9.8 Rock, Paper, Scissors Game with Computer Vision</li> <li>9.9 Fake News Detector</li> <li>9.10 Music Genre Classifier</li> <li>9.11 Language Detection App</li> <li>9.12 Number Plate Reader (OCR)</li> <li>9.13 Personal Voice Assistant (Basic)</li> </ul>"},{"location":"developertools/ai-engineering/ai-learning-roadmap/","title":"AI Engineer 2025: Learning Roadmap","text":"<p>1. Foundations of AI</p> <ul> <li>1.1 What is Artificial Intelligence?</li> <li>1.2 AI vs ML vs DL: Understanding the Differences</li> <li>1.3 Applications of AI in Industry</li> <li>1.4 History and Evolution of AI</li> </ul> <p>2. Mathematics for AI</p> <ul> <li>2.1 Linear Algebra<ul> <li>Vectors</li> <li>Matrices</li> <li>Eigenvalues and eigenvectors</li> <li>Matrix operations</li> </ul> </li> <li>2.2 Calculus<ul> <li>Limits</li> <li>Differentiation</li> <li>Integration</li> <li>Multivariable calculus</li> <li>Vector calculus</li> </ul> </li> <li>2.3 Probability and Statistics<ul> <li>Probability theory</li> <li>Random variables</li> <li>Probability distributions</li> <li>Statistical inference</li> <li>Bayesian statistics</li> </ul> </li> <li>2.4 Optimization Techniques <ul> <li>Gradient Descent</li> <li>Cost Functions</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>Khan Academy \u2013 Linear Algebra </li> <li>FreeCodeCamp \u2013 Linear Algebra </li> <li>StatQuest with Josh Starmer \u2013 YouTube </li> <li>FreeCodeCamp \u2013 Calculus </li> <li>Probability for Machine Learning</li> </ul> <p>Why learn this? </p> <p>Mathematics is essential to understand how AI models learn and optimize themselves. These topics will help you understand the inner workings of learning algorithms.</p> <p>3. Programming for AI</p> <ul> <li>3.1 Python<ul> <li>Basic syntax &amp; Variables</li> <li>Data structures</li> <li>Control structures</li> <li>Functions and modules</li> <li>Object-oriented programming</li> </ul> </li> <li>3.1 AI-related libraries<ul> <li>NumPy</li> <li>Pandas</li> <li>Data Visualization (Matplotlib, Seaborn)</li> <li>TensorFlow</li> <li>PyTorch</li> <li>Jupyter Notebooks &amp; Google Colab</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>W3Schools Python Tutorial</li> <li>FreeCodeCamp Python Course </li> <li>RealPython Python Basics</li> </ul> <p>Why learn this?</p> <p>Python is the most widely used programming language in AI. You need it to implement algorithms and work with data.</p> <p>4. Machine Learning (ML)</p> <ul> <li>4.1 Supervised Learning <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Support vector machines</li> <li>Decision Trees</li> <li>Random forests</li> <li>Gradient boosting machines</li> </ul> </li> <li>4.2 Unsupervised Learning <ul> <li>Clustering (K-means, DBSCAN)</li> <li>Dimensionality reduction (PCA, t-SNE)</li> <li>Anomaly detection</li> </ul> </li> <li>4.3 Reinforcement learning<ul> <li>Markov decision processes</li> <li>Q-learning</li> <li>Deep Q-networks</li> <li>Policy gradients</li> <li>Actor-critic methods</li> </ul> </li> <li>4.2 Evaluation and validation<ul> <li>Training, validation, and test sets</li> <li>Cross-validation</li> <li>Model selection and hyperparameter tuning</li> <li>Performance metrics</li> </ul> </li> </ul> <p>Why learn this?</p> <p>This is where AI begins \u2014 enabling machines to learn patterns and make predictions.</p> <p>Tools:</p> <ul> <li>Scikit-learn, </li> <li>Google Colab, </li> <li>Real Datasets from Kaggle and UCI ML Repository</li> </ul> <p>Resources:</p> <ul> <li>Google\u2019s Machine Learning Crash Course </li> <li>Kaggle Intro to Machine Learning </li> <li>DataCamp ML with Python </li> </ul> <p>Projects:</p> <ul> <li>Titanic Survival Prediction \u2013 YouTube Tutorial </li> <li>House Price Prediction    </li> <li>Spam Email Classifier    </li> <li>Customer Segmentation</li> </ul> <p>5. Deep Learning(DL)</p> <ul> <li>5.1 Neural networks<ul> <li>Multilayer perceptrons</li> <li>Activation functions</li> <li>Backpropagation</li> <li>Optimization algorithms</li> </ul> </li> <li>5.2 Convolutional neural networks<ul> <li>Convolutional layers</li> <li>Pooling layers</li> <li>Architectures (LeNet, AlexNet, VGG, ResNet)</li> </ul> </li> <li>5.3 Recurrent neural networks (RNNs)<ul> <li>Long short-term memory (LSTM)</li> <li>Gated recurrent units (GRU)</li> <li>Sequence-to-sequence models</li> </ul> </li> <li>5.4 Generative models<ul> <li>Variational autoencoders (VAE)</li> <li>Generative adversarial networks (GAN)</li> <li>Transformer models (BERT, GPT-2, T5)</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>DeepLearning.AI on Coursera </li> <li>Stanford CS231n </li> <li>PyTorch YouTube Tutorials </li> </ul> <p>Projects:</p> <ul> <li>Handwritten Digit Recognition \u2013 YouTube Tutorial </li> <li>Sentiment Analysis using Recurrent Networks    </li> <li>Build a Chatbot using Sequence-to-Sequence Models</li> </ul> <p>6. Natural Language Processing (NLP)</p> <ul> <li>6.1 Text preprocessing<ul> <li>Tokenization</li> <li>Stemming and lemmatization</li> <li>Stopword removal</li> <li>Part-of-speech tagging</li> </ul> </li> <li>6.2 Feature extraction<ul> <li>Bag of words</li> <li>TF-IDF</li> <li>Word embeddings (Word2Vec, GloVe,FastText)</li> </ul> </li> <li>6.3 Text classification<ul> <li>Sentiment analysis</li> <li>Topic modeling</li> </ul> </li> <li>6.4 Sequence Modeling (RNNs, LSTMs)<ul> <li>Named entity recognition</li> <li>Text summarization</li> <li>Machine translation</li> </ul> </li> <li>6.5 Transformers (BERT, GPT, T5)</li> <li>6.6 Chatbots and Language Generation</li> </ul> <p>Tools:</p> <ul> <li>NLTK, spaCy, Hugging Face Transformers, TensorFlow, PyTorch</li> </ul> <p>Projects:</p> <ul> <li>Build a News Classifier using BERT    </li> <li>Chatbot with custom FAQ data using Transformers    </li> <li>Twitter Sentiment Classifier using Hugging Face pipeline</li> </ul> <p>7. Computer Vision</p> <ul> <li>7.1 Image Preprocessing Techniques<ul> <li>Filtering techniques</li> <li>Edge detection</li> <li>Feature extraction</li> </ul> </li> <li>7.2 CNN Architectures (VGG, ResNet, EfficientNet)</li> <li>7.3 Object Detection (YOLO, SSD, Faster R-CNN)<ul> <li>Sliding window approach</li> <li>Region-based CNN (R-CNN)</li> <li>YOLO (You Only Look Once)</li> </ul> </li> <li>7.4 Image Segmentation (U-Net, Mask R-CNN)<ul> <li>Semantic segmentation</li> <li>Instance segmentation</li> </ul> </li> <li>7.5 Face Recognition, OCR</li> <li>7.6 Pose estimation<ul> <li>2D pose estimation</li> <li>3D pose estimation</li> </ul> </li> </ul> <p>Tools:</p> <ul> <li>OpenCV, TensorFlow, PyTorch, Keras, PIL</li> </ul> <p>Projects:</p> <ul> <li>Object detection in webcam feed    </li> <li>OCR number plate reader    </li> <li>Real-time face mask detection</li> </ul> <p>8. Data Engineering for AI</p> <ul> <li>8.1 Data Collection &amp; Pipelines</li> <li>8.2 Data Cleaning and Imputation</li> <li>8.3 Feature Stores and Data Versioning</li> <li>8.4 Big Data Tools (Spark, Hadoop basics)</li> </ul> <p>Tools:</p> <ul> <li>Apache Airflow, Apache Kafka, DVC, Great Expectations, Spark (PySpark)</li> </ul> <p>9. Model Deployment &amp; MLOps</p> <ul> <li>9.1 Model Serialization (Pickle, ONNX, TorchScript)</li> <li>9.2 REST APIs for ML Models (Flask, FastAPI)</li> <li>9.3 Model Serving (TensorFlow Serving, TorchServe)</li> <li>9.4 Docker for AI Applications</li> <li>9.5 CI/CD for ML (GitHub Actions, Jenkins)</li> <li>9.6 MLflow &amp; Weights &amp; Biases for Experiment Tracking</li> <li>9.7 Monitoring and Scaling ML Systems</li> </ul> <p>Projects:</p> <ul> <li>Deploy a sentiment analysis model with FastAPI + Docker    </li> <li>Track model experiments with MLflow</li> </ul> <p>10. Cloud &amp; Edge AI</p> <ul> <li>10.1 AI on Cloud (AWS SageMaker, GCP Vertex AI, Azure ML)</li> <li>10.2 Using GPUs and TPUs</li> <li>10.3 Edge AI (TinyML, TensorFlow Lite, NVIDIA Jetson)</li> <li>10.4 Serverless AI Architectures</li> </ul> <p>Projects:</p> <ul> <li>Deploy model to Vertex AI endpoint    </li> <li>Run image classifier on Jetson Nano using TensorFlow Lite</li> </ul> <p>11. Responsible AI &amp; Ethics</p> <ul> <li>11.1 Fairness, Accountability, and Transparency</li> <li>11.2 Bias in Data and Models</li> <li>11.3 Privacy and Security in AI</li> <li>11.4 AI Regulation and Governance</li> </ul> <p>12. Real-World Projects</p> <ul> <li>12.1 Predictive Analytics (Time Series Forecasting)</li> <li>12.2 Image Classification &amp; Detection</li> <li>12.3 NLP Chatbot</li> <li>12.4 AI for Healthcare or Finance</li> <li>12.5 Recommender System</li> <li>12.6 Custom AI SaaS Product</li> </ul> <p>Platforms to Practice:</p> <ul> <li>Kaggle </li> <li>Google Colab </li> <li>Hugging Face Datasets </li> <li>PapersWithCode</li> </ul> <p>13. AI Books</p> <ul> <li>13.1 AI Engineering - by Chip Huyen<ul> <li>AI Engineering - Gitgub</li> </ul> </li> <li>13.2 Build a Large Language Model -  From Scratch - by Sebastian Raschka<ul> <li>Build a Large Language Model - Github</li> </ul> </li> <li>13.3 LLM Engineer's Handbook -  by Paul Iusztin, Maxime Labonne<ul> <li>LLM Engineer's Handbook - Github</li> <li>SylphAI-Inc - LLM-engineer-handbook - Github</li> </ul> </li> <li>13.4 Artificial Intelligence with Python by Prateek Joshi</li> <li>13.5 Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aur\u00e9lien G\u00e9ron</li> <li>13.6 Deep Learning with Python by Fran\u00e7ois Chollet</li> <li>13.7 Machine Learning Yearning by Andrew Ng</li> <li>13.8 Artificial Intelligence: A Modern Approach by Stuart Russell and Peter Norvig</li> </ul> <p>14. Courses</p> <ul> <li>14.1 Machine Learning by Andrew Ng on Coursera</li> <li>14.2 Deep Learning Specialization by Andrew Ng on Coursera</li> <li>14.3 Applied Data Science with Python Specialization on Coursera</li> <li>14.4 Introduction to Artificial Intelligence with Python on edX</li> </ul> <p>15. Articles</p> <ul> <li>15.1 A Beginner's Guide to AI/ML by Analytics Vidhya</li> <li>15.2 What is Artificial Intelligence? A Beginner\u2019s Guide by Builtin</li> </ul> <p>By completing this roadmap, you will not only understand how artificial intelligence works \u2014 but also build, deploy, and scale real-world AI applications confidently.</p>"},{"location":"developertools/ai-engineering/ai-technology-stack/","title":"AI Engineer 2025: Tools, Technologies, Frameworks, and Communities","text":"<p>Here is essential tools, frameworks, languages, and communities you need to master on to becoming a successful AI Engineer in 2025.</p> <p>1. Programming Languages for AI</p> <ul> <li>1.1 Python (Primary Language)</li> <li>1.2 SQL (for data querying and ETL)</li> <li>1.3 Bash/Shell Scripting (for automation)</li> <li>1.4 C++ (for performance-critical tasks, optional)</li> <li>1.5 JavaScript (for frontend AI integration, e.g., chatbots)</li> </ul> <p>2. Development Environments</p> <ul> <li>2.1 Jupyter Notebooks</li> <li>2.2 Google Colab</li> <li>2.3 VS Code with Python &amp; AI Extensions</li> <li>2.4 PyCharm Professional (for larger AI/ML codebases)</li> </ul> <p>3. Key Python Libraries &amp; Packages</p> <ul> <li>3.1 NumPy, Pandas \u2013 Data manipulation</li> <li>3.2 Matplotlib, Seaborn, Plotly \u2013 Visualization</li> <li>3.3 Scikit-learn \u2013 Classical ML</li> <li>3.4 OpenCV \u2013 Image processing</li> <li>3.5 NLTK, spaCy \u2013 Basic NLP tasks</li> <li>3.6 Hugging Face Transformers \u2013 State-of-the-art NLP/LLMs</li> <li>3.7 XGBoost, LightGBM, CatBoost \u2013 Gradient boosting models</li> <li>3.8 PyCaret \u2013 Low-code ML experimentation</li> </ul> <p>4. Deep Learning Frameworks</p> <ul> <li>4.1 TensorFlow (incl. Keras API)</li> <li>4.2 PyTorch (preferred in research and modern projects)</li> <li>4.3 FastAI (on top of PyTorch)</li> <li>4.4 JAX (for advanced, high-performance ML)</li> </ul> <p>5. Generative AI &amp; LLM Ecosystem</p> <ul> <li>5.1 Hugging Face Transformers</li> <li>5.2 LangChain \u2013 LLM application framework</li> <li>5.3 LlamaIndex \u2013 Indexing and querying over documents</li> <li>5.4 Vector Databases \u2013 FAISS, Weaviate, ChromaDB, Qdrant</li> <li>5.5 Open Source LLMs \u2013 Mistral, LLaMA3, Phi-3, Mixtral</li> <li>5.6 Ollama \u2013 Local model runner for open-source LLMs</li> </ul> <p>6. Data Engineering &amp; Processing Tools</p> <ul> <li>6.1 Apache Spark (PySpark) \u2013 Big Data</li> <li>6.2 Apache Kafka \u2013 Real-time data pipelines</li> <li>6.3 Airflow \u2013 Workflow orchestration</li> <li>6.4 DVC \u2013 Data version control</li> <li>6.5 Great Expectations \u2013 Data validation framework</li> </ul> <p>7. Model Training, Tuning &amp; Experimentation</p> <ul> <li>7.1 Hyperparameter Optimization \u2013 Optuna, Ray Tune</li> <li>7.2 Model Tracking \u2013 MLflow, Weights &amp; Biases (W\\&amp;B)</li> <li>7.3 Model Explainability \u2013 SHAP, LIME, Captum</li> <li>7.4 Checkpointing &amp; Model Saving \u2013 ONNX, TorchScript, Pickle</li> </ul> <p>8. Model Deployment &amp; MLOps</p> <ul> <li>8.1 REST APIs \u2013 Flask, FastAPI</li> <li>8.2 Model Serving \u2013 TensorFlow Serving, TorchServe</li> <li>8.3 Docker \u2013 Containerization for AI apps</li> <li>8.4 Kubernetes \u2013 Scalable model deployment</li> <li>8.5 CI/CD \u2013 GitHub Actions, Jenkins, Azure DevOps</li> <li>8.6 Model Monitoring \u2013 Prometheus, Grafana, Evidently AI</li> </ul> <p>9. Cloud &amp; Compute Platforms</p> <ul> <li>9.1 Google Cloud Platform \u2013 Vertex AI, Colab Pro</li> <li>9.2 AWS \u2013 SageMaker, EC2 GPU, Lambda</li> <li>9.3 Microsoft Azure \u2013 Azure Machine Learning</li> <li>9.4 RunPod, Lambda Labs \u2013 Pay-as-you-go GPUs</li> <li>9.5 Paperspace, Kaggle Kernels \u2013 Free GPU access for learning</li> </ul> <p>10. Frontend &amp; Interface for AI Apps</p> <ul> <li>10.1 Streamlit \u2013 Python-based dashboards</li> <li>10.2 Gradio \u2013 Rapid ML model demos</li> <li>10.3 Flask/FastAPI + React \u2013 Full-stack AI projects</li> <li>10.4 Next.js \u2013 Deploy LLM tools with modern UI</li> </ul> <p>11. Version Control &amp; Collaboration</p> <ul> <li>11.1 Git &amp; GitHub \u2013 Code and model versioning</li> <li>11.2 GitHub Actions \u2013 CI/CD pipelines</li> <li>11.3 Git LFS / DVC \u2013 Large model &amp; data versioning</li> <li>11.4 Hugging Face Spaces \u2013 Share models and apps</li> </ul> <p>12. Popular GitHub Projects to Explore &amp; Learn From</p> <ul> <li>12.1 fastai/fastbook \u2013 Deep learning curriculum</li> <li>12.2 huggingface/transformers \u2013 NLP &amp; LLM hub</li> <li>12.3 explosion/spaCy \u2013 Industrial NLP</li> <li>12.4 mistralai \u2013 Open-source LLMs (Mixtral, Mistral)</li> <li>12.5 mlflow/mlflow \u2013 ML lifecycle management</li> <li>12.6 automl/auto-sklearn \u2013 AutoML pipeline builder</li> <li>12.7 openai/whisper \u2013 Speech-to-text model</li> <li>12.8 llamaindex/llamaindex \u2013 RAG apps</li> <li>12.9 langchain-ai/langchain \u2013 LLM apps with memory</li> <li>12.10 openai/chatgpt-retrieval-plugin \u2013 RAG plugin architecture</li> </ul> <p>13. AI Communities &amp; Learning Platforms</p> <ul> <li>13.1 Hugging Face Community &amp; Discord</li> <li>13.2 Papers with Code (SOTA models and papers)</li> <li>13.3 Kaggle \u2013 Competitions, datasets, and notebooks</li> <li>13.4 Reddit \u2013 r/MachineLearning, r/learnmachinelearning</li> <li>13.5 GitHub \u2013 Following trending ML/AI repositories</li> <li>13.6 Discord Servers \u2013 MLOps Community, Deep Learning.ai</li> <li>13.7 Meetup &amp; Devpost \u2013 AI hackathons and local events</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/","title":"ArgoCD Commands","text":""},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used ArgoCD commands with examples.</p> <p><code>ArgoCD</code> is a popular tool for managing Kubernetes applications and deploying them in a declarative manner. ArgoCD provides a web UI, but it also has a command-line interface (CLI) that can be used to manage applications, repositories, and other resources.</p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Argocd CLI</li> <li>Azure login</li> <li>Select the subscription</li> <li>Connect to k8s Cluster</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#installing-argocd","title":"Installing ArgoCD","text":"<p>Use the following commands to install ArgoCD CLI in MacOS and Windows.</p> <pre><code># MacOS (using Homebrew):\nbrew install argocd\n\n# Windows OS (using Choco)\nchoco install argocd-cli\n\n#  verify the installation by running \nargocd version\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#connection-to-kubernetes-cluster","title":"Connection to Kubernetes cluster","text":"<p>Note: Make sure that you login into azure, select the azure subscription &amp; connect k8s cluster before running any <code>argocd</code> commands.</p> <pre><code># Azure login\naz login\n\n# Select the subscription\naz account set -s \"anji.keesari\"\naz account show --output table\n\n# Connect to k8s Cluster\n\n# Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-commands-description","title":"ArgoCD Commands &amp; description","text":"<p>Here are some common ArgoCD CLI commands and their purposes:</p> <ul> <li><code>argocd login:</code> This command logs in to an ArgoCD server and saves the session token locally.</li> <li><code>argocd app create:</code> This command creates a new application from a Git repository.</li> <li><code>argocd app get:</code> This command retrieves information about an existing application, such as its status and configuration.</li> <li><code>argocd app sync:</code> This command synchronizes an application's configuration with the desired state specified in its Git repository.</li> <li><code>argocd app delete:</code> This command deletes an application from ArgoCD.</li> <li><code>argocd app diff:</code> This command displays the differences between the current state of an application and the desired state specified in its Git repository.</li> <li><code>argocd app history:</code> This command lists the deployment history of an application in ArgoCD.</li> <li><code>argocd app rollback:</code> This command rolls back an application to a previous deployment revision.</li> <li><code>argocd repo add:</code> This command adds a Git repository to ArgoCD's list of managed repositories.</li> <li><code>argocd repo list:</code> This command lists all the Git repositories that ArgoCD is currently managing.</li> <li><code>argocd repo rm:</code> This command removes a Git repository from ArgoCD's list of managed repositories.</li> <li><code>argocd repo list-resources:</code> This command lists all the Kubernetes resources in a Git repository.</li> <li><code>argocd proj create:</code> This command creates a new project in ArgoCD, which can be used to group related applications and apply shared policies.</li> <li><code>argocd proj get:</code> This command retrieves information about an existing project, such as its applications and policies.</li> <li><code>argocd proj delete:</code> This command deletes a project from ArgoCD.</li> <li><code>argocd proj list:</code> This command lists all the projects in ArgoCD.</li> <li><code>argocd proj delete:</code> This command deletes a project from ArgoCD.</li> <li><code>argocd cluster add:</code> This command adds a new Kubernetes cluster to ArgoCD's list of managed clusters.</li> <li><code>argocd cluster list:</code> This command lists all the Kubernetes clusters that ArgoCD is currently managing.</li> <li><code>argocd cluster rm:</code> This command removes a Kubernetes cluster from ArgoCD's list of managed clusters.</li> <li><code>argocd account update-password:</code> This command allows you to change the password for your ArgoCD account.</li> <li><code>argocd account list:</code> This command lists all the user accounts that have access to ArgoCD.</li> <li><code>argocd version:</code> This command retrieves the current version of ArgoCD.</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-help","title":"ArgoCD help","text":"<p>This command provides general help and usage information about ArgoCD. It gives an overview of available commands and their usage.</p> <p><pre><code>argocd help\nor \nargocd --help\n</code></pre> output</p> <pre><code>Available Commands:\n  account     Manage account settings\n  admin       Contains a set of commands useful for Argo CD administrators and requires direct Kubernetes access\n  app         Manage applications\n  cert        Manage repository certificates and SSH known hosts entries\n  cluster     Manage cluster credentials\n  completion  output shell completion code for the specified shell (bash or zsh)\n  context     Switch between contexts\n  gpg         Manage GPG keys used for signature verification\n  help        Help about any command\n  login       Log in to Argo CD\n  logout      Log out from Argo CD\n  proj        Manage projects\n  relogin     Refresh an expired authenticate token\n  repo        Manage repository connection parameters\n  repocreds   Manage repository connection parameters\n  version     Print version information\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-commands-help","title":"ArgoCD commands help","text":"<p>This command provides detailed help and usage information about individual ArgoCD commands. It can be used to get specific information about any command's usage, options, and arguments.</p> <p><pre><code>argocd help app\n</code></pre> output <pre><code>Manage applications\n\nUsage:\n  argocd app [flags]\n  argocd app [command]        \n\nExamples:\n  # List all the applications.\n  argocd app list\n\n  # Get the details of a application\n  argocd app get my-app\n\n  # Set an override parameter\n  argocd app set my-app -p image.tag=v1.0.1\n\nAvailable Commands:\n  actions         Manage Resource actions\n  create          Create an application\n  delete          Delete an application\n  delete-resource Delete resource in an application\n  diff            Perform a diff against the target and live state.\n  edit            Edit application\n  get             Get application details\n  history         Show application deployment history\n  list            List applications\n  logs            Get logs of application pods\n  manifests       Print manifests of an application\n  patch           Patch application\n  patch-resource  Patch resource in an application\n  resources       List resource of application\n  rollback        Rollback application to a previous deployed version by History ID, omitted will Rollback to the previous version\n  set             Set application parameters\n  sync            Sync an application to its target state\n  terminate-op    Terminate running operation of an application\n  unset           Unset application parameters\n  wait            Wait for an application to reach a synced and healthy state\n</code></pre></p> <p><pre><code>argocd help repo\n</code></pre> output <pre><code>Manage repository connection parameters\n\nUsage:\n  argocd repo [flags]\n  argocd repo [command]\n\nAvailable Commands:\n  add         Add git repository connection parameters\n  get         Get a configured repository by URL\n  list        List configured repositories\n  rm          Remove repository credentials\n</code></pre></p> <p><pre><code>argocd help account\n</code></pre> output <pre><code>Manage account settings\n\nUsage:\n  argocd account [flags]\n  argocd account [command]\n\nAvailable Commands:\n  can-i           Can I\n  delete-token    Deletes account token\n  generate-token  Generate account token\n  get             Get account details\n  get-user-info   Get user info\n  list            List accounts\n  update-password Update an account's password\n</code></pre></p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#login-to-argocd","title":"Login to argocd","text":"<p>Before running next set of command you've to login into ArgoCD.</p> <p>To login to ArgoCD, you can use the <code>argocd login</code> command followed by the URL of your ArgoCD server and your credentials. Here's an example:</p> <pre><code>argocd login &lt;ARGOCD_SERVER&gt; [--insecure] [--username &lt;USERNAME&gt;] [--password &lt;PASSWORD&gt;]\n</code></pre> <p>examples:</p> <p><pre><code># localhost argocd login\nargocd login localhost:8080 - need to test this\n\nargocd login yourdomainname.com\n\n# IP address of the ArgoCD service.\nargocd login 20.241.96.132\n</code></pre> Note: By default, the Argo CD API server is not exposed with an external IP. To access the API server, Change the argocd-server service type to LoadBalancer:</p> <p><pre><code>kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n</code></pre> If your ArgoCD server is using a self-signed SSL certificate, you may need to use the --insecure flag to bypass SSL verification.</p> <p>Enter ArgoCD credentials <pre><code>admin\nxxxxx - in bash you may need to right click the mouse instead of key board copy paste to make it work\n</code></pre> output <pre><code>WARNING: server is not configured with TLS. Proceed (y/n)? y\nUsername: admin\nPassword: \n'admin:login' logged in successfully\nContext '20.241.96.132' updated\n</code></pre></p> <p>in case if you use the yourdomainname.com URL, you will see output like this. <pre><code>time=\"2022-11-20T09:44:31-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nUsername: admin\nPassword: \n'admin:login' logged in successfully\nContext 'yourdomainname.com' updated\n</code></pre></p> <p>Once you have logged in successfully, ArgoCD will save a session token locally so that you don't have to log in again for subsequent commands.</p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#cluster-list","title":"Cluster list","text":"<p>This command lists the clusters connected to the ArgoCD server. It displays information about each cluster, such as name, server URL, and current context.</p> <p><pre><code>argocd cluster list\n</code></pre> output <pre><code>time=\"2022-11-20T09:48:31-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nSERVER                          NAME        VERSION  STATUS      MESSAGE  PROJECT\nhttps://kubernetes.default.svc  in-cluster  1.22     Successful\n</code></pre></p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#add-cluster","title":"Add cluster","text":"<p>This command is used to add a new external Kubernetes cluster to the ArgoCD server. It requires specifying the cluster's context name, server URL, and authentication credentials.</p> <pre><code>argocd cluster add aks-cluster2-dev\n</code></pre> <p>output - It will look like this:</p> <pre><code>WARNING: This will create a service account `argocd-manager` on the cluster referenced by context `aks-cluster2-dev` with full cluster level privileges. Do you want to continue [y/N]? y\n\n.\n.\n.\nCluster 'https://cluster2-dns-89d81b75.hcp.northcentralus.azmk8s.io:443' added\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#repository-list","title":"Repository List","text":"<p>This command lists the repositories configured in the ArgoCD server. It provides information about each repository, such as name, URL, and connection status.</p> <p><pre><code>argocd repo list\n</code></pre> output</p> <pre><code>TYPE  NAME  REPO                                                  INSECURE  OCI    LFS    CREDS  STATUS      MESSAGE  PROJECT\ngit         https://github.com/argoproj/argocd-example-apps.git   false     false  false  false  Successful           default\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-list","title":"Application List","text":"<p>This command lists all applications managed by ArgoCD. It displays information about each application, including its name, project, health status, and synchronization status.</p> <p><pre><code>argocd app list\n</code></pre> output</p> <pre><code>NAME        CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH       SYNCPOLICY  CONDITIONS  REPO                                                  PATH         TARGET\nguestbook   https://kubernetes.default.svc  default    default  Synced     Healthy      &lt;none&gt;      &lt;none&gt;      https://github.com/argoproj/argocd-example-apps.git   guestbook    HEAD \n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#app-resources","title":"App Resources","text":"<p>This command shows the Kubernetes resources associated with a specific application. It provides a detailed list of resources deployed by the application, including their types, names, and current status.</p> <p><pre><code>argocd app resources aspnetcore-webapp\n</code></pre> output</p> <pre><code>time=\"2022-11-20T09:53:32-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nGROUP  KIND        NAMESPACE     NAME               ORPHANED\n       Service     sample  aspnetcore-webapp  No      \napps   Deployment  sample  aspnetcore-webapp  No      \n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-details","title":"Application Details","text":"<p>This command displays detailed information about a specific application. It shows information such as the application's project, repository, target revision, and synchronization status.</p> <p><pre><code>argocd app get aspnetcore-webapp\n</code></pre> output <pre><code>time=\"2022-11-20T09:54:28-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\ntime=\"2022-11-20T09:54:28-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nName:               aspnetcore-webapp\nProject:            development\nServer:             https://kubernetes.default.svc\nNamespace:          sample\nURL:                https://yourdomainname.com/applications/aspnetcore-webapp\nRepo:               https://dev.azure.com/keesari/microservices/_git/argocd\nTarget:             develop\nPath:               sample/aspnetcore-webapp\nSyncWindow:         Sync Allowed\nSync Policy:        Automated (Prune)\nSync Status:        Unknown\nHealth Status:      Healthy\n\nCONDITION        MESSAGE                                                   LAST TRANSITION\nComparisonError  rpc error: code = Unknown desc = authentication required  2022-11-19 21:07:07 -0800 PST\n\n\nGROUP  KIND        NAMESPACE     NAME               STATUS   HEALTH   HOOK  MESSAGE\n       Service     sample  aspnetcore-webapp  Unknown  Healthy        service/aspnetcore-webapp unchanged\napps   Deployment  sample  aspnetcore-webapp  Unknown  Healthy        deployment.apps/aspnetcore-webapp configured\n</code></pre></p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-delete","title":"Application Delete","text":"<p>This command is used to delete a specific application managed by ArgoCD. It removes the application and all associated resources from the Kubernetes cluster.</p> <pre><code>argocd app delete guestbook\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-sync","title":"Application sync","text":"<p>This command triggers a synchronization of a specific application with its target state. It ensures that the application's deployed resources match the desired state defined in the repository.</p> <pre><code>argocd app sync guestbook\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#project-list","title":"Project List","text":"<p>This command lists the projects defined in ArgoCD. It provides information about each project, such as name, description, and application count.</p> <p><pre><code>argocd proj list\n</code></pre> output</p> <pre><code>NAME                DESCRIPTION                                       DESTINATIONS    SOURCES  CLUSTER-RESOURCE-WHITELIST  NAMESPACE-RESOURCE-BLACKLIST  SIGNATURE-KEYS  ORPHANED-RESOURCES\ndefault                                                               *,*             *        */*                         &lt;none&gt;                        &lt;none&gt;          disabled\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#create-project","title":"Create Project","text":"<p>This command is used to create a new project in ArgoCD. It requires specifying the project name, optionally providing a description, and setting other project-specific configurations.</p> <pre><code>argocd proj create myproj\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#logout-argocd","title":"Logout argocd","text":"<p>When you run argocd logout, ArgoCD will remove the session token that was saved when you logged in, so you will need to log in again with argocd login the next time you want to run any ArgoCD commands.</p> <p><pre><code>argocd logout 52.159.112.67\n</code></pre> output</p> <pre><code>Logged out from '52.159.112.67'\n</code></pre> <p>These commands allow you to interact with ArgoCD, manage clusters, repositories, applications, projects, and perform various administrative tasks related to continuous deployment and GitOps workflows.</p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/","title":"Azure acr Commands","text":""},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#introduction","title":"Introduction","text":"<p><code>az acr</code> commands used for managing private registries with Azure Container Registries.</p> <p>This page contains a list of commonly used <code>az acr</code> commands.</p> <p>Note: make sure that you login into azure and select the azure subscription before running any <code>az acr</code> commands.</p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#azure-login","title":"Azure login","text":"<pre><code>az login\naz account list --output table\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#select-the-subscription","title":"Select the subscription","text":"<pre><code>az account set -s \"anji.keesari\"\naz account show --output table\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#connect-to-container-registry","title":"Connect to Container Registry","text":"<p><pre><code>az acr login --name acr1dev\n</code></pre> output <pre><code>Login Succeeded\n</code></pre></p> <p>troubleshoot</p> <p>In case if you get following error run the docker desktop to fix the issue.</p> <pre><code>You may want to use 'az acr login -n acr1dev --expose-token' to get an access token, which does not require Docker to be installed.\n2023-02-20 21:38:37.187022 An error occurred: DOCKER_COMMAND_ERROR\nerror during connect: This error may indicate that the docker daemon is not running.: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/json\": open //./pipe/docker_engine: The system cannot find the file specified.\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#to-get-the-login-server-address","title":"To get the login server address","text":"<pre><code>az acr list -g \"rg-acr-dev\" --query \"[].{acrLoginServer:loginServer}\" --output table\n</code></pre> <p>output</p> <pre><code>AcrLoginServer\n-----------------------\nacr1dev.azurecr.io\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#import-container-images","title":"Import container images","text":"<pre><code>$acrName = \"acr1dev\"\n$imageName = \"mcr.microsoft.com/dotnet/aspnet:6.0\"\n\naz acr import --name $acrName --source $imageName --image $imageName \nor\naz acr import --name \"acr1dev\" --source \"mcr.microsoft.com/dotnet/sdk:6.0\" --image \"mcr.microsoft.com/dotnet/sdk:6.0\"\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#list-registries","title":"List registries","text":"<p>Lists all the container registries under the current subscription.</p> <pre><code>az acr repository list --name acr1dev --output table\n</code></pre> <p>output <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\n</code></pre></p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#show-tags","title":"show tags","text":"<p>show tags of a image in the acr</p> <p><pre><code>az acr repository show-tags --name acr1dev --repository mcr.microsoft.com/dotnet/aspnet --output table\n</code></pre> output</p> <pre><code>Result\n--------\n6.0\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#check-health","title":"check-health","text":"<pre><code>az acr check-health -n \"acr1dev\" -y\n</code></pre> <p>output</p> <pre><code>Docker daemon status: available\nDocker version: 'Docker version 20.10.17, build a89b842, platform linux/amd64'\nDocker pull of 'mcr.microsoft.com/mcr/hello-world:latest' : OK\nAzure CLI version: 2.44.1\nDNS lookup to acr1dev.azurecr.io at IP 20.62.128.9 : OK\nChallenge endpoint https://acr1dev.azurecr.io/v2/ : OK\nFetch refresh token for registry 'acr1dev.azurecr.io' : OK\nFetch access token for registry 'acr1dev.azurecr.io' : OK\nHelm version: 3.8.2\n2023-02-20 21:58:29.062713 An error occurred: NOTARY_COMMAND_ERROR\nPlease verify if notary is installed.\n\nPlease refer to https://aka.ms/acr/errors#notary_command_error for more information.\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#helm-list","title":"helm list","text":"<p>List all helm charts in an Azure Container Registry. <pre><code>az acr helm list -n 'acr1dev'\n</code></pre></p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#references","title":"References","text":"<ul> <li>https://learn.microsoft.com/en-us/cli/azure/acr?view=azure-cli-latest</li> </ul>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/","title":"Azure CLI Commands","text":""},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used azure cli commands with examples.</p> <p>The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. </p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#installing-azure-cli","title":"Installing Azure CLI","text":"<p>Use the following commands to install Azure CLI in Windows, MacOS and Linux environments.</p> <p><pre><code># MacOS (using Homebrew):\nbrew install azure-cli\n\n# Windows OS (using choco)\nchoco install azure-cli\n\n#  verify the installation by running \naz --version\n\n# updating cli\naz upgrade\n</code></pre> For more information, refer to the official documentation:  - How to install the Azure CLI</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-login","title":"az Login","text":"<pre><code>az login\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-account","title":"az account","text":"<pre><code>az account set -s \"anji-cloud\"\n\n# Get a list of subscriptions for the logged in account.\naz account list -o table \n\n# To view all the Azure subscription names and IDs for a specific Microsoft account,\naz account list --query \"[?user.name=='anjkeesari@gmail.com'].{Name:name, ID:id, Default:isDefault}\" --output Table\n\n# Get the details of a subscription.\n az account show\n az account show -o table\n\n# Get all subscriptions for a tenant.\naz account subscription list -o table\n\n# Get details about a specified subscription.\naz account subscription show --subscription-id \"85c49b84-b13d-4168-962c-8107c5b32b7e\"\n# or\naz account subscription show --id '85c49b84-b13d-4168-962c-8107c5b32b7e'\n\n# Get the tenants for your account.\naz account tenant list  \n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-group","title":"az group","text":"<pre><code># Create a new resource group.k\naz group create -l 'eastus' -n 'rg-demo'\n\n# List resource groups.\naz group list -o table\n\n# Check if a resource group exists.\naz group exists -n 'rg-demo'\n\n# Create a resource group lock.\n az group lock create --lock-type ReadOnly -n lockName -g 'rg-demo'\n az group lock create --lock-type CanNotDelete -n lockName -g 'rg-demo'\n\n# List lock information in the resource-group.\n az group lock list -g 'rg-demo'\n\n# Show the details of a resource group lock.\n az group lock show -n lockname -g 'rg-demo'\n\n# Delete a resource group lock.\naz group lock delete -n lockName -g 'rg-demo'  \n\n# Delete a resource group.\naz group delete -n 'rg-demo'\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-aks","title":"az aks","text":"<p>Manage Azure Kubernetes Services - Reference</p> <pre><code># List managed Kubernetes clusters.\naz aks list -o table\n\n# Get access credentials for a managed Kubernetes cluster.\n\n# user authentication\naz aks get-credentials --name 'aks-cluster1-dev' --resource-group 'rg-aks-dev'\n\n# admin authentication\naz aks get-credentials --name 'aks-cluster1-dev' --resource-group 'rg-aks-dev' --admin\n\n# Get the versions available for creating a managed Kubernetes cluster.\naz aks get-versions --location westus2 -o table \n\n# Run a shell command\naz aks command invoke -n 'aks-cluster1-dev' -g 'rg-aks-dev' --command \"kubectl get namespaces\"\naz aks command invoke -n 'aks-cluster1-dev' -g 'rg-aks-dev' --command \"kubectl create namespace test\"\n\n# Download and install kubectl, the Kubernetes command-line tool. \naz aks install-cli\n\n# List node pools in the managed Kubernetes cluster\naz aks nodepool list --cluster-name 'aks-cluster1-dev' -g 'rg-aks-dev'\naz aks nodepool list --cluster-name 'aks-cluster1-dev' -g 'rg-aks-dev' -o table\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-acr","title":"az acr","text":"<p>Manage private registries with Azure Container Registries. - Reference</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-ad","title":"az ad","text":"<p>Azure AD for Role Based Access Control - Reference</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-webapp","title":"az webapp","text":"<p>Manage web app logs - Reference</p> <pre><code># Start live log tracing for a web app.\naz webapp log tail --name 'feedback-api-dev' --resource-group 'aklab-rg-dev'\n\n# Download a web app's log history as a zip file.\naz webapp log download --name 'feedback-api-dev' --resource-group 'aklab-rg-dev' --log-file webapp_624221039.zip\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#reference","title":"Reference","text":"<ul> <li>Azure CLI reference</li> </ul>"},{"location":"developertools/cheatsheets/cloud-comare-cheat-sheet/","title":"Cloud Comparison Cheat Sheet","text":"<p>Here's the comparison cheat sheet across Azure, AWS, and GCP.</p> <p>This comprehensive cheat sheet covers a wide range of services across Azure, AWS, and GCP. Keep in mind that the availability of certain services may vary depending on the region and specific requirements. Also, these cloud platforms are constantly evolving, so new services may be introduced over time.</p> Category Azure Service AWS Service GCP Service Compute Azure Virtual Machines Amazon EC2 (Elastic Compute Cloud) Google Compute Engine Azure App Service AWS Elastic Beanstalk Google App Engine Azure Functions AWS Lambda Google Cloud Functions Azure Kubernetes Service (AKS) Amazon EKS (Elastic Kubernetes Service) Google Kubernetes Engine (GKE) Storage Azure Blob Storage Amazon S3 (Simple Storage Service) Google Cloud Storage (GCS) Azure Files Amazon EFS (Elastic File System) Google Cloud Filestore Azure Disk Storage Amazon EBS (Elastic Block Store) Google Compute Engine Persistent Disks Azure Data Lake Storage Amazon S3 (with appropriate configurations) Google Cloud Storage (GCS) Azure Queue Storage Amazon SQS (Simple Queue Service) Google Cloud Pub/Sub Database Azure SQL Database Amazon RDS (Relational Database Service) Cloud SQL Azure Cosmos DB Amazon DynamoDB Cloud Firestore / Cloud Bigtable Azure Database for MySQL Amazon RDS (MySQL) Cloud SQL for MySQL Azure Database for PostgreSQL Amazon RDS (PostgreSQL) Cloud SQL for PostgreSQL Azure Cache for Redis Amazon ElastiCache Cloud Memorystore Azure Synapse Analytics Amazon Redshift BigQuery Identity and Access Management Azure Active Directory (Microsoft Entra ID) AWS Directory Service Cloud Identity / Cloud IAM Azure Key Vault AWS Secrets Manager Cloud Key Management Service (KMS) Azure AD B2C Amazon Cognito Identity Platform Networking Azure Virtual Network Amazon VPC Virtual Private Cloud (VPC) Azure Load Balancer Elastic Load Balancing (ELB) Google Cloud Load Balancing Azure Application Gateway AWS Application Load Balancer Google Cloud Load Balancing Azure VPN Gateway AWS VPN Cloud VPN Azure CDN Amazon CloudFront Cloud CDN Azure ExpressRoute AWS Direct Connect Cloud Interconnect Analytics and Big Data Azure HDInsight Amazon EMR (Elastic MapReduce) Cloud Dataproc Azure Databricks Amazon EMR Cloud Dataproc Azure Stream Analytics Amazon Kinesis Cloud Dataflow Azure Data Factory AWS Glue Cloud Data Fusion Azure Analysis Services Amazon Redshift BigQuery Azure Data Lake Analytics Amazon Athena BigQuery Machine Learning Azure Machine Learning Amazon SageMaker AI Platform Azure Cognitive Services Amazon Rekognition / Amazon Polly / Amazon Comprehend Cloud Vision API / Cloud Text-to-Speech API / Cloud Natural Language API Development Tools Azure DevOps AWS CodePipeline / AWS CodeBuild / AWS CodeDeploy Cloud Build / Cloud Deployment Manager Azure DevTest Labs AWS Device Farm Firebase Test Lab Azure SDKs &amp; CLI AWS SDKs &amp; CLI Cloud SDK &amp; CLI Azure Logic Apps AWS Step Functions Cloud Workflows / Cloud Functions Azure API Management Amazon API Gateway Apigee API Platform Azure Functions AWS Lambda Google Cloud Functions Azure Repos AWS CodeCommit Cloud Source Repositories Management and Monitoring Azure Monitor Amazon CloudWatch Stackdriver Monitoring / Operations Azure Security Center AWS Security Hub Cloud Security Command Center Azure Automation AWS Systems Manager Cloud Scheduler Azure Resource Manager AWS CloudFormation Cloud Deployment Manager Azure Policy AWS Organizations Organization Policy Service Azure Advisor AWS Trusted Advisor Recommender Azure Service Health AWS Personal Health Dashboard Cloud Status Dashboard Azure Cost Management AWS Cost Explorer Cloud Billing Azure Backup AWS Backup Cloud Storage Transfer Service Security Azure Security Center AWS Security Hub Cloud Security Command Center Azure Sentinel AWS Security Hub / AWS GuardDuty Chronicle Security / Security Command Center Azure Key Vault AWS Secrets Manager Cloud Key Management Service (KMS) Azure Advanced Threat Protection AWS GuardDuty Cloud Security Scanner"},{"location":"developertools/cheatsheets/dig-cheat-sheet/","title":"dig Commands","text":""},{"location":"developertools/cheatsheets/dig-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used dig commands with examples.</p> <p>The <code>dig</code> command is a network administration tool used for querying <code>Domain Name System</code> (DNS) servers. It is commonly used on Unix-like operating systems, including Linux. The name <code>dig</code> stands for <code>domain information groper.</code></p> <p>This command is useful for retrieving various types of DNS information, such as IP addresses associated with domain names, mail exchange records, name server information, and more.</p>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#installing-dig","title":"Installing dig","text":"<p>Use the following commands to install dig in MacOS and Linux environments. The installation process varies depending on the operating system. </p>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#linux-debianubuntu","title":"Linux (Debian/Ubuntu):","text":"<p>On Debian-based systems, you can use the package manager, <code>apt</code>, to install <code>dnsutils</code>, which includes <code>dig</code>:</p> <pre><code>sudo apt update\nsudo apt install dnsutils\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#linux-red-hatcentosfedora","title":"Linux (Red Hat/CentOS/Fedora):","text":"<p>On Red Hat-based systems, you can use the <code>yum</code> or <code>dnf</code> package manager:</p> <pre><code># For CentOS 7 and earlier or RHEL 7 and earlier\nsudo yum install bind-utils\n\n# For CentOS 8, RHEL 8, and Fedora\nsudo dnf install bind-utils\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#macos","title":"macOS:","text":"<p>On macOS, you can use the package manager <code>brew</code> to install <code>dig</code>:</p> <pre><code># Install Homebrew if you haven't already\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install dig\nbrew install bind\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-help","title":"dig help","text":"<p>Displays general help information about dig, including a list of available commands and options.</p> <pre><code>dig -h\n# or\ndig -help\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-version","title":"dig version","text":"<p>Displays version informatio of dig</p> <pre><code>dig -v\n# or\ndig version\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-command-syntax","title":"dig Command Syntax","text":"<pre><code>dig [options] [domain]\n</code></pre> <ul> <li><code>[options]</code>: Additional parameters to customize the query.</li> <li><code>[domain]</code>: The domain you want to query.</li> </ul>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-a-domain-name","title":"Using dig for a domain name:","text":"<p><pre><code>dig anjikeesari.com\n</code></pre> output  <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 26652\n;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  A\n\n;; ANSWER SECTION:\nanjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\nanjikeesari.com.    600 IN  A   185.199.108.153\n\n;; Query time: 137 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:28:58 PST 2023\n;; MSG SIZE  rcvd: 108\n</code></pre></p>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-short-answers","title":"Using dig for short answers:","text":"<pre><code>dig +short anjikeesari.com\n</code></pre> <p>output</p> <pre><code>185.199.108.153\n185.199.109.153\n185.199.110.153\n185.199.111.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-detailed-answers","title":"Using dig for detailed answers:","text":"<p><pre><code>dig anjikeesari.com +noall +answer\n</code></pre> output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com +noall +answer\n;; global options: +cmd\nanjikeesari.com.    600 IN  A   185.199.108.153\nanjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-specifying-name-servers","title":"Using dig for specifying name servers:","text":"<pre><code>dig NS anjikeesari.com \n# or\ndig NS anjikeesari.com +short\n</code></pre> <p>output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; NS anjikeesari.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2372\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 5\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  NS\n\n;; ANSWER SECTION:\nanjikeesari.com.    3600    IN  NS  ns72.domaincontrol.com.\nanjikeesari.com.    3600    IN  NS  ns71.domaincontrol.com.\n\n;; ADDITIONAL SECTION:\nns71.domaincontrol.com. 35357   IN  A   97.74.105.46\nns71.domaincontrol.com. 35656   IN  AAAA    2603:5:2194::2e\nns72.domaincontrol.com. 35357   IN  A   173.201.73.46\nns72.domaincontrol.com. 35656   IN  AAAA    2603:5:2294::2e\n\n;; Query time: 40 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:36:48 PST 2023\n;; MSG SIZE  rcvd: 184\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-auery-all-dns-record-types","title":"Using dig for auery all DNS record types:","text":"<pre><code>dig anjikeesari.com ANY\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-to-search-for-record-type","title":"Using dig to search for record type:","text":"<p>Querying TXT records</p> <pre><code>dig anjikeesari.com -t TXT\n</code></pre> <p>output  <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com -t TXT\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 28965\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  TXT\n\n;; AUTHORITY SECTION:\nanjikeesari.com.    600 IN  SOA ns71.domaincontrol.com. dns.jomax.net. 2023071000 28800 7200 604800 600\n\n;; Query time: 101 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:34:22 PST 2023\n;; MSG SIZE  rcvd: 112\n</code></pre></p> <p>Querying A records </p> <pre><code>dig +nocmd anjikeesari.com a +noall +answer\n</code></pre> <p>output</p> <pre><code>anjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\nanjikeesari.com.    600 IN  A   185.199.108.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-to-trace-dns-path","title":"Using dig to trace DNS path","text":"<pre><code>dig +trace anjikeesari.com\n# or\ndig anjikeesari.com +trace\n</code></pre> <p>Querying CNAME records </p> <pre><code>dig +nocmd mail.google.com cname +noall +answer\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-mail-server-for-the-domain","title":"Using dig for mail server for the domain:","text":"<pre><code>dig MX anjikeesari.com +short\n# or \ndig MX anjikeesari.com\n</code></pre> <p>Querying MX records</p> <pre><code>dig +nocmd anjikeesari.com ms +noall +answer\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-reverse-dns-lookup","title":"Using dig for reverse DNS lookup:","text":"<p><pre><code>dig -x 185.199.108.153\n# or \ndig -x 185.199.108.153 +short\n</code></pre> output <pre><code>cdn-185-199-108-153.github.com.\n</code></pre></p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/","title":"Docker Commands","text":""},{"location":"developertools/cheatsheets/docker-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used <code>Docker</code> commands</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#installing-docker","title":"Installing Docker","text":"<p>Here are the commands to install Docker on different operating systems:</p> <pre><code># Ubuntu/Debian:\nsudo apt-get update\nsudo apt-get install docker.io\n\n\n# MacOS (using Homebrew):\nbrew install docker\n\n# Windows OS (using choco)\nchoco install docker-desktop\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-install-verify","title":"Docker Install verify","text":"<p>To know docker is installed or not</p> <pre><code>which docker\n\n# output\n/usr/bin/docker\n</code></pre> <p>What is the version installed on your machine</p> <pre><code>docker -version\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#general-commands","title":"General Commands","text":"<p>Start the docker daemon</p> <pre><code>docker -d\n</code></pre> <p>Get help with Docker. Can also use \u2013help on all subcommands</p> <pre><code>docker --help\n</code></pre> <p>Display system-wide information</p> <pre><code>docker info\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-image","title":"Docker Image","text":"<p>Docker image is a lightweight, standalone, and executable package that contains everything needed to run a piece of software, including the code, runtime, system tools, libraries, and dependencies. </p> <pre><code># List local images\ndocker images\n\n# Delete an Image\ndocker rmi &lt;image_name&gt;\n\n# Remove all unused images\ndocker image prune\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-build","title":"Docker Build","text":"<p>Build an image from a Dockerfile</p> <pre><code># Build an image from a Dockerfile and tag it with a specified name.\ndocker build -t &lt;image_name&gt;\n\n# build an image and tag with naming conventions\ndocker build -t projectname/domainname/appname:yyyymmdd.sequence .\n# Example\ndocker build -t sample/aspnet-api:20230226.1 .\n\n# Build an image from a Dockerfile without the cache\ndocker build -t &lt;image_name&gt; . \u2013no-cache\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-run","title":"Docker Run","text":"<pre><code># Create and run a container from an image, with a custom name:\ndocker run --name &lt;container_name&gt; &lt;image_name&gt;\n\n# Run a container with and publish a container\u2019s port(s) to the host.\ndocker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n\n# Run a container in the background\ndocker run -d &lt;image_name&gt;\n\n# Remove a stopped container:\ndocker rm &lt;container_name&gt;\n\n# Example: \ndocker run --rm -p 8080:80 project1/domain1/app1:20230226.1\n</code></pre> <ul> <li>--rm: This option automatically removes the container when it exits. It ensures that the container is cleaned up after it finishes running. This is useful for temporary or disposable containers.</li> <li>-p 8080:80: This option maps the host machine's port 8080 to the container's port 80. It establishes a network connection between the host and the container, allowing access to the containerized application via port 8080 on the host.</li> </ul> <p>Exit the container</p> <pre><code>exit\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-push","title":"Docker Push","text":"<pre><code># Publish an image to Docker Hub\ndocker push &lt;username&gt;/&lt;image_name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-container","title":"Docker container","text":"<p>A Docker container is a lightweight, standalone, and executable runtime instance of a Docker image. It represents a running process that is isolated from the host system and other containers. Docker container providing a consistent and reproducible environment for running applications. Containers are highly portable and can be easily moved and deployed across different environments, such as development, testing, staging, and production. </p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub is a cloud-based registry service provided by Docker that allows developers to store and share container images. It serves as a centralized repository for Docker images,</p> <pre><code># Login into Docker\ndocker login -u &lt;username&gt;\n\n# Publish an image to Docker Hub\ndocker push &lt;username&gt;/&lt;image_name&gt;\n\n# Search Hub for an image\ndocker search &lt;image_name&gt;\n\n# Pull an image from a Docker Hub\ndocker pull &lt;image_name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-network","title":"Docker network","text":"<p>This command creates a new bridge network named \"network1\" that containers can connect to for networked communication.</p> <pre><code>docker network create -d bridge network1\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#clean-up-resources","title":"Clean up resources","text":"<p>you can use the <code>docker system prune</code> command to clean up all dangling or unused resources, including images, containers, volumes, and networks that are not tagged or connected to a running container. This command is helpful for freeing up disk space and removing unnecessary resources.</p> <pre><code># before cleaning up Docker, first check all the available resources using the following commands:\n\ndocker  container ls\ndocker  image ls\ndocker  volume ls\ndocker  network ls\ndocker  info\n\ndocker system prune\n# or\ndocker system prune -a\n</code></pre> <p>If you need to clean up all containers and images locally in Docker Desktop, you can use the following commands:</p> <pre><code># To delete all containers including its volumes use,\ndocker rm -vf $(docker ps -aq)\n\n# To delete all volumes use,\ndocker volume rm $(docker volume ls -q)\n\n# To delete all the images,\ndocker rmi -f $(docker images -aq)\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-compose-commands","title":"Docker Compose Commands","text":"<p>Below are some commonly used Docker Compose commands:</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#starts-services","title":"Starts services","text":"<pre><code>docker-compose up\n</code></pre> <p>Starts the services defined in your <code>docker-compose.yml</code> file. It creates and starts containers as specified in the configuration.</p> <pre><code>docker-compose up -d\n</code></pre> <p>Starts the services in the background (detached mode).</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#stops-services","title":"Stops services","text":"<p><pre><code>docker-compose down\n</code></pre> Stops and removes containers, networks, volumes, and other services defined in your <code>docker-compose.yml</code> file.</p> <p><pre><code>docker-compose down -v\n</code></pre> Stops and removes containers, networks, volumes, and other services while also removing volumes.</p> <p><pre><code>docker-compose down --volumes --rmi all\n</code></pre> Stops and removes containers, networks, volumes, and other services, while also removing volumes and images.    </p> <pre><code>docker-compose stop\n</code></pre> <p>Stops the services defined in your <code>docker-compose.yml</code> file without removing them.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#lists-the-containers","title":"Lists the containers","text":"<pre><code>docker-compose ps\n</code></pre> <p>Lists the containers that are part of your Docker Compose setup, showing their status.</p> <p><pre><code>docker-compose ps -a\n</code></pre> Lists all containers, including stopped ones, that are part of your Docker Compose setup.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#displays-log","title":"Displays log","text":"<p><pre><code>docker-compose logs\n</code></pre> Displays log output from services. You can use the <code>-f</code> option to follow the logs in real-time.</p> <pre><code>docker-compose logs webserver\n</code></pre> <p>Displays logs for a specific service.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#executes-a-command","title":"Executes a command","text":"<p><pre><code>docker-compose exec webserver ls -l\n</code></pre> Executes a command in a running service container.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#builds-services","title":"Builds services","text":"<p><pre><code>docker-compose build\n</code></pre> Builds or rebuilds services defined in your <code>docker-compose.yml</code> file.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#restarts-services","title":"Restarts services","text":"<p><pre><code>docker-compose restart\n</code></pre> Restarts services.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#displays-configuration","title":"Displays configuration","text":"<p><pre><code>docker-compose config\n</code></pre> Validates and displays the configuration of your <code>docker-compose.yml</code> file.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#pauses-services","title":"Pauses services","text":"<p><pre><code>docker-compose pause\n</code></pre> Pauses all services. Containers remain running, but they stop processing requests.</p> <p><pre><code>docker-compose unpause\n</code></pre> Unpauses services after they have been paused.</p> <p><pre><code>docker-compose top\n</code></pre> Displays the running processes of a service.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#scales-service","title":"Scales service","text":"<p><pre><code>docker-compose scale webserver=3\n</code></pre> Scales a service to the specified number of instances.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#display-events","title":"Display events","text":"<pre><code>docker-compose events\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-compose-config","title":"docker compose config","text":"<p>Parse, resolve and render compose file in canonical forma</p> <pre><code>docker-compose config\n</code></pre> <p>Streams real-time events from your services.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-commands-summary","title":"Docker commands Summary","text":""},{"location":"developertools/cheatsheets/docker-cheat-sheet/#basic-commands","title":"Basic Commands","text":"<ul> <li><code>docker run [image]</code>: Start a new container from an image</li> <li><code>docker ps</code>: List all running containers</li> <li><code>docker stop [container]</code>: Stop a running container</li> <li><code>docker rm [container]</code>: Remove a container</li> <li><code>docker images</code>: List all available images</li> <li><code>docker pull [image]</code>: Download an image from a registry</li> <li><code>docker push [image]</code>: Upload an image to a registry</li> <li><code>docker build [options] [path]</code>: Build an image from a Dockerfile</li> </ul>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#advanced-commands","title":"Advanced Commands","text":"<ul> <li><code>docker exec [container] [command]</code>: Run a command inside a running container</li> <li><code>docker-compose up</code>: Start a Docker Compose application</li> <li><code>docker network [subcommand]</code>: Manage Docker networks</li> <li><code>docker volume [subcommand]</code>: Manage Docker volumes</li> <li><code>docker logs [container]</code>: View the logs of a container</li> <li><code>docker inspect [container]</code>: Inspect a container</li> <li><code>docker diff [container]</code>: Show changes to the filesystem of a container</li> <li><code>docker commit [container] [image]</code>: Create a new image from a container's changes</li> <li><code>docker save [image]</code>: Save an image to a tar archive</li> <li><code>docker load</code>: Load an image from a tar archive</li> </ul>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#references","title":"References","text":"<ul> <li>Overview of docker compose CLI</li> </ul>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/","title":"Dockerfile Commands","text":""},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used <code>Dockerfile</code> commands</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text document that contains instructions for building a Docker image. Docker can automatically build images by interpreting instructions from a Dockerfile. This page outlines the commands available for use within a Dockerfile.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#1-from","title":"1. FROM","text":"<p>Specifies the base image for your Docker image. <pre><code>FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]\n</code></pre> Example: <pre><code>FROM mcr.microsoft.com/dotnet/sdk:5.0 AS build\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#2-run","title":"2. RUN","text":"<p>Executes commands in the shell of the container. <pre><code>RUN &lt;command&gt;\n</code></pre> Example: <pre><code>RUN apt-get update &amp;&amp; apt-get install -y \\\n    git\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#3-copy","title":"3. COPY","text":"<p>Copies files or directories from the build context to the container's filesystem. <pre><code>COPY &lt;src&gt; &lt;dest&gt;\n</code></pre> Example: <pre><code>COPY . /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#4-workdir","title":"4. WORKDIR","text":"<p>Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it. <pre><code>WORKDIR /path/to/directory\n</code></pre> Example: <pre><code>WORKDIR /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#5-cmd","title":"5. CMD","text":"<p>Specifies the default command to run when the container starts. <pre><code>CMD [\"executable\", \"param1\", \"param2\"]\n</code></pre> Example: <pre><code>CMD [\"dotnet\", \"MyApi.dll\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#6-entrypoint","title":"6. ENTRYPOINT","text":"<p>Specifies the command to run when the container starts, allowing arguments to be passed. <pre><code>ENTRYPOINT [\"executable\", \"param1\", \"param2\"]\n</code></pre> Example: <pre><code>ENTRYPOINT [\"dotnet\", \"MyApi.dll\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#7-expose","title":"7. EXPOSE","text":"<p>Informs Docker that the container listens on specific network ports at runtime. <pre><code>EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]\n</code></pre> Example: <pre><code>EXPOSE 80\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#8-env","title":"8. ENV","text":"<p>Sets environment variables. <pre><code>ENV &lt;key&gt; &lt;value&gt;\n</code></pre> Example: <pre><code>ENV ASPNETCORE_ENVIRONMENT=Production\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#9-arg","title":"9. ARG","text":"<p>Defines build-time variables. <pre><code>ARG &lt;name&gt;[=&lt;default value&gt;]\n</code></pre> Example: <pre><code>ARG CONNECTION_STRING\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#10-volume","title":"10. VOLUME","text":"<p>Creates a mount point and marks it as holding externally mounted volumes from native host or other containers. <pre><code>VOLUME /path/to/volume\n</code></pre> Example: <pre><code>VOLUME /var/log/app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#11-label","title":"11. LABEL","text":"<p>Adds metadata to an image. <pre><code>LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...\n</code></pre> Example: <pre><code>LABEL maintainer=\"John Doe &lt;john@example.com&gt;\"\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#12-user","title":"12. USER","text":"<p>Sets the user or UID to use when running the image. <pre><code>USER &lt;username | UID&gt;\n</code></pre> Example: <pre><code>USER appuser\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#13-healthcheck","title":"13. HEALTHCHECK","text":"<p>Defines a command to periodically check the container's health. <pre><code>HEALTHCHECK [OPTIONS] CMD &lt;command&gt;\n</code></pre> Example: <pre><code>HEALTHCHECK --interval=30s --timeout=3s \\\n  CMD curl -f http://localhost/health || exit 1\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#14-onbuild","title":"14. ONBUILD","text":"<p>Adds a trigger instruction when the image is used as the base for another build. <pre><code>ONBUILD &lt;INSTRUCTION&gt;\n</code></pre> Example: <pre><code>ONBUILD COPY . /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#15-stopsignal","title":"15. STOPSIGNAL","text":"<p>Sets the system call signal that will be sent to the container to exit. <pre><code>STOPSIGNAL signal\n</code></pre> Example: <pre><code>STOPSIGNAL SIGTERM\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#16-shell","title":"16. SHELL","text":"<p>Overrides the default shell used for the shell form of commands. <pre><code>SHELL [\"executable\", \"parameters\"]\n</code></pre> Example: <pre><code>SHELL [\"/bin/bash\", \"-c\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#cmd-vs-entrypoint","title":"CMD vs ENTRYPOINT","text":"<p>CMD and ENTRYPOINT are used to specify the default command to run when a container is started. However, they have different behaviors and can be used together in different ways depending on the requirements of your Docker image.</p> <ul> <li> <p>CMD:</p> </li> <li> <p>Sets default command and/or parameters.</p> </li> <li>Can be overridden from the command line.</li> <li> <p>Last <code>CMD</code> instruction takes effect if multiple are present.</p> </li> <li> <p>ENTRYPOINT:</p> </li> <li> <p>Specifies main executable to run.</p> </li> <li>Allows arguments to be passed.</li> <li>Arguments passed to <code>docker run</code> are appended to the <code>ENTRYPOINT</code> command.</li> <li>Last <code>ENTRYPOINT</code> instruction takes effect if multiple are present.</li> </ul> <p>Best Practices:</p> <ul> <li>Use CMD for default command and parameters.</li> <li>Use ENTRYPOINT for main executable, allowing additional arguments.</li> </ul> <p>Example:</p> <pre><code>ENTRYPOINT [\"dotnet\", \"MyApi.dll\"]\n</code></pre> <p>In this example, <code>dotnet MyApi.dll</code> is the main executable, with any additional arguments passed when running the container.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#copy-vs-add","title":"COPY vs ADD","text":"<p>COPY and ADD are used to copy files and directories from the host machine into the container's filesystem. While they have similar functionalities, there are some differences between them.</p> <p>COPY Instruction:</p> <p>The COPY instruction copies files or directories from the build context (i.e., the directory containing the Dockerfile) into the container's filesystem. It can copy local files/directories as well as files/directories from URLs. However, it does not support extracting files from compressed archives (e.g., .tar.gz).</p> <p>ADD Instruction:</p> <p>The ADD instruction has the same functionality as COPY, but it also supports additional features such as extracting compressed archives (e.g., .tar.gz) and copying files from URLs. However, because of these additional features, it's considered less predictable and is recommended to use COPY instead unless the extra functionality of ADD is specifically required.</p> Feature COPY ADD Functionality Copies files/directories from build context Same as COPY, plus supports additional features like extracting compressed archives and copying files from URLs Predictability More predictable and straightforward Provides additional functionality but less predictable Best Practice Preferred for basic file copying tasks Use sparingly, only when additional features are needed <p>In summary, <code>COPY</code> is preferred for basic file copying tasks due to its predictability, while <code>ADD</code> offers additional functionality but should be used with caution.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#references","title":"References","text":"<ul> <li>Dockerfile reference</li> </ul>"},{"location":"developertools/cheatsheets/git-cheat-sheet/","title":"Git Commands","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Git commands with examples. </p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#installing-git","title":"Installing git","text":"<p>Here are the commands to install Git on different operating systems:</p> <pre><code># Ubuntu/Debian:\nsudo apt-get install git\n\n# MacOS (using Homebrew):\nbrew install git\n\n# Windows OS (using choco)\nchoco install git\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#setting-up-git-configuration","title":"Setting up git configuration:","text":"<p>To begin, it's important to configure your Git settings, associating your name and email with your commits. Use the following commands to set your name and email respectively:</p> <pre><code>git config --global user.name \"anji.keesari\"\ngit config --global user.email \"anjkeesari@gmail.com\"\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#caching-credentials","title":"Caching credentials:","text":"<p>Typing in login credentials repeatedly can be time consuming. To streamline this process, you can store your credentials in the cache using the command:</p> <pre><code>git config --global credential.helper cache\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#enable-automatic-coloring-of-git-output","title":"Enable automatic coloring of Git output","text":"<p>This command is used to enable automatic coloring of Git output in the command line interface. Enabling this option enhances the readability of Git's output by applying different colors to various elements.</p> <pre><code>git config --global color.ui auto\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#checking-git-configuration","title":"Checking git configuration:","text":"<p>To verify your Git configuration, including your username and email, use the following command:</p> <pre><code>git config -l\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#initializing-git","title":"Initializing git","text":"<p>Before diving into Git commands, you need to initialize a new Git repository locally in your project's root directory. Execute the command:</p> <pre><code>git init\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-clone","title":"Git clone","text":"<p>To work on an existing Git repository, you can clone it using the command</p> <pre><code>git clone &lt;repository-url&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#adding-files-to-the-staging-area","title":"Adding files to the staging area:","text":"<p>To stage changes and prepare them for commit, use the git add command. You can add specific files or entire directories to the staging area using the following commands:</p> <pre><code>git add &lt;file-name&gt;             # Add a specific file\ngit add .                       # Add all changes in the current directory (excluding deletions)\ngit add test*                   # Add all files starting with 'test' in the current directory\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#committing-changes","title":"Committing changes:","text":"<p>Committing changes captures a snapshot of your code at a specific point in time. Use the following commands to commit your changes:</p> <pre><code>git commit -m \"(message)\"       # Commits the changes with a custom message\ngit commit -am \"(message)\"      # Adds all changes to staging and commits them with a custom message\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-log","title":"Git log","text":"<p>To view the commit history of a repository, use the <code>git log</code> command. It provides you with an overview of past commits and their respective details. also, you can use <code>git log -p</code> to see the commit history along with the changes made to each file.</p> <pre><code>#  shows the commit history for the current repository:\ngit log\n# commit's history including all files and their changes:\ngit log -p\n\npress q any time to quit\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#commit-details","title":"Commit details","text":"<p>Use this command to see a specific commit in details</p> <p><pre><code>git show commit-id\n</code></pre> Note: replace <code>commit-id</code> with the id of the commit that you can find in the git log</p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-status","title":"Git status","text":"<p>This command will show the status of the current repository including staged, unstaged, and untracked files.</p> <pre><code>git status\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#undoing-changes","title":"Undoing changes:","text":"<p>If you have already pushed a commit to a remote repository and want to undo it, you need to create a new commit that undoes the changes. The following command will create a new commit that undoes the changes introduced by the specified commit:</p> <p><pre><code>git revert &lt;commit-id&gt;\n</code></pre> Replace  with the ID of the commit you want to undo. <p>If you have already committed changes and want to undo the most recent commit, you have a few options depending on your desired outcome: - Undo the commit and keep the changes as unstaged modifications: <pre><code>git reset HEAD^\n</code></pre> Undo the commit and completely discard the changes: <pre><code>git reset --hard HEAD^\n</code></pre></p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#viewing-differences","title":"Viewing differences","text":"<p>To compare the differences between versions, you can use the git diff command. It displays the changes made to files since the last commit. </p> <pre><code>git diff\n</code></pre> <p>This will show the line-by-line differences between the current state of the files and the last committed version.</p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#pushing-changes","title":"Pushing changes","text":"<p>To push your local commits to a remote repository, you need to use following command.</p> <pre><code>git push origin &lt;branch-name&gt;\n\n# if you haven't set the upstream branch yet, you can use this\n\ngit push --set-upstream origin aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#pulling-changes","title":"Pulling changes","text":"<p>Use this command to incorporate the latest changes from a remote repository into your local repository.</p> <pre><code>git pull origin &lt;branch-name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-fetch","title":"Git fetch","text":"<p>To fetch the latest changes from the remote repository without merging them into your local branches.</p> <pre><code>git fetch\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#creating-a-new-branch","title":"Creating a new branch:","text":"<p>To create a new branch in Git, you can use the git branch command followed by the name of the branch you want to create. </p> <pre><code>git branch &lt;branch-name&gt;\n\n# Creates a new branch, `aspnet-api` is name of the branch here\ngit branch aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#switching-branch","title":"Switching branch:","text":"<p>To switch to a different branch in your Git repository, you can utilize the <code>git checkout</code> command followed by the name of the branch you want to switch to. </p> <pre><code>git checkout &lt;branch-name&gt;\n\n# Switched to branch 'aspnet-api'\ngit checkout aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#list-branches","title":"List branches","text":"<p>It will show a list of all branches and mark the current branch with an asterisk and highlight it in green.</p> <pre><code># Shows the list of all branches.\ngit branch  \n# List all local branches in repository. With -a: show all branches (with remote).\ngit branch -a \n\n# press q to quit\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#get-remote-urls","title":"Get remote URLs","text":"<p>You can see all remote repositories for your local repository with this command:</p> <pre><code>git remote -v\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#more-info-about-a-remote-repo","title":"More info about a remote repo","text":"<p>How to get more info about a remote repo in Git:</p> <pre><code>git remote show origin\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#merging-branches","title":"Merging branches","text":"<p>In Git, merging allows you to combine the changes from one branch into another. To merge a branch into another branch, you can use the git merge command followed by the name of the branch you want to merge. Here's an example:</p> <pre><code>git merge &lt;branch-name&gt;\n\n# For instance, if you want to merge the changes from the develop branch into the main branch\n# cd to the folder\ngit checkout main\ngit merge develop\n</code></pre> <p>After performing the merge, it's a good practice to check the status of your repository using git status to ensure that the merge was successful and there are no conflicts to resolve. also, you can view the commit history using git log to see the merged commits and their details.</p> <pre><code>git status\ngit logs\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#delete-branch","title":"Delete branch","text":"<p>To delete a branch in Git, you can use either of the following commands:</p> <pre><code>git branch --delete &lt;branch-name&gt;\ngit branch -d &lt;branch-name&gt;\n\nexample\n# git branch to see list of branches before delete\ngit branch\n# delete the branch\ngit branch --delete &lt;branch-name&gt;\n# git branch again to see list of branches after delete\ngit branch\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#branch-from-a-previous-commit","title":"Branch from a previous commit","text":"<p>To create a new branch in Git using a specific commit hash, you can use the git branch command followed by the name of the branch and the commit hash</p> <pre><code>git branch branch_name &lt;commit-hash&gt;\n# Step 1: Create the branch from the commit hash\n\ngit branch new_branch 07615d50afde24d21e2180b90d3a0a58ec131980\n\n# this will create the local branch\n\n# Step 2: Switch to the new branch &amp; commit\n\ngit commit -am \u201c(message)\u201d \n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#rollback-an-old-commit","title":"Rollback an old commit","text":"<p>You can revert an old commit using its commit id. </p> <pre><code>git revert comit_id\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#how-to-resolve-merge-conflicts-using-git-commands","title":"How to resolve merge conflicts using git commands","text":"<p>Resolving merge conflicts in Git involves editing the conflicted files to choose which changes to keep and which to discard, and then committing the resolved changes. Here's a step-by-step guide:</p> <ol> <li> <p>Check the status of your repository to see if there are any merge conflicts:</p> <p><pre><code>git status\n</code></pre> If there are merge conflicts, you will see a message indicating which files have conflicts.</p> </li> <li> <p>Open the conflicted files in a text editor and look for the conflict markers. The markers will look something like this:</p> <p><pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the content from the current branch.\n=======\nThis is the content from the branch you are merging.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch-name&gt;\n</code></pre> 1. Decide what changes you want to keep and remove the conflict markers and any unnecessary content. The final content should only include the changes you want to keep.</p> </li> <li> <p>Stage the changes using git add:     <pre><code>git add &lt;file-name&gt;\n</code></pre></p> </li> <li>Commit the changes to the repository:     <pre><code>git commit -m \"Resolved merge conflicts\"\n</code></pre></li> <li>Push the changes to the remote repository if necessary:     <pre><code>git push origin &lt;branch-name&gt;\n</code></pre></li> </ol>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#temporary-commits","title":"Temporary commits","text":"<p>In Git, you can use temporary commits to store modified, tracked files temporarily, allowing you to switch branches without losing your changes. This is a useful technique when you want to work on a different branch but are not ready to commit your changes yet.</p> <ul> <li>Stash your changes:  This will create a temporary commit that stores your modifications, allowing you to switch branches. <pre><code>git stash\n</code></pre></li> <li>Git stash list Running this command will show you the stash ID, along with a description that includes the branch name and commit message. <pre><code>git stash list\n</code></pre></li> <li>Git stash pop: his command is used to apply the changes from the top of the stash stack and remove that stash from the stack. <pre><code>git stash pop:\n</code></pre></li> <li>Git stash drop:  This command allows you to discard a stash from the stash stack. It permanently removes a stash and its changes, freeing up space in the stack. <pre><code>git stash drop: \n</code></pre></li> </ul>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/","title":"Helm Commands","text":""},{"location":"developertools/cheatsheets/helm-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Helm commands with examples.</p> <p>Helm is a package manager for Kubernetes that helps you manage, install, and upgrade applications and services in a Kubernetes cluster.</p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#basic-concepts","title":"Basic Concepts","text":"<p>Having a recap of these Helm concepts will greatly assist you in following this article and understanding the Helm commands more easily.</p> <ul> <li>A Helm chart is a package containing all the necessary files, templates, and metadata required to describe and deploy an application on a Kubernetes cluster. </li> <li>Helm repositories are locations where Helm charts are stored and can be accessed for installation. Helm supports both public and private repositories. Public repositories, such as the official Helm Hub, provide a wide range of community-contributed charts. Private repositories can be set up within organizations to distribute custom charts internally.</li> <li>-The Helm Hub is a centralized repository maintained by the Helm community. It serves as a catalog of Helm charts contributed by the community members. The Helm Hub provides a convenient way to discover and search for charts that you can use in your applications. You can find charts for various applications, databases, services, and more on the Helm Hub.</li> <li>A Helm release represents an instance of a chart deployed on a Kubernetes cluster. When you install a Helm chart, it creates a release with a unique name. Each release has its own set of resources, configuration values, and version history. Releases are managed by Helm, allowing you to upgrade, rollback, and delete them easily. Helm maintains a release history, allowing you to view and manage the different versions of a release.</li> </ul>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster configured</li> <li>Helm package manager installed</li> </ul>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#installing-helm","title":"Installing Helm","text":"<p>Use the following commands to install Helm package manager in MacOS and Windows.</p> <pre><code># MacOS (using Homebrew):\nbrew install helm\n\n# Windows OS (using Choco)\nchoco install kubernetes-helm\n\n#  verify the installation by running \nhelm version\n</code></pre> <p>For more information, refer to the official Helm documentation:  - https://helm.sh/docs/intro/install/</p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#azure-login","title":"Azure login","text":"<p>Login into azure first.</p> <pre><code>az login\naz account list --output table\n\n# Select the subscription\naz account set -s \"anji.keesari\"\n\n# Connect to Azure Kubernetes Service Cluster with User Role\naz aks get-credentials -g \"rg-rgname-dev\" -n \"aks-clustername-dev\"\n\n# Verify the cluster info\nkubectl cluster-info\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-version","title":"Helm Version","text":"<p>See the installed version of Helm.</p> <pre><code>helm version\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-help","title":"Helm --help","text":"<p>Display the general help output for Helm</p> <pre><code>helm --help\nhelm [command] --help \n\n# examples\nhelm repo --help\nhelm repo list --help\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-repositories","title":"Helm repositories","text":"<p>List the available Helm repositories. </p> <p><pre><code>helm repo list\n</code></pre> output</p> <p><pre><code>NAME                    URL\nbitnami                 https://charts.bitnami.com/bitnami\nrunix                   https://helm.runix.net\ningress-nginx           https://kubernetes.github.io/ingress-nginx\njetstack                https://charts.jetstack.io\nprometheus-community    https://prometheus-community.github.io/helm-charts\napache-solr             https://solr.apache.org/charts\nazure-marketplace       https://marketplace.azurecr.io/helm/v1/repo\nbitnami-azure           https://marketplace.azurecr.io/helm/v1/repo\n</code></pre> Update list of Helm charts from repositories</p> <p><pre><code>helm repo update\n</code></pre> output <pre><code>Hang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"jaegertracing\" chart repository\n...Successfully got an update from the \"jetstack\" chart repository\n...Successfully got an update from the \"runix\" chart repository\n...Successfully got an update from the \"prometheus-community\" chart repository\n...Successfully got an update from the \"apache-solr\" chart repository\n...Successfully got an update from the \"azure-marketplace\" chart repository\n...Successfully got an update from the \"bitnami-azure\" chart repository\n...Successfully got an update from the \"bitnami\" chart repository\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#search-helm-repo","title":"Search helm repo","text":"<p><code>Helm search repo</code> searches the repositories that you have added to your local helm client (with helm repo add)</p> <pre><code>helm search &lt;chart-name&gt;\n# Examples\nhelm search repo bitnami\nhelm search repo azure-marketplace\nhelm search repo wordpress\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#search-helm-hub","title":"Search Helm Hub","text":"<p>The <code>helm search hub</code> command allows you to search for Helm charts specifically in the official Helm Hub repository (Artifact Hub). The Helm Hub is a centralized repository of publicly available charts maintained by the Helm community.</p> <pre><code>helm search hub &lt;chart-name&gt;\n# Examples\nhelm search hub hub\nhelm search hub bitnami\nhelm search hub microsoft\nhelm search hub wordpress \nhelm search hub prometheus\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#add-a-repository","title":"Add a repository","text":"<p>Add a repository from the internet</p> <pre><code>helm repo add [repository-name] [url]\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#remove-a-repository","title":"Remove a repository","text":"<p>Remove a repository from your system:</p> <pre><code>helm repo remove [repository-name]\nhelm repo remove bitnami\n</code></pre> <p>output <pre><code>\"bitnami\" has been removed from your repositories\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#list-installed-helm-charts","title":"List Installed Helm Charts","text":"<p>List Installed Helm Charts in default namespace</p> <pre><code>helm ls\n</code></pre> <p>output</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nmy-minio        default         1               2022-12-05 16:45:31.6927425 -0800 PST   deployed        minio-11.10.16  2022.11.11\n</code></pre> <p>List Installed Helm Charts from all namespace</p> <pre><code>helm ls -aA\n</code></pre> <p>output</p> <pre><code>NAME                                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncert-manager                            cert-manager    1               2022-11-09 17:54:00.8093352 -0800 PST   deployed        cert-manager-v1.10.0                    v1.10.0\ncsi-secrets-store-provider-azure        kube-system     1               2022-12-09 16:37:01.010911 -0800 PST    deployed        csi-secrets-store-provider-azure-1.3.0  1.3.0\n</code></pre> <p>List Installed Helm Charts in specific namespace</p> <pre><code>helm ls -n cert-manager\n</code></pre> <p>output</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ncert-manager    cert-manager    1               2022-11-09 17:54:00.8093352 -0800 PST   deployed        cert-manager-v1.10.0    v1.10.0\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#install-helm-chart","title":"Install helm chart","text":"<p>The <code>helm install</code> command is used to deploy a chart onto a Kubernetes cluster. It takes a chart package as input and deploys the associated resources, such as pods, services, and config maps, to the cluster.</p> <pre><code>helm install &lt;release-name&gt; &lt;chart&gt;\n</code></pre> <ul> <li><code>&lt;release-name&gt;</code> is the name you choose for the release of the chart. It is used to uniquely identify the deployed resources.</li> <li><code>&lt;chart&gt;</code> refers to the chart package or the path to the chart directory. This can be either a local chart or a chart from a remote repository.</li> </ul> <p>Install a chart from a remote repository:</p> <p><pre><code>helm install my-release stable/mysql\n</code></pre> This command installs the mysql chart from the stable repository with the release name my-release. It deploys the MySQL database to the cluster.</p> <p>Install a chart from a local directory:</p> <pre><code>helm install my-release ./my-chart\n</code></pre> <p>This command installs a chart located in the my-chart directory with the release name my-release. It deploys the resources defined in the chart to the cluster.</p> <p>Install a chart with custom configuration values:</p> <pre><code>helm install my-release stable/mysql --set mysqlRootPassword=secretpassword\n</code></pre> <p>Install a release in a specific namespace:</p> <pre><code>helm install -name release-name charts-name --namespace sample\n</code></pre> <p>Override the default values with those specified in a file of your choice:</p> <pre><code>helm install [app-name] [chart] --values [yaml-file/url]\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-dev.yaml\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-test.yaml\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-prod.yaml\n</code></pre> <p>Run a test installation to validate and verify the chart:</p> <pre><code>helm install [release-name] --dry-run --debug\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#uninstall-helm-chart","title":"Uninstall helm chart","text":"<p>To uninstall a Helm chart and delete the associated resources from your Kubernetes cluster, you can use the helm uninstall command. </p> <pre><code>helm uninstall [release]\n\n# Example\nhelm uninstall helm-release-name -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#upgrading-helm-charts","title":"Upgrading Helm Charts","text":"<p>To upgrade a Helm chart to a new version or with updated configuration, you can use the helm upgrade command. </p> <pre><code>helm upgrade [release] [chart]\nhelm upgrade -name helm-release-name helm-charts --namespace sample\n</code></pre> <p>Instruct Helm to rollback changes if the upgrade fails:</p> <pre><code>helm upgrade [release] [chart] --atomic\n</code></pre> <p>Upgrade a release. If it does not exist on the system, install it:</p> <pre><code>helm upgrade [release] [chart] --install\n</code></pre> <p>Upgrade to a specified version:</p> <pre><code>helm upgrade [release] [chart] --version [version-number]\n\n# first get the info\nhelm list --namespace sample\n\nhelm upgrade -name helm-release-name helm-charts --version 1.0.0 --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#rollback-helm-chart","title":"Rollback helm chart","text":"<p>The <code>helm rollback</code> command allows you to revert a failed or undesired Helm release upgrade to a previous version. This feature is especially useful when an upgrade introduces issues or unexpected behavior, enabling you to quickly restore the previous working state.</p> <pre><code>helm rollback [release] [revision]\n# first list installed helm charts \nhelm list --namespace sample\n# rollback to specific version.\nhelm rollback -name helm-release-name 18 --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#release-monitoring","title":"Release Monitoring","text":"<p>The helm list command enables listing releases in a Kubernetes cluster</p> <p>List all available releases in the current namespace:</p> <pre><code>helm list\n</code></pre> <p>List all available releases across <code>all namespaces</code>:</p> <p><pre><code>helm ls -aA\nhelm list --all-namespaces\n</code></pre> List all releases in a <code>specific namespace</code>:</p> <pre><code>helm list --namespace [namespace]\nhelm list --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-status","title":"helm status","text":"<p>See the <code>status</code> of a specific release:</p> <pre><code>helm status [release]\nhelm status argocd --namespace argocd\n</code></pre> <p>output</p> <pre><code>NAME: argocd\nLAST DEPLOYED: Fri Jan 20 20:18:33 2023\nNAMESPACE: argocd\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nIn order to access the server UI you have the following options:\n\n1. kubectl port-forward service/argocd-server -n argocd 8080:443\n\n    and then open the browser on http://localhost:8080 and accept the certificate\n\n2. enable ingress in the values file `server.ingress.enabled` and either\n      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough\n      - Set the `configs.params.\"server.insecure\"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts\n\n\nAfter reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:\n\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-history","title":"Helm history","text":"<p>Display the release <code>history</code></p> <pre><code>helm history [release]\nhelm history argocd --namespace argocd\n</code></pre> <p>output  <pre><code>REVISION        UPDATED                         STATUS    CHART                                                                                                                                                   APP VERSION      DESCRIPTION\n1               Fri Jan 20 20:18:33 2023        deployed  argo-cd-5.13.7                                                                                                                                          v2.5.2           Install complete\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#chart-management","title":"Chart Management","text":"<p>Create new helm chart </p> <pre><code>helm create my-chart\n</code></pre> <p>output</p> <p><pre><code>Creating my-chart\n</code></pre> Run tests (lint) to examine a chart and identify possible issues:</p> <p><pre><code>helm lint my-chart\n</code></pre> output <pre><code>==&gt; Linting my-charts\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre></p> <p>Inspect a chart and list its contents: <pre><code>helm show all my-chart\n</code></pre> output</p> <pre><code>apiVersion: v2\nappVersion: 1.0.0\ndescription: A Helm chart for project1 projects for AKS cluster\nname: helm-release-name\ntype: application\nversion: \"20220823.12\"\n.\n.\n.\n</code></pre> <p>Display the chart\u2019s values: <pre><code>helm show values my-charts\n</code></pre></p> <p>Download a chart:</p> <pre><code>helm pull my-charts\n</code></pre> <p>Display a list of a chart\u2019s dependencies: <pre><code>helm dependency list my-charts\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-client-environment","title":"Helm client environment","text":"<p>See information about the Helm client environment:</p> <pre><code>helm env\n</code></pre> <p>output <pre><code>HELM_BIN=\"C:\\ProgramData\\chocolatey\\lib\\kubernetes-helm\\tools\\windows-amd64\\helm.exe\"\nHELM_CACHE_HOME=\"C:\\Users\\ANJI~1.KEE\\AppData\\Local\\Temp\\helm\"\nHELM_CONFIG_HOME=\"C:\\Users\\anji.keesari\\AppData\\Roaming\\helm\"\nHELM_DATA_HOME=\"C:\\Users\\anji.keesari\\AppData\\Roaming\\helm\"\nHELM_DEBUG=\"false\"\nHELM_KUBEAPISERVER=\"\"\nHELM_KUBEASGROUPS=\"\"\nHELM_KUBEASUSER=\"\"\nHELM_KUBECAFILE=\"\"\nHELM_KUBECONTEXT=\"\"\nHELM_KUBETOKEN=\"\"\nHELM_MAX_HISTORY=\"10\"\nHELM_NAMESPACE=\"default\"\nHELM_PLUGINS=\"C:\\Users\\anji.keesari\\AppData\\Roaming\\helm\\plugins\"\nHELM_REGISTRY_CONFIG=\"C:\\Users\\anji.keesari\\AppData\\Roaming\\helm\\registry\\config.json\"\nHELM_REPOSITORY_CACHE=\"C:\\Users\\ANJI~1.KEE\\AppData\\Local\\Temp\\helm\\repository\"\nHELM_REPOSITORY_CONFIG=\"C:\\Users\\anji.keesari\\AppData\\Roaming\\helm\\repositories.yaml\"\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#commands-summary","title":"Commands Summary","text":"<pre><code>completion  generate autocompletion scripts for the specified shell\ncreate      create a new chart with the given name\ndependency  manage a chart's dependencies\nenv         helm client environment information\nget         download extended information of a named release\nhelp        Help about any command\nhistory     fetch release history\ninstall     install a chart\nlint        examine a chart for possible issues\nlist        list releases\npackage     package a chart directory into a chart archive\nplugin      install, list, or uninstall Helm plugins\npull        download a chart from a repository and (optionally) unpack it in local directory\npush        push a chart to remote\nregistry    login to or logout from a registry\nrepo        add, list, remove, update, and index chart repositories\nrollback    roll back a release to a previous revision\nsearch      search for a keyword in charts\nshow        show information of a chart\nstatus      display the status of the named release\ntemplate    locally render templates\ntest        run tests for a release\nuninstall   uninstall a release\nupgrade     upgrade a release\nverify      verify that a chart at the given path has been signed and is valid\nversion     print the client version information\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/","title":"Kubectl Commands","text":""},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Kubectl commands with examples.</p> <p>Kubectl is the command line configuration tool for Kubernetes that communicates with a Kubernetes API server. Using Kubectl allows you to create, inspect, update, and delete Kubernetes objects.</p>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#installing-kubectl","title":"Installing Kubectl","text":"<p>Use the following commands to install kubectl on Linux, macOS, and Windows.</p> <pre><code># Linux\nsudo apt-get update\nsudo apt-get install -y kubectl\n\n# MacOS (using Homebrew):\nbrew install kubectl\n\n# Windows OS (using Choco)\nchoco install kubernetes-cli\n\n#  verify the installation by running \nkubectl version\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#connection-to-kubernetes-cluster","title":"Connection to Kubernetes cluster","text":"<p>We need to establish a connection to a Kubernetes cluster before we can utilize the power of kubectl commands. In this article, I will demonstrate the process using Azure Kubernetes Service (AKS), but keep in mind that you can connect to any Kubernetes cluster like Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE), or any other Kubernetes platform to execute kubectl commands.\"</p> <pre><code># login into azure first\naz login\n\naz account list\n# or\naz account list --output table\n\n# Select the subscription\naz account set -s \"anji.keesari\"\n\n# Display information about the currently logged-in Azure subscription\naz account show\n# or\naz account show --output table\n\n# Connect to Cluster\n\n# Connect to Azure Kubernetes Service Cluster with User Role\naz aks get-credentials -g \"rg-rgname-dev\" -n \"aks-clustername-dev\"\n\n# Connect to Azure Kubernetes Service Cluster with Admin Role\naz aks get-credentials -g \"rg-rgname-dev\" -n \"aks-clustername-dev\" --admin\n\n# Display detailed information about an AKS cluster\naz aks show -g \"rg-rgname-dev\" -n \"aks-clustername-dev\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#cluster-information","title":"Cluster Information","text":"<p>This command provides an overview of the cluster information, including the cluster endpoint, certificate authority, and other relevant details. </p> <pre><code>kubectl cluster-info\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#version","title":"Version","text":"<p>Retrieves the Kubernetes version information for the client, server, and other components.</p> <pre><code>kubectl version\nkubectl version --short\nkubectl version --short --client\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-help","title":"kubectl --help","text":"<p>This command provides a comprehensive overview of the kubectl command-line tool's usage, available commands, and options. It displays a detailed help message that guides you through the various functionalities and usage patterns of kubectl</p> <pre><code>kubectl --help\nkubectl logs --help\nkubectl exec --help\nkubectl describe --help\nkubectl port-forward --help\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#set-an-alias-for-kubectl","title":"Set an Alias for kubectl","text":"<p>Set an alias for kubectl in Powershell</p> <p><pre><code># set Alias\nNew-Alias -Name 'k' -Value 'kubectl'\n\n# Verify\nk get pods -n sample\n</code></pre> Set an alias for kubectl in Bash</p> <pre><code>alias k=kubectl\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#context-and-configuration","title":"Context and configuration","text":"<p>Kubernetes context and configuration are crucial concepts for managing multiple clusters and switching between them using kubectl.</p> <p>Configuration file</p> <p>Kubectl uses a configuration file, usually located at <code>~/.kube/config</code> by default, to store cluster information, authentication details, and other settings. The configuration file is written in YAML format and can contain multiple contexts, each representing a different cluster.</p> <pre><code># Show Merged kubeconfig settings.\nkubectl config view\n\n# display the first user\nkubectl config view -o jsonpath='{.users[].name}'\n\n # get a list of users\nkubectl config view -o jsonpath='{.users[*].name}'\n\n# display list of contexts\nkubectl config get-contexts\n\n# display the current-context\nkubectl config current-context\n\n# set the default context to my-cluster-name\nkubectl config use-context my-cluster-name\n\n# set a cluster entry in the kubeconfig\nkubectl config set-cluster my-cluster-name\n\n# delete a cluster entry in the kubeconfig\nkubectl config delete-context my-cluster-name\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#listing-resources","title":"Listing resources","text":"<p>Use the <code>kubectl get</code> command followed by the resource type you want to list.</p> <pre><code># List all pods in the default namespace:\nkubectl get pods\n\n# List all pods in a specific namespace:\nkubectl get pods -n sample\n\n# List all pods in all namespaces:\nkubectl get pods --all-namespaces\n# or\nkubectl get pods -A\n\n# List all services in the default namespace:\nkubectl get services\n\n# List all nodes in the cluster:\nkubectl get nodes\n\n# List all deployments in a namespace:\nkubectl get deployments -n sample\n\n# List all replica sets:\nkubectl get replicasets\n\n# List all persistent volumes:\nkubectl get pv\n\n# List all persistent volume claims in a namespace:\nkubectl get pvc -n sample\n\n# List all config maps in a namespace:\nkubectl get configmaps -n sample\n\n# List all secrets in a namespace:\nkubectl get secrets -n sample\n\n# List all namespaces:\nkubectl get namespaces\n\n# List all events in a namespaces:\nkubectl get events -n sample\n\n# List all ingress in a namespaces:\nkubectl get ingress -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#creating-a-resource","title":"Creating a Resource","text":"<p>Create a resource such as a service, deployment, job, or namespace using the <code>kubectl create</code> command.</p> <pre><code># create a new namespace\nkubectl create namespace sample\n\n# Create a Deployment\nkubectl create deployment nginx-deployment  --image=nginx -n sample\n\n# Create a Service\nkubectl create service clusterip my-service --tcp=80:8080\n\n# Create a Secret from literal values:\nkubectl create secret generic my-secret --from-literal=username=admin \n--from-literal=password=pass123\n\n# Create a PersistentVolume\nkubectl create persistentvolume my-pv --capacity=1Gi --host-path=/data\n\n# Create a PersistentVolumeClaim\nkubectl create persistentvolumeclaim my-pvc --namespace=my-namespace \n--storageClassName=standard --request=1Gi\n\n# Create a resource from a YAML file\nkubectl create -f my-filename.yaml\n# or\nkubectl apply -f my-filename.yaml\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#applying-and-updating-a-resource","title":"Applying and Updating a Resource","text":"<pre><code># Create a resource from a YAML file\nkubectl apply -f my-filename.yaml\n\n # create from multiple files\nkubectl apply -f ./file1.yaml -f ./file1.yaml \n\n# create resource(s) in all manifest files in folder\nkubectl apply -f ./folder1\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#creating-vs-apply","title":"Creating vs Apply","text":"<p><code>kubectl create</code> is used for creating new resources, while <code>kubectl apply</code> is used for creating and updating resources. kubectl apply provides a more flexible and incremental approach to managing resource configurations.</p>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#viewing-and-finding-resources","title":"Viewing and finding resources","text":"<pre><code># List all pods in the current namespace, with more details\nkubectl get pods -o wide\n# Get a pod's YAML\nkubectl get pod my-pod -o yaml\n\n# List Services Sorted by Name\nkubectl get services -n my-namespace --sort-by=.metadata.name\n\n# List pods Sorted by Restart Count\nkubectl get pods -n my-namespace \n--sort-by='.status.containerStatuses[0].restartCount'\n\n# List PersistentVolumes sorted by capacity\nkubectl get pv -n my-namespace --sort-by=.spec.capacity.storage\n\n# Get all worker nodes (use a selector to exclude results that have a label\n# named 'node-role.kubernetes.io/control-plane')\nkubectl get node --selector='!node-role.kubernetes.io/control-plane'\n\n# List Events sorted by timestamp\nkubectl get events -n my-namespace --sort-by=.metadata.creationTimestamp\n\n# List all warning events\nkubectl events -n my-namespace --types=Warning\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#updating-resources","title":"Updating resources","text":"<pre><code># Rolling update \"www\" containers of \"frontend\" deployment, updating the image\nkubectl set image deployment/frontend www=image:v2               \n# Check the history of deployments including the revision\nkubectl rollout history deployment/frontend                      \n# Rollback to the previous deployment\nkubectl rollout undo deployment/frontend                         \n# Rollback to a specific revision\nkubectl rollout undo deployment/frontend --to-revision=2         \n# Watch rolling update status of \"frontend\" deployment until completion\nkubectl rollout status -w deployment/frontend                    \n# Rolling restart of the \"frontend\" deployment\nkubectl rollout restart deployment/frontend                      \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#patching-resources","title":"Patching resources","text":"<pre><code># Update a container's image; spec.containers[*].name is required because it's a merge key\nkubectl patch pod my-pod -n my-namespace -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}'\n\n# Update a container's image using a json patch with positional arrays\nkubectl patch pod my-pod -n my-namespace --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]'\n\n# Disable a deployment livenessProbe using a json patch with positional arrays\nkubectl patch deployment my-deployment -n my-namespace --type json   -p='[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]'\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#editing-resources","title":"Editing resources","text":"<pre><code># Edit the service named my-service\nkubectl edit svc/my-service -n my-namespace\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#scaling-resources","title":"Scaling resources","text":"<pre><code> # Scale a replicaset named 'my-rs' to 3\nkubectl scale --replicas=2 rs/my-rs -n my-namespace\n\n# Scale a resource specified in \"my-file.yaml\" to 3\nkubectl scale --replicas=3 -f my-file.yaml    \n\n# If the deployment named my-deployment's size is 2, scale my-deployment to 3\nkubectl scale --current-replicas=1 --replicas=3 deployment/nginx-deployment -n sample\nkubectl scale --current-replicas=3 --replicas=1 deployment/nginx-deployment -n sample\nkubectl get deployments -n sample # use this for verify\n\n# Scale multiple replication controllers\nkubectl scale --replicas=5 rc/my-rc1 rc/my-rc1 rc/my-rc1                   \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#deleting-resources","title":"Deleting resources","text":"<pre><code># Delete pods and services with same names \"aspnet-api\" and \"aspnet-api\"\nkubectl delete pod,service aspnet-api aspnet-api -n sample\n\n# Delete a pod with no grace period\nkubectl delete pod unwanted --now\n\n# Delete pods and services with same names \"baz\" and \"foo\"\nkubectl delete pod,service my-pod1, my-pod2\n\n# Delete pods and services with label name=my-Label\nkubectl delete pods,services -l name=my-Label \n\n# Delete all pods and services in namespace my-namespace,\nkubectl -n my-namespace delete pod,svc --all                             \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-logs","title":"kubectl logs","text":"<pre><code>kubectl logs pod/my-pod -n my-namespace\nkubectl logs svc/my-svc -n my-namespace\n\nkubectl logs pods/sample-server-f486b9bd7-wtw9f -n sample\n# fetches the logs generated by the pod in the last 2 minutes.\nkubectl logs --since=2m pods/sample-server-f486b9bd7-wtw9f -n sample\n\n# logs from all the pods\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs $_.Split()[0] -n sample }\n\n## logs from all the pods since 5 hours\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs --since=5h $_.Split()[0] -n sample }\n\n# show log output if any word \"exception\" in it\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs --since=5m $_.Split()[0] -n sample } | Select-String \"exception\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#port-forward","title":"Port-forward","text":"<pre><code>kubectl port-forward my-pod 5000:6000 \nkubectl port-forward service/aspnet-api 80:80 -n sample\nkubectl port-forward svc/pgadmin4 -n pgadmin4 80:80\n\n# listen on local port 5000 and forward to port 5000 on Service backend\nkubectl port-forward svc/my-service 5000                  \n\n# listen on local port 5000 and forward to Service target port with name &lt;my-service-port&gt;\nkubectl port-forward svc/my-service 5000:my-service-port  \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#interacting-with-running-pods","title":"Interacting with running Pods","text":"<pre><code>kubectl exec -n my-namespace -it my-pod -- bash\nkubectl exec -n my-namespace -it my-pod -- ls /\n\n# list files\n# root@my-pod:/app# ls\n# root@my-pod:/app# exit\n\n# print environment variables\nkubectl exec -n my-namespace my-pod -- printenv\n\n# describe pod\nkubectl describe pod/nginx-deployment-9456bbbf9-ngv7f -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#show-metrics","title":"Show metrics","text":"<pre><code># Show node metrics:\nkubectl top nodes\n\n# Show pod metrics:\nkubectl top pods\n\n#  --sort-by flag with the memory \nkubectl top nodes --sort-by=memory \nkubectl top pods --sort-by=memory -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#formatting-output","title":"Formatting output","text":"<p>You can format the output of commands to customize the information displayed.</p> <pre><code>kubectl get pods -n sample -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName\n\n# all pods\nkubectl get pods -n sample -o=yaml\nkubectl get pods -n sample -o=json\nkubectl get pods -n sample -o=name\n\n# single pod\nkubectl get pod/aspnet-api-6699db6d4b-66d7m -n sample -o=yaml\n\nkubectl describe pod/aspnet-api-6699db6d4b-66d7m -o go-template='{{range .status.containerStatuses}}{{.name}}:{{.restartCount}}{{\"\\n\"}}{{end}}'\n\n# Show labels of all pods\nkubectl get pods --show-labels\n\nkubectl get pods -n sample -o wide\n\n# All images running in a cluster\nkubectl get pods -A -o=custom-columns='IMAGE:spec.containers[*].image'\n\n# All images running in a namespace\nkubectl get pods -n sample -o=custom-columns='IMAGE:spec.containers[*].image'\n\n# All container name running in a namespace\nkubectl get pods -n sample -o=custom-columns='NAME:spec.containers[*].name'\n\n# All images running in namespace: sample, grouped by Pod\nkubectl get pods -n sample --output=custom-columns=\"NAME:.metadata.name,IMAGE:.spec.containers[*].image\"\n\n# This will get all container with the namespace in a pretty format:\nkubectl get pods --all-namespaces -o=custom-columns=NameSpace:.metadata.namespace,NAME:.metadata.name,CONTAINERS:.spec.containers[*].name\n\n# kubectl describe pod aspnet-api-6699db6d4b-66d7m -n sample  | grep \"Container ID\"\n# or\nkubectl describe pod aspnet-api-6699db6d4b-66d7m -n sample  | Select-String \"Container ID\"\n\n# all pods in a namespace\nkubectl describe pods -n sample  | Select-String \"name|Container ID\"\n\n# display NAME and CONTAINERID\nkubectl get pods -n sample -o=custom-columns='NAME:.metadata.name,CONTAINERID:.status.containerStatuses[*].containerID'\n\nkubectl logs aspnet-api-6699db6d4b-66d7m -n sample\nkubectl logs azure-vote-back-6dbbb4bccc-dnwmg -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#command-invoke","title":"Command invoke","text":"<p>Use command invoke to access a private Azure Kubernetes Service (AKS) cluster, reference - https://learn.microsoft.com/en-us/azure/aks/command-invoke</p> <pre><code>  az aks command invoke --resource-group 'rg-rgname-dev' --name 'aks-aksname-dev' --command \"kubectl get namespaces\"\n  az aks command invoke --resource-group 'rg-rgname-dev' --name 'aks-aksname-dev' --command \"kubectl create namespace test\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-api-resources","title":"kubectl api-resources","text":"<p>The <code>kubectl api-resources</code> command allows you to view the available resource types in your Kubernetes cluster. It provides a list of the supported resource types along with their short names, API group, and whether they are namespaced or not.</p> <pre><code># All namespaced resources\nkubectl api-resources --namespaced=true\nkubectl api-resources -o name\nkubectl api-resources -o wide\nkubectl api-resources --verbs=list,get \nkubectl api-resources | more\n# use ctlr + c to exit\n</code></pre> <pre><code>NAME                               SHORTNAMES               APIVERSION                             NAMESPACED   KIND\nbindings                                                    v1                                     true         Binding\ncomponentstatuses                  cs                       v1                                     false        ComponentStatus\nconfigmaps                         cm                       v1                                     true         ConfigMap\nendpoints                          ep                       v1                                     true         Endpoints\nevents                             ev                       v1                                     true         Event\nlimitranges                        limits                   v1                                     true         LimitRange\nnamespaces                         ns                       v1                                     false        Namespace\nnodes                              no                       v1                                     false        Node\npersistentvolumeclaims             pvc                      v1                                     true         PersistentVolumeClaim\npersistentvolumes                  pv                       v1                                     false        PersistentVolume\npods                               po                       v1                                     true         Pod\npodtemplates                                                v1                                     true         PodTemplate\nreplicationcontrollers             rc                       v1                                     true         ReplicationController\nresourcequotas                     quota                    v1                                     true         ResourceQuota\nsecrets                                                     v1                                     true         Secret\nserviceaccounts                    sa                       v1                                     true         ServiceAccount\nservices                           svc                      v1                                     true         Service\nazureassignedidentities                                     aadpodidentity.k8s.io/v1               true         AzureAssignedIdentity\nazureidentities                                             aadpodidentity.k8s.io/v1               true         AzureIdentity\nazureidentitybindings                                       aadpodidentity.k8s.io/v1               true         AzureIdentityBinding\nazurepodidentityexceptions                                  aadpodidentity.k8s.io/v1               true         AzurePodIdentityException\nchallenges                                                  acme.cert-manager.io/v1                true         Challenge\norders                                                      acme.cert-manager.io/v1                true         Order\nmutatingwebhookconfigurations                               admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration\nvalidatingwebhookconfigurations                             admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration\ncustomresourcedefinitions          crd,crds                 apiextensions.k8s.io/v1                false        CustomResourceDefinition\napiservices                                                 apiregistration.k8s.io/v1              false        APIService\ncontrollerrevisions                                         apps/v1                                true         ControllerRevision\ndaemonsets                         ds                       apps/v1                                true         DaemonSet\ndeployments                        deploy                   apps/v1                                true         Deployment\nreplicasets                        rs                       apps/v1                                true         ReplicaSet\nstatefulsets                       sts                      apps/v1                                true         StatefulSet\ntokenreviews                                                authentication.k8s.io/v1               false        TokenReview\nlocalsubjectaccessreviews                                   authorization.k8s.io/v1                true         LocalSubjectAccessReview\nselfsubjectaccessreviews                                    authorization.k8s.io/v1                false        SelfSubjectAccessReview\nselfsubjectrulesreviews                                     authorization.k8s.io/v1                false        SelfSubjectRulesReview\nsubjectaccessreviews                                        authorization.k8s.io/v1                false        SubjectAccessReview\nhorizontalpodautoscalers           hpa                      autoscaling/v2                         true         HorizontalPodAutoscaler\ncronjobs                           cj                       batch/v1                               true         CronJob\njobs                                                        batch/v1                               true         Job\ncertificaterequests                cr,crs                   cert-manager.io/v1                     true         CertificateRequest\ncertificates                       cert,certs               cert-manager.io/v1                     true         Certificate\nclusterissuers                                              cert-manager.io/v1                     false        ClusterIssuer\nissuers                                                     cert-manager.io/v1                     true         Issuer\ncertificatesigningrequests         csr                      certificates.k8s.io/v1                 false        CertificateSigningRequest\nconfigs                            config                   config.gatekeeper.sh/v1alpha1          true         Config\nk8sazurev1blockdefault                                      constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1BlockDefault\nk8sazurev1ingresshttpsonly                                  constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1IngressHttpsOnly\nk8sazurev1serviceallowedports                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1ServiceAllowedPorts\nk8sazurev2blockautomounttoken                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2BlockAutomountToken\nk8sazurev2blockhostnamespace                                constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2BlockHostNamespace\nk8sazurev2containerallowedimages                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2ContainerAllowedImages\nk8sazurev2noprivilege                                       constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2NoPrivilege\nk8sazurev3allowedcapabilities                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3AllowedCapabilities\nk8sazurev3allowedusersgroups                                constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3AllowedUsersGroups\nk8sazurev3containerlimits                                   constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3ContainerLimits\nk8sazurev3disallowedcapabilities                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3DisallowedCapabilities\nk8sazurev3enforceapparmor                                   constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3EnforceAppArmor\nk8sazurev3hostfilesystem                                    constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3HostFilesystem\nk8sazurev3hostnetworkingports                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3HostNetworkingPorts\nk8sazurev3noprivilegeescalation                             constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3NoPrivilegeEscalation\nk8sazurev3readonlyrootfilesystem                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3ReadOnlyRootFilesystem\nleases                                                      coordination.k8s.io/v1                 true         Lease\nendpointslices                                              discovery.k8s.io/v1                    true         EndpointSlice\nevents                             ev                       events.k8s.io/v1                       true         Event\nflowschemas                                                 flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema\nprioritylevelconfigurations                                 flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration\nclustertriggerauthentications      cta,clustertriggerauth   keda.sh/v1alpha1                       false        ClusterTriggerAuthentication\nscaledjobs                         sj                       keda.sh/v1alpha1                       true         ScaledJob\nscaledobjects                      so                       keda.sh/v1alpha1                       true         ScaledObject\ntriggerauthentications             ta,triggerauth           keda.sh/v1alpha1                       true         TriggerAuthentication\nnodes                                                       metrics.k8s.io/v1beta1                 false        NodeMetrics\npods                                                        metrics.k8s.io/v1beta1                 true         PodMetrics\ningressclasses                                              networking.k8s.io/v1                   false        IngressClass\ningresses                          ing                      networking.k8s.io/v1                   true         Ingress\nnetworkpolicies                    netpol                   networking.k8s.io/v1                   true         NetworkPolicy\nruntimeclasses                                              node.k8s.io/v1                         false        RuntimeClass\npoddisruptionbudgets               pdb                      policy/v1                              true         PodDisruptionBudget\nclusterrolebindings                                         rbac.authorization.k8s.io/v1           false        ClusterRoleBinding\nclusterroles                                                rbac.authorization.k8s.io/v1           false        ClusterRole\nrolebindings                                                rbac.authorization.k8s.io/v1           true         RoleBinding\nroles                                                       rbac.authorization.k8s.io/v1           true         Role\npriorityclasses                    pc                       scheduling.k8s.io/v1                   false        PriorityClass\nsecretproviderclasses                                       secrets-store.csi.x-k8s.io/v1          true         SecretProviderClass\nsecretproviderclasspodstatuses                              secrets-store.csi.x-k8s.io/v1          true         SecretProviderClassPodStatus\nvolumesnapshotclasses              vsclass,vsclasses        snapshot.storage.k8s.io/v1             false        VolumeSnapshotClass\nvolumesnapshotcontents             vsc,vscs                 snapshot.storage.k8s.io/v1             false        VolumeSnapshotContent\nvolumesnapshots                    vs                       snapshot.storage.k8s.io/v1             true         VolumeSnapshot\nconstraintpodstatuses                                       status.gatekeeper.sh/v1beta1           true         ConstraintPodStatus\nconstrainttemplatepodstatuses                               status.gatekeeper.sh/v1beta1           true         ConstraintTemplatePodStatus\ncsidrivers                                                  storage.k8s.io/v1                      false        CSIDriver\ncsinodes                                                    storage.k8s.io/v1                      false        CSINode\ncsistoragecapacities                                        storage.k8s.io/v1                      true         CSIStorageCapacity\nstorageclasses                     sc                       storage.k8s.io/v1                      false        StorageClass\nvolumeattachments                                           storage.k8s.io/v1                      false        VolumeAttachment\nconstrainttemplates                constraints              templates.gatekeeper.sh/v1             false        ConstraintTemplate\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#help-summary","title":"Help summary","text":"<pre><code>Basic Commands (Beginner):\n  create          Create a resource from a file or from stdin\n  expose          Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service\n  run             Run a particular image on the cluster\n  set             Set specific features on objects\n\nBasic Commands (Intermediate):\n  explain         Get documentation for a resource\n  get             Display one or many resources\n  edit            Edit a resource on the server\n  delete          Delete resources by file names, stdin, resources and names, or by resources and label selector\n\nDeploy Commands:\n  rollout         Manage the rollout of a resource\n  scale           Set a new size for a deployment, replica set, or replication controller\n  autoscale       Auto-scale a deployment, replica set, stateful set, or replication controller\n\nCluster Management Commands:\n  certificate     Modify certificate resources.\n  cluster-info    Display cluster information\n  top             Display resource (CPU/memory) usage\n  cordon          Mark node as unschedulable\n  uncordon        Mark node as schedulable\n  drain           Drain node in preparation for maintenance\n  taint           Update the taints on one or more nodes\n\nTroubleshooting and Debugging Commands:\n  describe        Show details of a specific resource or group of resources\n  logs            Print the logs for a container in a pod\n  attach          Attach to a running container\n  exec            Execute a command in a container\n  port-forward    Forward one or more local ports to a pod\n  proxy           Run a proxy to the Kubernetes API server\n  cp              Copy files and directories to and from containers\n  auth            Inspect authorization\n  debug           Create debugging sessions for troubleshooting workloads and nodes\n  events          List events\n\nAdvanced Commands:\n  diff            Diff the live version against a would-be applied version\n  apply           Apply a configuration to a resource by file name or stdin\n  patch           Update fields of a resource\n  replace         Replace a resource by file name or stdin\n  wait            Experimental: Wait for a specific condition on one or many resources\n  kustomize       Build a kustomization target from a directory or URL.\n\nSettings Commands:\n  label           Update the labels on a resource\n  annotate        Update the annotations on a resource\n  completion      Output shell completion code for the specified shell (bash, zsh, fish, or powershell)\n\nOther Commands:\n  alpha           Commands for features in alpha\n  api-resources   Print the supported API resources on the server\n  api-versions    Print the supported API versions on the server, in the form of \"group/version\"\n  config          Modify kubeconfig files\n  plugin          Provides utilities for interacting with plugins\n  version         Print the client and server version information\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#references","title":"References","text":"<ul> <li>https://kubernetes.io/docs/reference/kubectl/cheatsheet/</li> </ul>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/","title":"Redis Cache Commands","text":"<p>Azure Cache for Redis is a fully managed, in-memory data store service provided by Microsoft Azure. It offers high-performance data caching capabilities, enabling applications to achieve low-latency access to frequently accessed data. Redis, being an open-source, in-memory data structure store, supports various commands for data manipulation and management. </p> <p>In this cheat sheet, we'll explore some commonly used Redis commands specifically for Azure Cache for Redis.</p> <p>Key Commands</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#set-key-value","title":"SET key value","text":"<p>Sets the value of a key.</p> <pre><code>SET mykey \"Hello\"\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#get-key","title":"GET key","text":"<p>Retrieves the value of a key.</p> <pre><code>GET mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#exists-key","title":"EXISTS key","text":"<p>Checks if a key exists.</p> <pre><code>EXISTS mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#del-key","title":"DEL key","text":"<p>Deletes one or more keys.</p> <pre><code>DEL mykey\n</code></pre> <p>String Commands</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#append-key-value","title":"APPEND key value","text":"<p>Appends a value to a key.</p> <pre><code>APPEND mykey \" World\"\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#strlen-key","title":"STRLEN key","text":"<p>Returns the length of the value stored in a key.</p> <pre><code>STRLEN mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#incr-key","title":"INCR key","text":"<p>Increments the integer value of a key by one.</p> <pre><code>INCR mycounter\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#monitor","title":"MONITOR","text":"<p>The <code>MONITOR</code> command in Redis is used to stream all commands received by the Redis server in real-time. It's a helpful tool for debugging and monitoring purposes, allowing you to observe the activity on your Redis instance. Here's how you can use it:</p> <pre><code>MONITOR\n</code></pre> <p>output</p> <pre><code>1711242811.987417 [0 fd40:d75d:22:cae2:6b22:700:a41:ab2:40288] \"GET\" \"mykey\"\n1711242814.532343 [0 fd40:d75d:22:cae2:6b22:700:a41:849:42348] \"PING\"\n1711242814.532363 [0 fd40:d75d:22:cae2:6b22:700:a41:849:42346] \"INFO\" \"replication\"\n1711242815.257225 [0 fd40:d75d:22:cae2:6b22:700:a41:acb:52114] \"INFO\" \"replication\"\n1711242815.257260 [0 fd40:d75d:22:cae2:6b22:700:a41:acb:52128] \"PING\"\n1711242815.442303 [0 fd40:d75d:22:cae2:6b22:700:a41:840:36272] \"PING\"\n</code></pre> <p>Each line in the monitor output represents a command executed on the server, along with the timestamp, client information, and the command itself.</p> <p>To stop monitoring, simply close the connection or issue another command on the client.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#ping","title":"PING","text":"<p>the <code>PING</code> command to verify the connectivity status before executing other commands. If the server responds with a <code>PONG</code>, it indicates that the connection is alive and operational, allowing clients to proceed with executing Redis commands.</p> <pre><code>PING\n</code></pre> <p>output</p> <pre><code>PONG\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#keys","title":"KEYS","text":"<p>The <code>KEYS</code> command in Redis is used to retrieve all keys matching a specified pattern. When you issue the KEYS command with a pattern argument, Redis returns a list of keys that match the specified pattern. The * wildcard character matches zero or more characters in the key name, allowing for flexible pattern matching.</p> <pre><code>KEYS *\n</code></pre> <p>output <pre><code>1) \"key1\"\n2) \"key2\"\n3) \"another_key\"\n</code></pre></p> <pre><code>KEYS pattern\n\nKEYS user:*\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#flushdb","title":"FLUSHDB","text":"<p>The <code>FLUSHDB</code> command in Redis is used to delete all keys from the currently selected database. It effectively removes all data stored in the Redis database associated with the current connection</p> <pre><code>FLUSHDB\nFLUSHDB ASYNC\n</code></pre> <p>Note</p> <p>The FLUSHDB command deletes all keys from the currently selected Redis database permanently. Use it with caution, as data loss is irreversible.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#flushall","title":"FLUSHALL","text":"<p>The <code>FLUSHALL</code> command in Redis is used to delete all keys from all databases in the Redis instance. It effectively removes all data stored across all databases associated with the Redis server.</p> <pre><code>FLUSHALL\nFLUSHALL ASYNC\n</code></pre> <p>Note</p> <p>The FLUSHALL command deletes all keys from all databases in the Redis instance permanently. Use it with extreme caution, as data loss is irreversible.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#info","title":"INFO","text":"<p>The <code>INFO</code> command provides a wealth of information about the Redis server's state, making it a valuable tool for monitoring, troubleshooting, and performance tuning. By analyzing the output of the INFO command, administrators can gain insights into various aspects of the Redis server's operation and health.</p> <pre><code># Server\nredis_version:6.0.14\nredis_mode:standalone\nos:Windows\narch_bits:64\nmultiplexing_api:winsock_IOCP\nrun_id:2394a7426e43250f6e6412d256ad82d2989b844c\nuptime_in_seconds:2077180\nuptime_in_days:24\nhz:10\n\n# Clients\nconnected_clients:21\nmaxclients:7500\nclient_recent_max_input_buffer:8\nclient_recent_max_output_buffer:0\nclient_total_writes_outstanding:0\nclient_total_sent_bytes_outstanding:0\nblocked_clients:0\ntracking_clients:0\nclients_in_timeout_table:0\n\n# Memory\nused_memory:196544424\nused_memory_human:187.44M\nused_memory_rss:415309824\nused_memory_rss_human:396.07M\nused_memory_peak:906410480\nused_memory_peak_human:864.42M\nused_memory_peak_perc:21.68%\nused_memory_overhead:106090184\nused_memory_startup:691856\nused_memory_dataset:90454240\nused_memory_dataset_perc:46.18%\nused_memory_lua:32768\nmaxmemory:6100000000\nmaxmemory_reservation:638000000\nmaxfragmentationmemory_reservation:638000000\nmaxmemory_desired_reservation:638000000\nmaxfragmentationmemory_desired_reservation:638000000\nmaxmemory_human:5.68G\nmaxmemory_policy:volatile-lru\nmem_allocator:jemalloc-4.0.3\n\n# Stats\ntotal_connections_received:32639883\ntotal_commands_processed:154211013\ninstantaneous_ops_per_sec:21\nbytes_received_per_sec:6490\nbytes_sent_per_sec:547314\nbytes_received_per_sec_human:6.34K\nbytes_sent_per_sec_human:534.49K\nrejected_connections:0\nexpired_keys:7549179\nevicted_keys:0\nkeyspace_hits:58233911\nkeyspace_misses:26941507\npubsub_channels:2\npubsub_patterns:0\ntotal_oom_messages:0\n\n# Replication\nrole:master\n\n# CPU\nused_cpu_sys:6094.812500\nused_cpu_user:5064.234375\nused_cpu_avg_ms_per_sec:3\nserver_load:0.75\nevent_wait:37\nevent_no_wait:43\nevent_wait_count:46\nevent_no_wait_count:38\n\n# Cluster\ncluster_enabled:0\ncluster_myself_name:\n\n# Keyspace\ndb0:keys=31308,expires=26294,avg_ttl=26464306438\n</code></pre> <p>This cheat sheet provides a quick reference to some of the most commonly used Redis commands in Azure Cache for Redis environments. Make sure to refer to the official documentation for comprehensive details and additional commands.</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/","title":"Terraform Commands","text":""},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Terraform commands with examples.</p> <p>Terraform is the infrastructure as code tool from HashiCorp. It is a tool for building, changing, and managing infrastructure in a safe, repeatable way.</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#installing-terraform","title":"Installing Terraform","text":"<p>Use the following commands to install Terraform in Windows, MacOS and Linux environments.</p> <p><pre><code># MacOS (using Homebrew):\n# First, install the HashiCorp tap\nbrew brew tap hashicorp/tap\n# Now, install Terraform\nbrew install hashicorp/tap/terraform\n\n# Windows OS (using choco)\nchoco install terraform\n</code></pre> For more information, refer to the official documentation:  - Install Terraform</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#upgrade-terraform","title":"Upgrade Terraform","text":"<p>Use the following commands to upgrade Terraform in Windows</p> <pre><code>#  verify the installation by running \nterraform --version\n\n# update to the latest version of Terraform in Mac OS\nbrew update\nbrew upgrade hashicorp/tap/terraform\n\n# update to the latest version of Terraform in Windows OS\nchoco upgrade terraform\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-help","title":"terraform help","text":"<p>Displays general help information about Terraform, including a list of available commands and options.</p> <pre><code>terraform -h\n# or\nterraform --help\n\n# Displays help information for a specific Terraform command\nterraform &lt;command&gt; --help\nterraform plan --help\nterraform workspace --help\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-init","title":"terraform init","text":"<p>Initializes a Terraform working directory by downloading and configuring providers.</p> <pre><code>terraform init\n\n# Forces Terraform to reconfigure the backend, even if it is already initialized.\nterraform init -reconfigure\n\n# Specifies a backend configuration file to use during initialization.\nterraform init -backend-config=\"access_key=$(az storage account keys list --resource-group \"rg-terraform-mgmt-poc\" --account-name \"sttfstatespoc01\" --query '[0].value' -o tsv)\"\n\n# if you don\u2019t want to hold a state lock during backend migration.\nterraform init -lock=false \n\n#  user this to disable interactive prompts.\nterraform init -input=false \n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-plan","title":"terraform plan","text":"<p>Generates an execution plan, showing the changes that will be made without actually applying them.</p> <pre><code>terraform plan\n\n# Saves the generated plan to a file named tfplan for later use.\nterraform plan -out=tfplan\n\n# Specifies a file containing variable values to be used during planning.\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-apply","title":"terraform apply","text":"<p>Applies the changes required to reach the desired state defined in the Terraform configuration.</p> <pre><code>terraform apply\n\n# Automatically approves and applies the changes without requiring manual confirmation.\nterraform apply -auto-approve\n\n# use this command to refresh the state for manual changes done from portal directly.\nterraform apply -refresh-only -var-file=\"./environments/dev-variables.tfvars\"\n\n# applies the changes with planfilename\nterraform apply poc-plan\n\nterraform apply -lock=false\n#  Do not hold a state lock during the Terraform apply operation. \n#  Use with caution if other engineers might run concurrent commands against the same workspace\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-destroy","title":"terraform destroy","text":"<p>Destroys the Terraform-managed infrastructure, terminating all resources defined in the Terraform configuration.</p> <pre><code>terraform destroy\n\n# Destroy the infrastructure without having to interactively type \u2018yes\u2019 to the plan. Useful in automation CI/CD pipelines.\nterraform destroy --auto-approve\n\nterraform destroy -var-file=\"./environments/poc-variables.tfvars\"\n\n# Destroy an instance of a resource created with for_each.\n\nterraform destroy -target=\"module.appgw.resource[\\\"key\\\"]\" \n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-validate","title":"terraform validate","text":"<p>Checks if the Terraform configuration files are valid and properly formatted.</p> <pre><code>terraform validate\n\n# see errors and warnings that you have.\nterraform validate -json\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-fmt","title":"terraform fmt","text":"<p>Automatically updates and formats the Terraform configuration files to follow the canonical format.</p> <pre><code>terraform fmt\n\n# format files in subdirectories\nterraform fmt --recursive\n\n#  Display differences between original configuration files and formatting changes.\nterraform fmt --diff\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-state","title":"terraform state","text":"<p>Manages Terraform's state, allowing you to inspect, modify, and manage the state file.</p> <pre><code>terraform state list\n\n# Remove the specified instance from the state file\n# Useful when a resource has been manually deleted outside of Terraform.\nterraform state rm 'azurerm_logic_app_workflow.logic[\\\"1\\\"]'\nterraform state rm 'azurerm_resource_group.rg' \n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-output","title":"terraform output","text":"<p>Displays the outputs defined in the Terraform configuration after applying the changes.</p> <pre><code>terraform output\n\n# state file in JSON format \nterraform output -json\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-import","title":"terraform import","text":"<p>Imports existing infrastructure into Terraform state, allowing you to manage resources that were not created by Terraform.</p> <pre><code># use this when the resource is created with for_each \nterraform import -var-file=\"./environments/dev-variables.tfvars\" 'azurerm_resource_group.rg[\\\"3\\\"]' /subscriptions/342334ec-8a2e-4b7d-a886-e772dc017316/resourceGroups/rg-sitecore-dev\n\nterraform import -var-file=\"./environments/dev-variables.tfvars\" 'azurerm_resource_group.rg' '/subscriptions/dsaf2343-8a2e-4b7d-a886-e772dc017316/resourceGroups/rg-demo'\n\n\nterraform import azurerm_resource_group.rg\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-workspace","title":"terraform workspace","text":"<p>Manages Terraform workspaces, allowing you to create, select, and delete different named workspaces.</p> <pre><code># Creates a new Terraform workspace.\nterraform workspace new dev\n\n# Selects an existing Terraform workspace.\nterraform workspace select prod\n\n# Lists all available Terraform workspaces.\nterraform workspace list\n\n# Shows the currently selected Terraform workspace.\nterraform workspace show\n\n# Deletes an existing Terraform workspace.\nterraform workspace delete dev\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-show","title":"terraform show","text":"<p>Show the state file in a human-readable format.</p> <pre><code>terraform show\n\nterraform show &lt;path to statefile&gt; \n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-providers","title":"terraform providers","text":"<p>Prints a tree of all available providers and their versions.</p> <pre><code>terraform providers\n\n# Locks the provider versions in the Terraform configuration to ensure reproducible builds.\nterraform providers lock\n\nterraform providers init\nterraform providers migrate\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-force-unlock","title":"terraform force-unlock","text":"<p>Remove the lock with the specified lock ID from your workspace. Useful when a lock has become \u2018stuck\u2019, usually after an incomplete Terraform run.</p> <pre><code>terraform force-unlock &lt;lock_id&gt; \n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-graph","title":"terraform graph","text":"<p>Produce a graph in DOT language showing the dependencies between objects in the state file. </p> <pre><code>terraform graph\n</code></pre>"},{"location":"developertools/introduction/introduction/","title":"Introduction","text":"<p>Welcome to my personal knowledge-sharing platform!</p> <p>This website brings together years of my hands-on experience, continuous learning, and teaching across modern cloud-native technologies. It is created for developers, architects, and learners who aim to build reliable, production-grade solutions using today\u2019s most relevant tools and technologies.</p> <p>This site is Organized with following topics:</p> <p>Developer Tools</p> <p>This section focuses on helping you set up your local development environment quickly and efficiently, covering both Windows and Mac operating systems. It includes:</p> <ul> <li>Download &amp; Install Software \u2013 Step-by-step installation guides for essential developer tools.</li> <li>Developer Workstation Configuration \u2013 Best practices for setting up a clean, powerful development environment.</li> <li>Useful Tools \u2013 A list of network related tools that enhance network engineer productivity.</li> <li> <p>Cheat Sheets \u2013 A collection of clear, quick-reference guides for common technologies:</p> </li> <li> <p>Git, Docker, Dockerfile, Kubectl, Helm, ArgoCD, Azure CLI, ACR, Terraform, Redis, Cloud Comparison, Dig, and more.</p> </li> </ul> <p>Resources</p> <p>This section covers the books I\u2019ve authored on Azure and Kubernetes, along with YouTube video tutorials covering .NET and Azure-based development.</p> <p>Articles</p> <p>This section covers the articles and blogs of detailed technical content based on real-world experience, use cases, and implementation patterns each article is written with examples, code snippets, deployment patterns, and architecture diagrams where applicable and more.</p> <p>About</p> <p>This section covers a brief overview of my background, technical journey.</p> <p>Contact</p> <p>I encourage you to reach out if you\u2019re interested in contributing, or exploring project ideas.</p> <p>Thank you for visiting my website.</p> <p>\u2014 Anji Keesari</p>"},{"location":"developertools/software/mac/","title":"Download &amp; install Software in Mac OS","text":""},{"location":"developertools/software/mac/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive list of essential tools and software commonly needed by developers in the IT industry. If you are using Mac operating system, you can download and install the following software as per your requirements. It is recommended to use the <code>brew</code> tool for installation, but if you encounter any issues, you can also perform a direct manual install.</p>"},{"location":"developertools/software/mac/#what-is-homebrew","title":"What is Homebrew?","text":"<p>Homebrew is a free and open-source software package management system that simplifies the installation of software on Apple's operating system, macOS, as well as Linux. </p>"},{"location":"developertools/software/mac/#install-homebrew","title":"Install homebrew","text":"<p>This is the first software you may need to install before installing anything on a Mac. For more information, refer to this link: https://brew.sh/</p> <p>Homebrew is similar to Chocolatey in the Windows environment.</p> <p>homebrew (for Mac users) = cocho (for Windows users)</p> <p>To use Homebrew, you will need to have a terminal window open and install Homebrew on your system. To install Homebrew, you can copy and paste the following command into the terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>output</p> <p><pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/anjikeesari/.zprofile   \neval \"$(/opt/homebrew/bin/brew shellenv)\"  \n</code></pre> upgrade brew</p> <p><pre><code>brew upgrade \n</code></pre> <pre><code>brew --version\n# or\nbrew doctor\n</code></pre> Once the installation is finished, you can use the <code>brew</code> command to install, upgrade, and manage software packages using Homebrew.</p>"},{"location":"developertools/software/mac/#install-google-chrome","title":"Install Google Chrome","text":"<pre><code>brew install --cask google-chrome\n</code></pre> <p>verify the installed version #</p> <pre><code>/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --version\n</code></pre>"},{"location":"developertools/software/mac/#install-iterm2","title":"Install iTerm2","text":"<p>iTerm2 is a popular terminal emulator for macOS. It provides a more advanced and feature-rich interface for working with the command line.</p> <pre><code>brew install --cask iterm2\n</code></pre>"},{"location":"developertools/software/mac/#install-powershell","title":"Install PowerShell","text":"<p>PowerShell is a command-line shell and scripting language developed by Microsoft. It allows you to automate tasks and manage your computer system efficiently.</p> <pre><code># install\nbrew install --cask powershell\n\n# upgrade\nbrew upgrade --cask powershell\n</code></pre>"},{"location":"developertools/software/mac/#install-vs-code","title":"Install vs code","text":"<p>It is the most commonly used IDE,  It is widely used for programming and supports various languages with extensive customization options.</p> <pre><code>brew install --cask visual-studio-code\n</code></pre>"},{"location":"developertools/software/mac/#install-git","title":"Install git","text":"<p>Git is a version control system used for tracking changes in software projects. It allows multiple developers to collaborate on a project efficiently and helps manage different versions of the code.</p> <pre><code>brew install git \n\n# verify the installation\ngit --version    \n</code></pre>"},{"location":"developertools/software/mac/#install-docker","title":"Install docker","text":"<p>Docker is a platform that simplifies the process of creating, deploying, and running applications using containers. </p> <pre><code>brew install docker --cask\n\n# verify the installation\ndocker --version\n\n# output\nDocker version 20.10.21, build baeda1f\n</code></pre>"},{"location":"developertools/software/mac/#install-node","title":"Install node","text":"<p>Node.js is a JavaScript runtime that allows you to execute JavaScript code outside of a web browser.</p> <pre><code>brew install node\n\n# verify the installation\nnode --version  \nnpm version\n</code></pre>"},{"location":"developertools/software/mac/#install-dotnet","title":"Install dotnet","text":"<p>It provides a runtime environment and libraries for building and running different types of applications, especially those developed using the C# programming language. </p> <pre><code>brew install --cask dotnet\n\n# verify the installation\ndotnet --version\n</code></pre>"},{"location":"developertools/software/mac/#install-python3","title":"Install python3","text":"<p>Python is a popular programming language known for its simplicity and versatility. Installing Python 3 refers to setting up the latest version of Python on your system for development purposes.</p> <pre><code>brew install python3\n\n# verify the installation\npython3 --version\n</code></pre>"},{"location":"developertools/software/mac/#upgrade-pip","title":"Upgrade pip","text":"<pre><code>pip3 install --upgrade pip\n</code></pre>"},{"location":"developertools/software/mac/#install-mkdocs","title":"Install mkdocs","text":"<pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-material-extensions\n</code></pre>"},{"location":"developertools/software/mac/#install-azure-cli","title":"Install azure-cli","text":"<p>Azure CLI is a command-line interface for managing and interacting with Microsoft Azure cloud services. It provides a convenient way to automate and control your Azure resources.</p> <pre><code>brew install azure-cli\n</code></pre>"},{"location":"developertools/software/mac/#install-kubectl","title":"Install kubectl","text":"<p>kubectl is a command-line tool used to interact with Kubernetes clusters. It enables you to deploy, manage, and monitor applications running on Kubernetes.</p> <p>For more information, please refer to the following link: - - https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/</p> <p><pre><code>brew install kubectl\n</code></pre> verify the installation</p> <pre><code>kubectl version \n</code></pre>"},{"location":"developertools/software/mac/#install-lens","title":"Install Lens","text":"<p>Lens is a popular Kubernetes platform that serves as a robust and advanced development and management environment for Kubernetes clusters. It provides a comprehensive graphical user interface (GUI) that simplifies the management, monitoring, and interaction with Kubernetes resources and clusters.</p> <pre><code>brew install --cask lens\n</code></pre>"},{"location":"developertools/software/mac/#install-terraform","title":"Install Terraform","text":"<p>Terraform is an infrastructure-as-code tool used for provisioning and managing cloud resources. It allows you to define your infrastructure in code and automates the deployment and management of resources across different cloud providers.</p> <pre><code>brew install terraform\n</code></pre> <p>verify the installation</p> <pre><code>terraform version\n</code></pre>"},{"location":"developertools/software/mac/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. It allows you to define and install applications using charts, which are packages of pre-configured Kubernetes resources.</p> <pre><code>brew install helm\n</code></pre> <p>verify the installation</p> <pre><code>helm version\n</code></pre>"},{"location":"developertools/software/mac/#install-argocd","title":"Install argocd","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.</p> <pre><code>brew install argocd\n</code></pre> <p>verify the installation</p> <pre><code>argocd version\n</code></pre>"},{"location":"developertools/software/mac/#install-pgadmin4","title":"Install pgadmin4","text":"<p>pgAdmin is a graphical administration and development platform for PostgreSQL, a popular open-source relational database management system. It provides a user-friendly interface for managing databases, running queries, and monitoring server activity.</p> <pre><code>brew install pgadmin4\n</code></pre>"},{"location":"developertools/software/mac/#install-azure-data-studio","title":"Install Azure Data Studio","text":"<p>Data management tool that enables working with SQL Server, PostgerSQL, MySQL and more.</p> <pre><code>brew install --cask azure-data-studio\n</code></pre>"},{"location":"developertools/software/mac/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<p>Azure Storage Explorer is a standalone application provided by Microsoft that allows users to interact Azure storage account services and making it easier to work with Azure Blob Storage, Azure Queue Storage, Azure Table Storage, and Azure Cosmos DB.</p> <pre><code>brew install --cask microsoft-azure-storage-explorer\n</code></pre>"},{"location":"developertools/software/mac/#install-microsoft-remote-desktop","title":"Install Microsoft Remote Desktop","text":"<p>Microsoft Remote Desktop is a Microsoft application that allows users to remotely access and control Windows-based computers or virtual machines from other devices.</p> <pre><code>brew install --cask microsoft-remote-desktop\n</code></pre>"},{"location":"developertools/software/mac/#install-zoom","title":"Install zoom","text":"<p>Zoom is a video conferencing and online meeting platform that enables you to communicate with others through video, audio, and chat. It's widely used for remote work and virtual meetings.</p> <pre><code>brew install --cask zoom\n</code></pre>"},{"location":"developertools/software/mac/#install-teams","title":"Install Teams","text":"<p>Teams is a collaboration platform developed by Microsoft. It offers features such as chat, video meetings, file sharing, and integration with other Microsoft services to facilitate teamwork and communication.</p> <pre><code># install\nbrew install --cask microsoft-teams\n\n# uninstall teams, it will ask the Mac login password for any software uninstall.\nbrew uninstall microsoft-teams\n</code></pre>"},{"location":"developertools/software/mac/#install-whatsapp","title":"Install WhatsApp","text":"<p>Native desktop client for WhatsApp</p> <pre><code>brew install --cask whatsapp\n</code></pre>"},{"location":"developertools/software/mac/#install-net-7-sdk-for-mac","title":"Install .NET 7 SDK for Mac","text":"<p>The .NET SDK (Software Development Kit) is a set of tools and libraries provided by Microsoft for building applications using the .NET framework. Installing the .NET 7 SDK specifically refers to setting up the latest version of the .NET framework for Mac development.</p> <p>Use the following link for download and install it manually.</p> <p>https://dotnet.microsoft.com/en-us/download/dotnet/thank-you/sdk-7.0.100-macos-x64-installer?journey=vs-code</p>"},{"location":"developertools/software/mac/#install-github","title":"Install github","text":"<p>GitHub is a web-based platform for version control and collaboration that allows developers to host, review, and manage code repositories. It's widely used for open-source projects and collaborative development.</p> <pre><code>brew install gh\n</code></pre>"},{"location":"developertools/software/mac/#install-jq","title":"Install jq","text":"<p>JQ is a lightweight and flexible command-line tool for processing JSON data. It provides various features to extract, manipulate, and transform JSON files efficiently.</p> <pre><code>brew install jq\n</code></pre>"},{"location":"developertools/software/mac/#install-postman","title":"Install postman","text":"<p>Postman is a popular API development and testing tool. It allows you to make HTTP requests, test APIs, and automate API workflows, making it easier to develop and debug APIs.</p> <pre><code>brew install --cask postman\n</code></pre>"},{"location":"developertools/software/mac/#install-lightshot","title":"Install Lightshot","text":"<p>Lightshot is a free program that offers a quick and easy way to capture a screen including basic editing tools. </p> <p>I don't see brew commands to install this tool but you can install it manually from here:</p> <p>Lightshot downloads</p>"},{"location":"developertools/software/windows/","title":"Download &amp; install Software in Windows OS","text":""},{"location":"developertools/software/windows/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive list of essential tools and software commonly needed by developers in the IT industry. If you are using Window operating system, you can download and install the following software as per your requirements. It is recommended to use the choco tool for installation, but if you encounter any issues, you can also perform a direct install.</p> <p>Note</p> <p>Restart your computer when prompted or needed and also restart your terminal for every install before validate it.</p>"},{"location":"developertools/software/windows/#install-chocolatey","title":"Install Chocolatey","text":"<p>We will be using Chocolatey commands to install all the required software and developer tools.</p> <p>What is Chocolatey?</p> <p>Chocolatey is a package manager for Windows that enables you to install, upgrade, and manage software packages from the command line.</p> <p>It is strongly recommended to use choco commands for searching and installing the required software instead of attempting manual installations using the provided links.</p> <p>To use Chocolatey, you will need to have Windows PowerShell installed on your system. You can then install Chocolatey using a PowerShell command, and use it to install and manage packages from the command line.</p> <p>Install Chocolatey</p> <p>This is the first step you need to take before installing anything on the Windows OS.</p> <p>To install Chocolatey on your Windows system, open a terminal window (such as PowerShell or Command Prompt) and run as Administrator with the following command:</p> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> <p>Reference - https://docs.chocolatey.org/en-us/choco/setup</p> <p>Verify the version installed <pre><code>choco --version\n</code></pre></p> <p>Upgrading Chocolatey <pre><code>choco upgrade chocolatey\n</code></pre></p>"},{"location":"developertools/software/windows/#install-vs-code","title":"Install VS code","text":"<p>It is the most commonly used IDE, It is widely used for programming and supports various languages with extensive customization options.</p> <p>Open Command Prompt (cmd) or PowerShell as Administrator and run:</p> <pre><code>choco install vscode\n</code></pre> <p>Verify the installation</p> <pre><code>code --help\n\n# or\n\ncode --version\n\n\n# Check for the latest version (available via choco)\n\nchoco info vscode\n</code></pre> <p>Upgrade VS Code to the latest version</p> <pre><code>choco upgrade vscode -y\n</code></pre> <p>Note:    \"Yes to all prompts\" or \"Automatically accept all confirmations.\"</p>"},{"location":"developertools/software/windows/#vs-code-extensions","title":"VS Code Extensions","text":"<p>After install VS code, install the following extensions in vs code as per the need:</p> <pre><code>Azure CLI Tools\nAzure Account\nAzure Kubernetes Services\nKubernetes\nAzure Terraform\nC#\nBridge to Kubernetes\nDev Containers\nRemote Development\nDocker\nDotnet\nHelm Intelligence\nvscode-helm\nKubernetes \u2013 very helpful for debugging services in AKS \nPostgreSQL - get this from Microsoft \nPowerShell Extension- https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell\nMongoDB\n</code></pre> <p>You can install directly using the VS Code CLI:</p> <pre><code>code --install-extension ms-vscode.azurecli\n# or\ncode --install-extension ms-vscode.azurecli --force\n\ncode --install-extension ms-vscode.azure-account\ncode --install-extension ms-kubernetes-tools.vscode-aks-tools\ncode --install-extension ms-kubernetes-tools.vscode-kubernetes-tools\ncode --install-extension hashicorp.terraform\ncode --install-extension ms-dotnettools.csharp\ncode --install-extension microsoft.k8s-bridge-to-kubernetes\ncode --install-extension ms-vscode-remote.remote-containers\ncode --install-extension ms-vscode-remote.vscode-remote-extensionpack\ncode --install-extension ms-azuretools.vscode-docker\ncode --install-extension formulahendry.dotnet\ncode --install-extension tht13.helm-intellisense\ncode --install-extension tim-koehler.vscode-helm\ncode --install-extension ms-vscode.mssql\ncode --install-extension ms-vscode.powershell\ncode --install-extension mongodb.mongodb-vscode\n</code></pre> <p>List All Installed Extensions</p> <pre><code>code --list-extensions\n</code></pre>"},{"location":"developertools/software/windows/#install-git","title":"Install Git","text":"<p>Git is a version control system used for tracking changes in software projects. It allows multiple developers to collaborate on a project efficiently and helps manage different versions of the code.</p> <pre><code>choco install git -y\n</code></pre> <pre><code>git --version\n</code></pre> <p>Upgrade Git to the Latest Version</p> <pre><code>choco upgrade git -y\n</code></pre>"},{"location":"developertools/software/windows/#install-chrome","title":"Install Chrome","text":"<p>Chrome is a popular web browser developed by Google. It provides a fast and secure browsing experience and supports various web technologies.</p> <pre><code>choco install googlechrome  -y\n</code></pre> <p>Upgrade Google Chrome</p> <pre><code>choco upgrade googlechrome -y\n</code></pre>"},{"location":"developertools/software/windows/#install-node-js","title":"Install Node JS","text":"<p>Node.js is a JavaScript runtime that allows you to execute JavaScript code outside of a web browser.</p> <pre><code>choco install nodejs -y\n</code></pre> <pre><code>node --version\n# or\nnode -v\nnpm -v\n</code></pre> <p>Upgrade Node.js</p> <pre><code>choco upgrade nodejs -y\n</code></pre>"},{"location":"developertools/software/windows/#install-docker","title":"Install Docker","text":"<p>Docker is a platform that simplifies the process of creating, deploying, and running applications using containers.</p> <pre><code>choco install docker-desktop -y\n</code></pre> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>Upgrade Docker Desktop</p> <pre><code>choco upgrade docker-desktop -y\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-cli","title":"Install Azure CLI","text":"<p>Azure CLI is a command-line interface for managing and interacting with Microsoft Azure cloud services. It provides a convenient way to automate and control your Azure resources.</p> <pre><code>choco install azure-cli -y\n</code></pre> <pre><code>az --version\n</code></pre> <p>Upgrade Azure CLI</p> <pre><code>choco upgrade azure-cli -y\n</code></pre> <p>Useful Commands</p> <pre><code>az account list --output table\naz account show\naz --help\n</code></pre>"},{"location":"developertools/software/windows/#install-terraform","title":"Install Terraform","text":"<p>Terraform is an infrastructure-as-code tool used for provisioning and managing cloud resources. It allows you to define your infrastructure in code and automates the deployment and management of resources across different cloud providers.</p> <pre><code>choco install terraform -y\n</code></pre> <pre><code>terraform --version\n</code></pre> <p>Upgrade Terraform</p> <pre><code>choco upgrade terraform -y\n</code></pre> <pre><code>terraform -help\n</code></pre>"},{"location":"developertools/software/windows/#install-kubectl","title":"Install kubectl","text":"<p>kubectl is a command-line tool used to interact with Kubernetes clusters. It enables you to deploy, manage, and monitor applications running on Kubernetes.</p> <p><pre><code>choco install kubernetes-cli\n</code></pre> For more information, please refer to the following link: - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/</p> <pre><code># Test to ensure the version you installed is up-to-date:\nkubectl version\nkubectl version --client\n\n# use this for detailed view of version:\nkubectl version --client --output=yaml\n\nor \nkubectl version --output=json\n\n#Verify kubectl configuration\nkubectl cluster-info\n</code></pre> <p>Upgrade kubectl</p> <pre><code>choco upgrade kubernetes-cli -y\n</code></pre>"},{"location":"developertools/software/windows/#install-lens","title":"Install Lens","text":"<p>Lens is a popular Kubernetes platform that serves as a robust and advanced development and management environment for Kubernetes clusters. It provides a comprehensive graphical user interface (GUI) that simplifies the management, monitoring, and interaction with Kubernetes resources and clusters.</p> <pre><code>choco install lens\n</code></pre> <pre><code>lens --version\n</code></pre> <p>Upgrade Lens</p> <pre><code>choco upgrade lens -y\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-kubelogin","title":"Install azure kubelogin","text":"<p>Azure Kubelogin is a tool that enables seamless authentication and access to Azure Kubernetes Service (AKS) clusters using your Azure credentials.</p> <pre><code>choco install azure-kubelogin\n</code></pre>"},{"location":"developertools/software/windows/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. It allows you to define and install applications using charts, which are packages of pre-configured Kubernetes resources.</p> <pre><code>choco install kubernetes-helm -y\n</code></pre> <pre><code>helm version\n</code></pre> <pre><code>choco upgrade kubernetes-helm -y\n</code></pre>"},{"location":"developertools/software/windows/#install-pgadmin4","title":"Install pgadmin4","text":"<p>pgAdmin is a graphical administration and development platform for PostgreSQL, a popular open-source relational database management system. It provides a user-friendly interface for managing databases, running queries, and monitoring server activity.</p> <pre><code>choco install pgadmin4 -y\n</code></pre> <p>Upgrade pgAdmin 4</p> <pre><code>choco upgrade pgadmin4 -y\n</code></pre>"},{"location":"developertools/software/windows/#install-postgresql","title":"Install PostgreSQL","text":"<p>To install PostgreSQL, you can use Chocolatey by running the following command from the command line or PowerShell:</p> <pre><code>choco install postgresql -y\n</code></pre> <p>Alternatively, you can choose to use the graphical installation wizard for PostgreSQL on Windows.</p> <p>Downloading PostgreSQL</p> <p>Follow the installation wizard instructions to complete the PostgreSQL installation on your Windows system.</p> <p>Note</p> <p>Depending on your requirements, you may want to uncheck the option for <code>Stack Builder</code> during the installation process.</p> <p>psql - Command-line tools</p> <p>By default, the installer does not modify the system path. If you wish to use command-line tools like <code>psql</code>, you will need to manually add PostgreSQL to your system's  path after installation.</p> <p>Update Environment Variables</p> <p>To update the system path and include PostgreSQL in it, add the following directory to your PATH variable:</p> <p><pre><code>C:\\Program Files\\PostgreSQL\\16\\bin\n</code></pre> Make sure to replace 16 with your specific PostgreSQL version if it differs.</p> <p>Note</p> <p>System restart is needed after the isntallation</p> <p>verify the psql installation</p> <pre><code>psql --version\n\n#output\npsql (PostgreSQL) 16.1\n</code></pre>"},{"location":"developertools/software/windows/#install-argocd","title":"Install argocd","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.</p> <pre><code>choco install argocd -y\n</code></pre> <pre><code>argocd version\n\n# output\nargocd: v2.4.7+81630e6\n  BuildDate: 2022-07-18T21:49:23Z\n  GitCommit: 81630e6d5075ac53ac60457b51343c2a09a666f4\n  GitTreeState: clean\n  GoVersion: go1.18.3\n  Compiler: gc\n  Platform: windows/amd64\nargocd-server: v2.5.2+148d8da\n</code></pre> <p>Upgrade Argo CD</p> <pre><code>choco upgrade argocd -y\n</code></pre>"},{"location":"developertools/software/windows/#install-sql-server","title":"Install SQL server","text":"<p>SQL Server is a relational database management system developed by Microsoft. It allows you to store, manage, and retrieve structured data efficiently.</p> <pre><code>choco install sql-server-management-studio\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-data-studio","title":"Install Azure Data Studio","text":"<p>Azure Data Studio is a cross-platform database tool for data professionals using the Microsoft family of on-premises and cloud data platforms on Windows, MacOS, and Linux.</p> <pre><code>choco install azure-data-studio\n</code></pre>"},{"location":"developertools/software/windows/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<p>Azure Storage Explorer is a standalone application provided by Microsoft that allows users to interact Azure storage account services and making it easier to work with Azure Blob Storage, Azure Queue Storage, Azure Table Storage, and Azure Cosmos DB.</p> <pre><code>choco install microsoftazurestorageexplorer\n</code></pre>"},{"location":"developertools/software/windows/#install-rdcman","title":"Install RDCMan","text":"<p>RDCMan, short for Remote Desktop Connection Manager, is a free Microsoft Windows utility used to manage multiple remote desktop connections from a single application. It is particularly useful to connect and manage multiple remote servers, workstations, or virtual machines.</p> <p>Key Features</p> <ul> <li>Group RDP sessions for easy organization    </li> <li>Save credentials per group or server    </li> <li>Automatically reconnect dropped sessions    </li> <li>Session-wide actions like log off, disconnect, or reconnect    </li> <li>Custom display resolution and settings for individual servers</li> </ul> <p>Option 1: Install via Chocolatey </p> <pre><code>choco install remote-desktop-connection-manager\n\n# To upgrade:\nchoco upgrade remote-desktop-connection-manager\n</code></pre> <p>Option 2: Manual Installation</p> <p>Download RDCMan from the official Microsoft site or trusted repository: *   Microsoft RDCMan Download Page</p>"},{"location":"developertools/software/windows/#mremoteng","title":"mRemoteNG","text":"<p>mRemoteNG (multi-Remote Next Generation) is an open-source. It supports multiple remote connection protocols, making it a powerful alternative to RDCMan with broader capabilities.   </p> <p>Key Features</p> <ul> <li>Tabbed interface for managing multiple sessions    </li> <li>Support for a wide range of protocols - RDP, VNC, SSH, Telnet, ICA, HTTP/HTTPS, RLogin, Raw Socket   </li> <li>Credential inheritance and secure storage    </li> <li>Connection tree and grouping for organization    </li> <li>Import/Export of configuration files</li> </ul> <p>Option 1: Install via Chocolatey (Recommended)</p> <p><pre><code>choco install mremoteng\n\n# To upgrade\nchoco upgrade mremoteng\n</code></pre> Option 2: Manual Installation</p> <p>Download Installer from the official site:     *   https://mremoteng.org/download</p>"},{"location":"developertools/software/windows/#install-dotnet","title":"Install dotnet","text":"<p>Microsoft .NET 8.0 Runtime 8.0.10</p> <pre><code>choco install dotnet-8.0-runtime\n</code></pre> <p>Microsoft .NET 8.0 SDK 8.0.403</p> <pre><code>choco install dotnet-8.0-sdk\n# verify the installation\ndotnet --version\n</code></pre>"},{"location":"developertools/software/windows/#install-python","title":"Install Python","text":"<p>Python is a popular programming language known for its simplicity and versatility. It is widely used for web development, data analysis, and scripting tasks.</p> <pre><code>choco install python\n</code></pre>"},{"location":"developertools/software/windows/#verify-pip-installation","title":"verify Pip installation","text":"<pre><code>pip --version\n</code></pre>"},{"location":"developertools/software/windows/#install-wsl","title":"Install WSL","text":"<p>Windows Subsystem for Linux (WSL) is a feature of Windows that allows developers to run a Linux environment without the need for a separate virtual machine or dual booting. </p> <pre><code>choco install wsl\n</code></pre> <pre><code>wsl --help\n// or\nwsl --list --verbose\n</code></pre> <p>Update WSL</p> <pre><code>wsl --update\n</code></pre>"},{"location":"developertools/software/windows/#install-ubunto","title":"Install Ubunto","text":"<p>Ubuntu on Windows via WSL (Windows Subsystem for Linux) is incredibly useful \u2014 it gives you the power of Linux without leaving Windows.</p> <p>install Ubuntu</p> <pre><code>wsl --install -d Ubuntu\n</code></pre> <p>Check WSL Version</p> <pre><code>wsl --list --verbose\n</code></pre>"},{"location":"developertools/software/windows/#install-ansible","title":"Install Ansible","text":"<p>Ansible is an open-source automation tool used for configuring systems, deploying software, and orchestrating IT infrastructure. It uses simple, human-readable YAML files (called playbooks) and requires no agents on managed machines.</p> <p>install Ansible easily on Ubuntu (on WSL or native Linux)</p> <pre><code># Step 1: Update your package list\nsudo apt update\n# Step 2: Install required dependencies\nsudo apt install -y software-properties-common\n# Step 3: Add the official Ansible PPA (Personal Package Archive)\nsudo add-apt-repository --yes --update ppa:ansible/ansible\n# Step 4: Install Ansible\nsudo apt install -y ansible\n# Step 5: Verify the installation\nansible --version\n</code></pre> <p>Test Ansible with a Simple Command</p> <pre><code>ansible localhost -m ping\n\n# If it's working, you\u2019ll see:\nlocalhost | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"developertools/software/windows/#install-jq","title":"Install JQ","text":"<p>JQ is a lightweight and flexible command-line tool for processing JSON data. It provides various features to extract, manipulate, and transform JSON files efficiently.</p> <pre><code>choco install jq\n</code></pre>"},{"location":"developertools/software/windows/#install-mongodb-compass","title":"Install MongoDB Compass","text":"<p>Install MongoDB Compass using Chocolatey <pre><code>choco install mongodb-compass -y\n</code></pre> Check MongoDB Version</p> <pre><code>mongod --version\n</code></pre> <pre><code>choco upgrade mongodb-compass -y\n</code></pre>"},{"location":"developertools/software/windows/#install-postman","title":"Install Postman","text":"<p>Postman is a popular API development and testing tool. It allows you to make HTTP requests, test APIs, and automate API workflows, making it easier to develop and debug APIs.</p> <pre><code>choco install postman -y\n</code></pre> <p>Upgrade Postman <pre><code>choco upgrade postman -y\n</code></pre></p>"},{"location":"developertools/software/windows/#install-notepad","title":"Install Notepad++","text":"<p><pre><code>choco install notepadplusplus -y\n</code></pre> Verify Installation: <pre><code>notepad++\n</code></pre></p>"},{"location":"developertools/software/windows/#windows-terminal","title":"Windows Terminal","text":"<p>Allows us to access multiple command-line tools and shells in one customizable interface. It is an open-source project developed and maintained by Microsoft.</p> <pre><code>choco install microsoft-windows-terminal\n</code></pre>"},{"location":"developertools/software/windows/#install-lightshot","title":"Install Lightshot","text":"<p>Lightshot is a free program that offers a quick and easy way to capture a screen including basic editing tools. </p> <p>I don't see brew commands to install this tool but you can install it manually from here:</p> <p>Lightshot downloads</p>"},{"location":"developertools/software/windows/#manual-install","title":"Manual Install","text":"<p>If you would prefer to install manually then here is the same list of software you can download and Install them manually</p> <ul> <li>Visual studio code (recommended) - https://code.visualstudio.com/download/</li> <li>Visual studio (optional) - https://visualstudio.microsoft.com/downloads/</li> <li>SQL Server Management Studio (optional) - https://www.microsoft.com/en-us/sql-server/sql-server-downloads</li> <li>Notepad++ - https://notepad-plus-plus.org/downloads/</li> <li>Google Chrome \u2013 search in google for later version of Google Chrome and download &amp; install it</li> <li>Node JS - https://nodejs.org/en/download/</li> <li>Git - https://git-scm.com/download/win</li> <li>Docker desktop - https://docs.docker.com/desktop/install/windows-install/</li> </ul>"},{"location":"developertools/software/windows/#additional-software","title":"Additional Software","text":"<p>You may need these additional software to perform daily activities.</p> <ul> <li>Zoom - https://zoom.us/download</li> <li>Teams - https://www.microsoft.com/en-us/microsoft-teams/download-app</li> <li>WhatsApp - https://www.whatsapp.com/download</li> <li>SQL Search - https://www.red-gate.com/products/sql-development/sql-search/</li> <li>JSON viewer online - https://codebeautify.org/jsonviewer</li> <li>regexr validator - https://regexr.com/</li> <li>SAML Tracer Browser extention</li> <li>WSL - https://learn.microsoft.com/en-us/windows/wsl/install</li> <li>Azure storage-explorer</li> <li>RDC - https://learn.microsoft.com/en-us/sysinternals/downloads/rdcman</li> <li>base64encode - https://www.base64encode.org/</li> <li>LightShot (prntscr) - https://app.prntscr.com/en/</li> </ul>"},{"location":"developertools/software/workstation/","title":"Developer Workstation Configuration","text":"<p>In this article, I am going to present some general recommendations for a developer workstation. However, if you are an expert in computer hardware, you can customize and build your own workstation according to your specific needs:</p> <ul> <li>A fast processor with multiple cores (e.g. Intel Core i7 or AMD Ryzen)</li> <li>At least 8GB of RAM (16GB or more is recommended for larger projects)</li> <li>A solid-state drive (SSD) for the operating system and frequently-used files</li> <li>A larger hard drive (HDD) for storing project files and data</li> <li>A high-resolution dual monitors with good color accuracy</li> <li>A comfortable and ergonomic keyboard and mouse</li> <li>A modern operating system (e.g. Windows 10, macOS, or a Linux distribution)</li> <li>A development environment or integrated development environment (IDE) suitable for the programming languages and frameworks being used</li> <li>Any necessary tools and libraries for the developer's projects - will be explained in the next topic</li> <li>A version control system (e.g. Git) for managing code</li> </ul> <p>It's also important for a developer workstation to have good connectivity and a reliable internet connection, as developers often need to access online resources and collaborate with other team members.</p>"},{"location":"developertools/tools/useful-tools/","title":"Useful Tools","text":""},{"location":"developertools/tools/useful-tools/#network-troubleshooting","title":"Network troubleshooting","text":"<p>Here are some useful tools and links that can be helpful for troubleshooting network-related issues, monitoring network performance, and analyzing network traffic. </p> Tool/Resource Description Ping Ping is a command-line tool that sends ICMP Echo Request packets to test network connectivity. Traceroute Traceroute is used to trace the route packets take through the internet to a destination host. Wireshark Wireshark is a powerful network protocol analyzer that can capture and inspect network traffic. Netstat Netstat is a command-line tool that displays network connections, routing tables, and interface statistics. Nslookup Nslookup is a command-line tool for querying DNS (Domain Name System) to resolve domain names to IP addresses. Nmap Nmap is a versatile network scanning tool that can discover hosts and services on a network. TCPDump TCPDump is a command-line packet analyzer that captures and displays network traffic in real-time. ipconfig (Windows) / ifconfig (Linux) Display and configure network interfaces on Windows (ipconfig) and Linux (ifconfig). Netcat Netcat, also known as the \"Swiss Army Knife\" of networking, is a versatile networking utility. Fiddler Fiddler is a web debugging proxy that can capture and inspect HTTP/HTTPS traffic between a client and server. GRC ShieldsUP! ShieldsUP! is an online tool for testing the security of your firewall by probing for open ports. MTR (My TraceRoute) MTR combines the functionality of ping and traceroute to provide detailed network path analysis. Speedtest.net Speedtest.net allows you to test your internet connection's speed and latency to various servers. WhatIsMyIP.com WhatIsMyIP.com provides information about your public IP address and geolocation."},{"location":"developertools/tools/useful-tools/#dns-troubleshooting","title":"DNS troubleshooting","text":"<p>Here are some useful tools and links for DNS (Domain Name System) troubleshooting:</p> Tool/Resource Description Nslookup Nslookup is a command-line tool for querying DNS to resolve domain names to IP addresses and vice versa. Dig Dig is a command-line tool for querying DNS servers to retrieve DNS records and information. Nslookup Online An online Nslookup tool that provides DNS lookup results for domain names and IP addresses. MXToolBox MXToolBox provides a collection of DNS and network diagnostic tools, including DNS lookup and DNS health checks. DNSQuery.org DNSQuery.org offers online DNS lookup and DNS resolution services for domain names and IP addresses. DNS Checker DNS Checker allows you to check DNS records, DNS propagation, and DNS speed for a domain name. WhatIsMyDNS WhatIsMyDNS provides information about the DNS servers that resolve a specific domain name. IntoDNS IntoDNS checks the health and configuration of your DNS servers and provides a detailed report. DnsViz DnsViz is a tool for visualizing the DNS resolution process and displaying DNSSEC-related information. Google Public DNS Information about using Google Public DNS servers, which can help with DNS resolution issues. Cloudflare DNS Cloudflare offers a fast and privacy-focused DNS resolver that can be used to troubleshoot DNS issues. Quad9 DNS Quad9 provides a secure and privacy-aware DNS service that blocks malicious domains. OpenDNS OpenDNS offers DNS resolution with security features and content filtering capabilities."},{"location":"developertools/tools/useful-tools/#other-tools","title":"Other Tools","text":"Tool/Resource Description Password Generator Random Password Generator base64encode Encode to Base64 format JSON Web Tokens JSON Web Tokens ipchicken identifies the IP address of your system"},{"location":"developertools/tools/useful-tools/#azure","title":"Azure","text":"Tool/Resource Description Azure Resource Explorer Azure Resource Explorer"},{"location":"personal/breakfast-recipes/","title":"Breakfast Recipes","text":""},{"location":"personal/breakfast-recipes/#upma-recipe","title":"Upma Recipe","text":"<p>Upma! Basic and popular South Indian Breakfast Recipe which is loved by everyone.</p> <p>Ingredients</p> <ul> <li>2 cup Rava | suji or semolina flour</li> <li>\u00bd tsp Asafoetida</li> <li>5 piece Beans</li> <li>1 Carrot, medium</li> <li>4 Chili pepper, Green</li> <li>2 tbsp Coriander</li> <li>2 tsp Ginger</li> <li>2 Onion, medium</li> <li>1 tsp Lemon juice</li> <li>2 Salt,- 1 tsp Turmeric, Powder</li> <li>5 tbsp Vegetable oil</li> <li>1 cup Milk</li> <li>10 piece Cashews, - 1 tsp Split Urad dal</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/breakfast-recipes/#poha-recipe","title":"Poha Recipe","text":"<p>Poha is a most popular breakfast or snack recipe made with flattened rice (poha), onions, roasted peanuts, and a few spices.</p> <p>Ingredients</p> <ul> <li>2 tbsp Coriander, leaves</li> <li>2 Green chilies</li> <li>1 Onion ((about \u00be cup), medium</li> <li>2 tbsp Peanuts</li> <li>1 Potato, medium</li> <li>1 tbsp Lemon juice</li> <li>\u00bd tsp Mustard seeds</li> <li>1 Salt</li> <li>\u00bd tsp Sugar</li> <li>\u00bd tsp Turmeric</li> <li>2 tbsp Oil</li> <li>1 sprig Curry, leaves</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/breakfast-recipes/#uttapam-recipe","title":"Uttapam Recipe","text":"<p>vegetable uttapam recipe | veg uttapam | mixed veggie uttapa</p> <p>Ingredients</p> <ul> <li>1 Carrot</li> <li>1 Chilli</li> <li>2 tbsp Coriander</li> <li>1 inch Ginger</li> <li>1 Onion</li> <li>2 cup Idli rice</li> <li>1 cup Poha / aval / flattened rice, thin</li> <li>\u00bd tsp Methi / fenugreek</li> <li>2 \u00bc tsp Salt</li> <li>1 Oil</li> <li>3 Curry, leaves</li> <li>1 Water (for soaking &amp; grinding)</li> <li>\u00bd capsicum (finely chopped)</li> <li>1 To mato (finely chopped)</li> <li>\u00bd cup urad dal</li> </ul> <p>Reference: - Pintrest:</p>"},{"location":"personal/breakfast-recipes/#mooli-paratha","title":"Mooli Paratha","text":"<p>Make delicious and healthy stuffed mooli parathas with this easy recipe. They go well with yogurt, pickle or a chutney. </p> <p>Ingredients</p> <ul> <li>3 tbsp Coriander, fine leaves</li> <li>\u00bd inch Ginger</li> <li>1 Green chilies</li> <li>4 cups Radish</li> <li>\u00bd tsp Garam masala</li> <li>\u2153 tsp Kashmiri red chili powder</li> <li>\u00bd tsp Salt</li> <li>\u215b tsp Turmeric</li> <li>2 cups Wheat flour</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/dinner-recipes/","title":"Dinner Recipes","text":""},{"location":"personal/dinner-recipes/#dosa-recipe","title":"Dosa Recipe","text":"<p>Dosa is a traditional South Indian crepe/pancake made from fermented rice and dal batter. </p> <p>Ingredients</p> <ul> <li>3 cups Idli rice</li> <li>1 \u00bd tsp Fenugreek seeds</li> <li>2 tsp Salt</li> <li>1 Water</li> <li>\u00be Cup Poha</li> <li>1 Cup Urad dal</li> </ul> <p>Reference: - Pintrest:</p>"},{"location":"personal/dinner-recipes/#idli-recipe","title":"Idli Recipe","text":"<p>Ingredients</p> <ul> <li>2 cup Idli rice or sona masuri rice</li> <li>1 cup Poha / aval / avalakki / flattened rice, thick</li> <li>1 tsp Salt</li> <li>1 Oil</li> <li>1 Water</li> <li>1 cup Urad dal</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/lunch-recipes/","title":"Lunch Recipes","text":""},{"location":"personal/lunch-recipes/#hyderabad-bawarchi-style-mutton-dum-biryani","title":"Hyderabad Bawarchi Style Mutton Dum Biryani","text":"<p>The Special Hyderabadi Mutton Dum Biryani </p> <p>Ingredients</p> <ul> <li>Marinate meat</li> <li>1 Chillies, Green</li> <li>1 Coriander, Powder</li> <li>2 Coriander small bunch, Green</li> <li>1 Juice of a lemon</li> <li>1 Mint - small bunch</li> <li>3 Onions, Medium Size</li> <li>1 Onions, Fried Half</li> <li>1 Curd</li> <li>1 Ginger garlic paste</li> <li>1 Basmati rice</li> <li>1 Cardamoms, Black</li> <li>2 Cinnamon</li> <li>2 Clove</li> <li>2 Garam masala</li> <li>2 Salt</li> <li>1 Turmeric</li> <li>1 Oil</li> <li>1 Cumin, Roasted Powder</li> <li>2 Cumin, Black</li> <li>2 Ghee</li> <li>2 Saffron milk</li> <li>2 Biryani, leaves</li> <li>Mirchi</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#hyderabad-chicken-dum-biryani","title":"Hyderabad Chicken Dum Biryani","text":"<p>The Special Hyderabadi Mutton Dum Biryani </p> <p>Ingredients</p> <ul> <li>1 Chicken</li> <li>1 Chilli, Red Powder</li> <li>1 Chillies, Green</li> <li>1 Coriander, Roasted Powder</li> <li>2 Coriander, Green</li> <li>2 Lemons, Juice </li> <li>1 Little green coriander</li> <li>1 Mint, leaves</li> <li>1 Oil from fried onions</li> <li>1 Oil used to fry onions</li> <li>2 Onions, Big fried</li> <li>1 Onions, Fried</li> <li>1 Curd</li> <li>2 Ginger garlic spread</li> <li>1 Maida flour paste</li> <li>1 Basmati rice</li> <li>2 Cardamom</li> <li>1 Cardamom, Powder</li> <li>2 Cinnamon</li> <li>2 Clove</li> <li>2 Garam masala</li> <li>2 Mace</li> <li>1 Saffron</li> <li>3 Salt</li> <li>1 Star anise</li> <li>2 Cumin, Black</li> <li>1 Cumin, Roasted Powder</li> <li>2 Ghee</li> <li>1 Milk cream</li> <li>1 Biryani, Leaf</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#chicken-curry","title":"Chicken Curry","text":"<p>The Special Hyderabadi Chicken curry recipe </p> <p>Ingredients</p> <ul> <li>500 g Chicken, bone-in</li> <li>1 tsp Coriander, powder</li> <li>3 Onions, medium size</li> <li>3 Tomato puree, fresh medium size</li> <li>1 tbsp Ginger-garlic paste or adrak lahsun paste</li> <li>2 cup Water adjust as per gravy need</li> <li>1 tsp Black pepper, powder</li> <li>2 tsp Fenugreek leaves or kasuri methi., dry</li> <li>2 tsp Garam masala, powder</li> <li>2 tsp Red chili powder</li> <li>1 \u00bd tsp Salt adjust as per need</li> <li>\u00bd tsp Turmeric or haldi, powder</li> <li>2 tsp Lemon juice or vinegar</li> <li>5 tbsp Vegetable oil</li> <li>1 tbsp Cumin seed</li> <li>8 Water soaked cashew nut .optional, hot</li> <li>1 Bay leaf or tej patta</li> <li>4 5 green cardamon or elaichi</li> <li>2 inch Cinnamon or dalchini</li> <li>4 5 cloves or laungh</li> <li>2 tbsp Finely chopped coriander leaves or dhania patta</li> <li>3 Vertically sliced green chilli</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#chicken-65","title":"Chicken 65","text":"<p>The Special Chicken 65 </p> <p>Ingredients</p> <ul> <li>1 \u00be lbs Chicken thighs</li> <li>2 Green chilis</li> <li>2 \u00bd tbsp Ginger garlic spread</li> <li>\u00bc tsp Black pepper</li> <li>2 \u00bd tbsp Corn starch</li> <li>\u00bc tsp Garam masala</li> <li>2 \u2153 tbsp Kashmiri chili powder</li> <li>3 tbsp Rice flour</li> <li>\u00bd tsp Salt</li> <li>\u215b tsp Turmeric</li> <li>1 Neutral oil</li> <li>1 tbsp Olive oil</li> <li>\u00bd tsp Cumin, powder</li> <li>2 sprig Curry, leaves</li> <li>\u00bc cup Dahi</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/yosemite-clouds-rest/","title":"Yosemite clouds rest","text":"<ul> <li>Hike: Clouds Rest (Tenaya Lake / Sunrise Lakes route)</li> <li>Round-trip: ~14.5 miles </li> <li>Elevation gain: ~1,775 ft </li> <li>Summit elevation: ~9,926 ft</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#timeline","title":"Timeline","text":"<ul> <li>04:00 \u2014 Final group check at Yosemite Valley (pack, restrooms, load vehicles).</li> <li>04:30 \u2014 Depart Yosemite Valley.</li> <li>05:30 \u2013 06:00 \u2014 Arrive Sunrise Lakes / Tenaya Lake (Tioga Road) trailhead; final kit check, restroom.</li> <li>06:00 \u2013 06:30 \u2014 Start hike.</li> <li>~10:30 \u2013 11:30 \u2014 Target summit arrival. Lunch / photos (30\u201345 min).</li> <li>11:30 \u2013 15:00 \u2014 Descend to trailhead.</li> <li>~15:30 \u2014 Back to Yosemite Valley or continue onward plans.</li> </ul> <p>Note</p> <p>Driving time Valley \u2192 Tenaya Lake \u2248 ~1 hour depending on traffic and Tioga Road conditions. Confirm Tioga Road seasonal status before leaving.</p>"},{"location":"personal/yosemite-clouds-rest/#route-summary-permits","title":"Route summary &amp; permits","text":"<ul> <li>Trailhead: Sunrise Lakes / Tenaya Lake on Tioga Road.</li> <li>Route: Out-and-back via Sunrise Lakes \u2192 switchbacks \u2192 Clouds Rest ridge \u2192 summit \u2192 return. Ridge has exposed granite and expansive views (Half Dome, Tenaya Canyon).</li> <li>Permits: No wilderness/day-hike permit required for a Clouds Rest day hike. (Overnight/backpacking requires permit.)</li> <li>Roads/season: Tioga Road is seasonal; check current NPS/Yosemite conditions for closures or restrictions before departure.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#personal-checklist","title":"Personal checklist","text":"<ul> <li> Daypack (20\u201330 L)</li> <li> Water: 2.5\u20133 L (minimum 2 L; 3 L recommended at high elevation)</li> <li> Lunch + extra snacks (energy bars, trail mix, nuts)</li> <li> Lightweight lunch bag / ziplock for trash (pack out all trash)</li> <li> Sturdy hiking boots / trail shoes (broken in)</li> <li> Trekking poles (recommended for steep sections)</li> <li> Hat, sunglasses, sunscreen (SPF 30+)</li> <li> Layers: moisture-wicking base, insulating mid-layer, wind/rain shell</li> <li> Headlamp (with fresh batteries)</li> <li> Personal first-aid items (blister care, personal meds)</li> <li> ID, cash / credit card</li> <li> Phone with offline maps preloaded + power bank</li> <li> Lightweight emergency blanket or bivy + whistle</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#group-shared-checklist-distribute-among-group","title":"Group / shared checklist (distribute among group)","text":"<ul> <li> Navigation: printed map or route printout + one device with GPX (designate navigator)</li> <li> Comprehensive first-aid kit \u2014 designate responsible person</li> <li> Emergency shelter / extra emergency blanket(s)</li> <li> Multi-tool + small repair kit (duct tape, patch, needle/thread)</li> <li> Extra 1\u20132 L water (group spare) &amp; basic water-treatment method (filter/drops) \u2014 optional</li> <li> Spare headlamp or spare batteries</li> <li> Trash bag for packing out group waste</li> <li> Bear-aware food storage plan for vehicles (follow NPS rules)</li> <li> Small roll of medical tape, blister supplies</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#safety-reminders-read-aloud-before-starting","title":"Safety reminders (read aloud before starting)","text":"<ul> <li>Start early to avoid afternoon thunderstorms; high country storms develop quickly.</li> <li>Summit altitude is ~9,900 ft \u2014 pace for altitude, hydrate, and rest as needed.</li> <li>Wear traction-capable shoes; granite can be slippery when wet.</li> <li>Cell service is spotty \u2014 do not rely on cell for navigation or rescue. Carry offline maps and tell someone your plan.</li> <li>If anyone shows severe altitude sickness signs (confusion, severe shortness of breath, fainting), descend immediately and seek help.</li> <li>Pack out all trash; follow Leave No Trace principles.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#pre-trip-checks-4824-hours-before","title":"Pre-trip checks (48\u201324 hours before)","text":"<ul> <li> Check Yosemite Current Conditions / Tioga Road status and park alerts.</li> <li> Check weather forecast for Tenaya Lake / Tuolumne Meadows (watch for thunderstorms).</li> <li> Confirm vehicles have fuel and emergency supplies (water, jumper cables, spare tire).</li> <li> Confirm group contact person who is not on the hike and share itinerary (route, start time, vehicle plate).</li> <li> Print or download map/GPS route and verify trailhead coordinates.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#emergency-park-contacts-use-in-an-emergency","title":"Emergency &amp; park contacts (use in an emergency)","text":"<ul> <li>Emergency: 911 (park emergency response).</li> <li>Non-emergency / park dispatch: consult Yosemite NPS current conditions page or your park materials for the local non-emergency number. (Cell service may be limited; plan accordingly.)</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#vehicle-parking-notes","title":"Vehicle &amp; parking notes","text":"<ul> <li>Parking at Sunrise Lakes trailhead is limited; arrive early. If lot is full, find legal roadside parking on Tioga Road and obey all posted signs.</li> <li>If you plan point-to-point logistics, arrange vehicle shuttles in advance. For an out-and-back hike from Sunrise Lakes, a single vehicle is sufficient.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#optional-group-items-if-wanted","title":"Optional group items (if wanted)","text":"<ul> <li>Lightweight camera / binoculars</li> <li>Small pot / stove for warm drink at summit (consider weight and fire rules)</li> <li>Extra warm layer for summit photo stop</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#printable-instruction","title":"Printable instruction","text":"<p>To print: select this checklist text, paste into any text editor or document app, format as needed, then print on US Letter or A4. If you want, I can reformat this to a single-column PDF optimized for printing (US Letter or A4) and post it here again.</p> <p>Have a safe hike \u2014 if you want I can now:</p> <ol> <li>Reformat to A4 or US Letter single-page PDF for printing (render here).</li> <li>Shorten to a \u00bd-page quick checklist.</li> <li>Add a vehicle packing checklist.</li> </ol> <p>Tell me which and I\u2019ll render it immediately in the chat.</p>"},{"location":"resources/azure-cloud/","title":"Azure Cloud - Reference Architecture Guide","text":""},{"location":"resources/azure-cloud/#introduction","title":"Introduction","text":"<p>Welcome to `Step-by-Step Practical Guide on Azure Reference Architecture. In this course, you will learn how to build a comprehensive Azure architecture suitable for small, medium, and even enterprise-scale organizations. Throughout the labs, I'll guide you in deploying a website and its microservices in Azure using Azure DevOps CI &amp; CD Pipelines.  We strongly recommend following the labs in the listed sequence to ensure a clear understanding of the concepts that can be directly applied to your own projects. Our goal is to save your time by providing a similar design solution for your organization. By the end of this course, you will become familiar with the major services offered by Azure and gain real-life project experience. </p>"},{"location":"resources/azure-cloud/#youtube-videos","title":"YouTube videos","text":"Labs Title Lab-1: Introduction &amp; Course Overview Lab-1.1: Create new Azure Account or Tenant or Subscription Lab-2: Application High Level Architecture Lab-3: Azure Architecture Design Principles Lab-4: Azure High Level Architecture (HLD) Lab-5: Azure Architecture - Security Design Lab-6: Azure Architecture Networking Design Lab-7: Azure DevOps Deployment View for Azure Resources Lab-8: Azure Setup Guide / Governance Lab-9: Azure Resource Naming &amp; Tagging Conventions Lab-10: ARM Template Project setup &amp; Create Resource Group Lab-11: Azure Virtual Network Concepts &amp; Create Virtual Network Lab-12: Create Azure AppService Environment from Azure Portal Lab-12.1: Create Azure AppService Environment using ARM template Lab-12.2: Create Tags in ASE using ARM templates Lab-13: Create Azure App Service Plan from Azure Portal Lab-13.1: Create Azure App Service Plan and App Services using ARM Template Lab-14: Deploying ARM Templates using Azure DevOps Pipelines Lab-15:  Deploy a website or APIs to Azure with Azure App Service Lab-16:  Create Azure Application Gateway"},{"location":"resources/azure-cloud/#source-code","title":"Source code","text":"<p>Work in progress...</p>"},{"location":"resources/webapi/","title":".NET Core Web API - Clean Architecture","text":""},{"location":"resources/webapi/#introduction","title":"Introduction","text":"<p>Welcome to the Step-by-Step Practical Guide on .NET Core Web API Clean Architecture YouTube series. In this series, you will learn how to build a RESTful API using the latest version of ASP.NET Core. Throughout the course, you will gain a comprehensive understanding of the clean architecture project structure and learn the fundamentals of RESTful Web API development from scratch. The course also covers topics such as Azure Key Vault configuration, API Health Checks, API versioning, CRUD operations, deploying to Azure, Unit Testing, and more. The course also covers the usage of Entity Framework Core, which will enable you to connect your project's data models with underlying databases. Also, We highly recommend following this series in sequence to ensure a clear understanding of the concepts, which you can then apply to your own projects</p>"},{"location":"resources/webapi/#youtube-videos","title":"YouTube videos","text":"Labs Title YouTube Link Lab-1: Clean Architecture Fundamentals (Theory) Lab-2: Clean Architecture Project Structure Lab-3: ASP.NET Core Web API Basic Configuration Lab-4: Azure Key Vault configuration &amp; Health Checks in ASP.NET Core REST API Lab-5: CRUD Operations In ASP.NET Core Web API with Entity Framework Core Lab-6: How to deploy .NET Core Web API to Azure from locally  How to deploy .NET Core Web API to Azure using DevOps pipeline  Create CI/CD Pipeline for ASP.NET Core Web API  Automate Build and Deployment of Azure SQL Database Lab-7: REST API Best Practices  Lab-8: Unit Testing in ASP.NET Core Web API"},{"location":"resources/webapi/#source-code","title":"Source code","text":"<p>You can access the source code on GitHub by using the following link. This will take you to the repository where all the code for our project is stored. </p> <p>GitHub Source Code Repository</p>"},{"location":"resources/books/kubernetes/","title":"Building Scalable Kubernetes Infrastructure for Microservices","text":"<p>Welcome to my e-Book, <code>Building Scalable Kubernetes Infrastructure for Microservices (A Practical Guide)</code>. In this book, I'm going to share my practical knowledge about creating scalable kubernetes cloud infrastructure for microservices architecture using azure and terraform.</p> <p>To dive into the complete contents of this eBook, simply follow the link provided below. This link will take you to the eBook's dedicated website, where you can access and explore each chapter as it becomes available. I'm excited to have you join me on this journey of learning and discovery, and I look forward to your feedback as we continue to enhance and refine this valuable resource.</p> <p>Read online</p> <p>Thank you for your interest, and happy reading!</p> <p>Note</p> <p>This book is still a work in progress, However, you are still welcome to continue reading what has been written so far.</p> <p> </p>"},{"location":"resources/books/microservices/","title":"Building Microservices with Containers: A Practical Guide","text":"<p>Welcome to my e-Book, <code>Building Microservices with Containers (A Practical Guide)</code>. In this book, I'm going to share my practical knowledge about creating  microservices with docker.</p> <p>To delve into the complete contents of this e-Book, simply download the PDF from the website's or  Read online Free e-Book</p> <p>Thank you for your interest, and happy reading!</p> <p> </p>"},{"location":"resources/publications/algo-trading/algo-trading/","title":"An AI-Driven Real-Time Algorithmic Trading System: Integrating Machine Learning, Hybrid Technical Indicators, and Risk Management","text":""},{"location":"resources/publications/algo-trading/algo-trading/#abstract","title":"Abstract","text":"<p>Algorithmic trading has transformed financial markets by enabling automated, data-driven decision-making at scale. However, many existing systems rely on single-indicator strategies or lack robust real-time validation, limiting their predictive power and adaptability. This paper presents an AI-driven, real-time algorithmic trading system that integrates supervised machine learning, hybrid technical indicators (including VWAP, MACD, RSI, and Bollinger Bands), and dynamic risk management. The system leverages multiple real-time market data providers and alternative data sources, such as news sentiment, to generate high-confidence trading signals. A key innovation is the use of topgainer filters and advanced selection criteria\u2014including price range (\\(1\u2013\\)20), gap percentage, float, and volume\u2014to focus on momentum trading opportunities during high-activity market periods. The platform is highly modular and configurable, supporting rapid adaptation to changing market conditions and robust data validation to ensure signal reliability. Backtesting and live trading simulations across multiple market conditions demonstrate significant improvements in prediction accuracy, risk-adjusted returns, and drawdown control compared to traditional rule-based and single-indicator approaches. For example, the system achieved up to 14% higher returns and 7% lower drawdown in simulated trading compared to baseline strategies. These findings highlight the potential of combining technical analysis, alternative data, and AI for next-generation, robust, and adaptive trading platforms.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#introduction","title":"Introduction","text":"<p>Algorithmic trading, the use of computer algorithms to automate trading decisions and execution, has revolutionized modern financial markets. By leveraging vast amounts of real-time data and advanced analytics, algorithmic trading systems can identify and exploit market opportunities with speed and precision unattainable by human traders. The increasing complexity and volatility of financial markets, however, have exposed the limitations of traditional rule-based and single-indicator strategies, which often fail to adapt to rapidly changing conditions and diverse data sources.</p> <p>Recent advances in artificial intelligence (AI), machine learning (ML), and data availability have opened new avenues for developing intelligent, adaptive trading systems. These systems can process heterogeneous data streams\u2014including price, volume, technical indicators, and alternative data such as news sentiment\u2014to generate more robust and accurate trading signals. Despite these advances, integrating multiple data sources, real-time analytics, and dynamic risk management into a unified, production-ready trading platform remains a significant challenge.</p> <p>This work addresses these challenges by presenting an AI-driven, real-time algorithmic trading system designed specifically for momentum trading during high-activity market periods. The system uses topgainer lists and advanced filtering logic\u2014including price range (\\(1\u2013\\)20), gap percentage, float, and volume\u2014to identify stocks with strong price movement and liquidity. Multiple real-time market data providers are used to ensure data reliability and coverage, while hybrid technical indicators and supervised machine learning models drive signal generation and adaptive decision-making. Robust risk management modules dynamically control position sizing and stop-loss thresholds, and the system\u2019s modular, highly configurable architecture allows for rapid adaptation to changing market conditions. The platform also features robust data validation and missing data handling, further enhancing reliability in volatile markets.</p> <p>The main contributions of this work are:</p> <ol> <li>The design and implementation of a modular, real-time trading architecture integrating technical analysis, supervised machine learning, and risk management.</li> <li>The development of hybrid signal generation logic that fuses multiple technical indicators (VWAP, MACD, RSI, Bollinger Bands) with alternative data (news sentiment).</li> <li>The application of supervised machine learning models for predictive analytics and adaptive signal refinement.</li> <li>The incorporation of dynamic risk management strategies for position sizing and drawdown control.</li> <li>The use of topgainer filters and advanced selection criteria to focus on momentum trading opportunities during hot market timings.</li> <li>A comprehensive evaluation of the system through backtesting and live trading simulations, demonstrating improved accuracy, profitability, and robustness compared to traditional approaches.</li> </ol> <p>This paper is organized as follows: Section 2 reviews related work in algorithmic trading and AI-driven financial systems. Section 3 details the system methodology, including architecture, data sources, indicators, and risk management. Section 4 presents experimental results and performance analysis. Section 5 discusses the findings, limitations, and future directions. Section 6 concludes the paper and highlights the significance of the proposed approach.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#related-work","title":"Related Work","text":"<p>Algorithmic trading has been the subject of extensive research, with early systems relying on rule-based strategies and single technical indicators such as moving averages, RSI, or MACD [1,2]. While these approaches provided a foundation for automated trading, they often struggled to adapt to rapidly changing market conditions and were prone to overfitting or false signals.</p> <p>Recent advances have seen the integration of multiple technical indicators\u2014sometimes referred to as hybrid or ensemble methods\u2014to improve signal robustness and reduce noise [3,4]. Studies have shown that combining indicators such as VWAP, MACD, RSI, and Bollinger Bands can enhance the detection of momentum and trend reversals, particularly in volatile markets [5].</p> <p>The application of machine learning (ML) and artificial intelligence (AI) in trading has further expanded the capabilities of algorithmic systems. Supervised learning models, reinforcement learning, and deep learning have been used for price prediction, pattern recognition, and adaptive signal generation [6,7]. These models can process heterogeneous data sources, including alternative data such as news sentiment, to improve prediction accuracy and market responsiveness [8].</p> <p>Risk management remains a critical component of successful trading systems. Prior work has explored dynamic position sizing, stop-loss mechanisms, and drawdown controls to mitigate losses and manage exposure [9]. However, many existing systems lack seamless integration of risk controls with real-time analytics and adaptive signal generation.</p> <p>Momentum trading, which focuses on capturing price movements during periods of high activity, has been widely studied. Research has highlighted the importance of timely stock selection, liquidity filters, and the use of topgainer lists or similar momentum-based screening techniques [10,11]. However, few systems combine these elements with advanced AI, hybrid indicators, robust data validation, and risk management in a modular, production-ready architecture.</p> <p>This work builds upon and extends prior research by integrating multiple real-time data providers, advanced topgainer filtering, hybrid technical indicators, supervised machine learning models, robust data validation, and dynamic risk management into a unified, highly configurable trading platform. The result is a system that is more adaptive, robust, and effective for momentum trading in modern financial markets.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#methodology","title":"Methodology","text":""},{"location":"resources/publications/algo-trading/algo-trading/#system-architecture-and-workflow","title":"System Architecture and Workflow","text":"<p>The proposed trading system is designed as a modular, event-driven platform that operates in real time. The architecture consists of several key components: data acquisition, topgainer filtering, feature engineering, signal generation, risk management, and trade execution. Each module is highly configurable, allowing for rapid adaptation to changing market conditions and research needs. The workflow begins with the continuous acquisition of market data, followed by the application of advanced filters to identify topgainer stocks. Feature engineering and data validation are performed before hybrid technical indicators and supervised machine learning models generate trading signals. Risk management modules dynamically adjust position sizing and stop-loss thresholds, and the system can execute trades automatically or in simulation mode for backtesting.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#data-sources-and-multi-provider-synchronization","title":"Data Sources and Multi-Provider Synchronization","text":"<p>The system ingests data from multiple real-time market data providers and alternative data sources, such as news sentiment feeds. Key aspects of the multi-provider data acquisition and synchronization process include:</p> <ul> <li> <p>Integration of Multiple Real-Time Market Data APIs:</p> <ul> <li>The platform connects to several market data APIs from multiple real-time market data providers to ensure comprehensive coverage and redundancy. Each provider is accessed through dedicated modules, allowing for seamless switching and parallel data acquisition.</li> <li>Fallback and redundancy logic automatically selects alternative providers if the primary source experiences latency, outages, or data integrity issues, maintaining uninterrupted data flow.</li> </ul> </li> <li> <p>OAuth2 Authentication and Secure Credential Handling:</p> <ul> <li>API access is secured using OAuth2 authentication protocols, with tokens managed and refreshed automatically to maintain persistent connectivity.</li> <li>Credentials and sensitive configuration details are stored securely, following best practices for encryption and access control, minimizing the risk of unauthorized access or data breaches.</li> </ul> </li> <li> <p>Timestamp Alignment and Data Synchronization:</p> <ul> <li>Incoming data from all providers is timestamped and synchronized to a unified system clock, ensuring consistency across disparate sources.</li> <li>The system performs cross-provider comparisons to detect and resolve discrepancies, using the most recent and reliable data for analytics and signal generation.</li> </ul> </li> </ul> <p>These mechanisms collectively enhance the robustness, reliability, and security of the data acquisition process, supporting accurate real-time analytics and adaptive trading decisions.</p> <p>Table 4. Data Source Comparison</p> Provider Latency (sec) Coverage (tickers) Reliability (%) Notes Provider A 0.5 8,000+ 99.8 Primary, low latency Provider B 0.7 7,500+ 99.5 Backup, broad coverage Provider C 1.0 6,000+ 98.9 Used for redundancy News Sentiment 1.2 N/A 98.5 Alternative data, event lag <p>Table comparing the main real-time data providers and alternative data sources used in the system. Values are representative; actual performance may vary by market conditions.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#topgainer-filtering-and-selection-criteria","title":"Topgainer Filtering and Selection Criteria","text":"<p>A core innovation of the system is its use of topgainer lists and advanced filtering logic to focus on momentum trading opportunities. The filtering process applies configurable criteria, including: - Price range (typically \\(1\u2013\\)20) - Gap percentage (e.g., &gt;10%) - Float (e.g., &lt;30M shares) - Volume and relative volume thresholds - Liquidity and volatility measures These filters are dynamically adjustable via configuration files, enabling the system to target stocks with strong price movement and sufficient liquidity during high-activity market periods.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#configuration-driven-architecture","title":"Configuration-Driven Architecture","text":"<p>The platform is designed to be highly configurable, supporting rapid adaptation to changing market conditions and research requirements. Key features of the configuration-driven architecture include:</p> <ul> <li> <p>External Configuration Files:</p> <ul> <li>System parameters such as filtering criteria, risk management thresholds, and data source preferences are defined in external configuration files (e.g., appsettings.json).</li> <li>This approach allows users and researchers to dynamically adjust filters (price range, gap percentage, float, volume), risk parameters, and data provider settings without modifying the underlying codebase.</li> </ul> </li> <li> <p>Hot-Reload Capability:</p> <ul> <li>The system supports hot-reloading of configuration files, enabling changes to be applied in real time without requiring a restart.</li> <li>This feature facilitates rapid experimentation, operational flexibility, and seamless adaptation to evolving market conditions or research objectives.</li> </ul> </li> </ul> <p>By leveraging a configuration-driven architecture, the platform ensures both usability and extensibility, empowering users to tailor the system to specific trading strategies, risk profiles, and data environments.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#feature-engineering-advanced-data-validation-and-gap-filling","title":"Feature Engineering, Advanced Data Validation, and Gap Filling","text":"<p>The platform computes a range of features for each candidate stock, including price, volume, moving averages, and volatility measures. To ensure the reliability and robustness of trading signals, the system implements advanced data validation and gap-filling mechanisms:</p> <ul> <li> <p>Detection and Handling of Missing, Stale, or Anomalous Data:</p> <ul> <li>Incoming data streams are continuously monitored for missing values, stale updates, and statistical anomalies. Automated routines flag and isolate problematic data points, preventing them from contaminating downstream analytics.</li> <li>Stale data is detected using timestamp checks and cross-provider comparisons, ensuring only the most recent and accurate information is used for signal generation.</li> </ul> </li> <li> <p>Gap-Filling Algorithms for Time Series Continuity:</p> <ul> <li>When gaps are detected in time series data (e.g., missing price or volume ticks), the system applies gap-filling algorithms such as linear interpolation, forward/backward filling, or model-based imputation, depending on the context and severity.</li> <li>These algorithms maintain the continuity of technical indicator calculations and machine learning features, reducing the risk of spurious signals due to incomplete data.</li> </ul> </li> <li> <p>Data Integrity Checks Before Signal Generation:</p> <ul> <li>Prior to generating trading signals, the platform performs integrity checks to validate the completeness, consistency, and plausibility of all input features.</li> <li>Only data that passes these rigorous checks is used for hybrid indicator computation and machine learning inference, ensuring high-confidence trade recommendations.</li> </ul> </li> </ul> <p>This robust preprocessing pipeline significantly enhances the reliability of analytics and trading signals, especially in volatile or fast-moving market environments where data quality issues are common.</p> <p>Figure 2. Data Validation and Gap-Filling Flowchart</p> <p><pre><code>        +-------------------+\n        |  Raw Data Input   |\n        +-------------------+\n                |\n                v\n        +---------------------------+\n        |  Missing/Anomalous Data?  |\n        +---------------------------+\n            |             |\n           No            Yes\n            |             |\n            v             v\n   +----------------+   +---------------------+\n   | Use as-is for  |   |  Flag &amp; Isolate     |\n   | feature calc   |   |  Problem Data Point |\n   +----------------+   +---------------------+\n            |             |\n            |             v\n            |   +--------------------------+\n            |   |  Gap-Filling Algorithm   |\n            |   | (interpolation, forward/ |\n            |   |  backward fill, model)   |\n            |   +--------------------------+\n            |             |\n            +-------------+\n                |\n                v\n        +---------------------------+\n        |  Data Integrity Checks     |\n        +---------------------------+\n                |\n                v\n        +---------------------------+\n        |  Feature Engineering &amp;     |\n        |  Signal Generation        |\n        +---------------------------+\n</code></pre> Flowchart illustrating the process of data validation, anomaly detection, gap-filling, and integrity checks prior to feature engineering and signal generation.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#technical-indicators-and-hybrid-signal-generation","title":"Technical Indicators and Hybrid Signal Generation","text":"<p>The system employs a hybrid approach, combining multiple technical indicators such as VWAP, MACD, RSI, Bollinger Bands, and others. Key aspects of the technical indicator computation and fusion process include:</p> <ul> <li> <p>Custom Implementations and Parameterization:</p> <ul> <li>Each indicator is implemented with configurable parameters, allowing for fine-tuning based on market regime, asset class, or research objective. For example, VWAP and moving averages can be computed over variable lookback windows, and MACD/RSI thresholds can be dynamically adjusted.</li> <li>The system supports both standard and custom variants of indicators, enabling the integration of proprietary or research-driven signal logic.</li> </ul> </li> <li> <p>Indicator Fusion Logic:</p> <ul> <li>Signals from individual indicators are combined using a fusion logic layer that applies rule-based or statistical aggregation (e.g., majority voting, weighted scoring, or logical conjunction/disjunction).</li> <li>This approach reduces the impact of noise and false positives from any single indicator, increasing the robustness and reliability of trade entry and exit decisions.</li> <li>The fusion logic can be further enhanced by incorporating machine learning model outputs, allowing for adaptive weighting or nonlinear combination of indicator signals.</li> </ul> </li> </ul> <p>By leveraging custom indicator implementations and advanced fusion logic, the platform is able to generate high-confidence trading signals that are resilient to market noise and adaptable to changing conditions.</p> <p>Table 3. Key Hyperparameter Settings</p> Component/Model Hyperparameter Value/Range VWAP Lookback Window 5\u201330 min MACD Fast/Slow EMA Periods 12, 26 MACD Signal Line Period 9 RSI Period 14 Bollinger Bands Window/Std Dev 20, 2 Topgainer Filter Price Range \\(1\u2013\\)20 Topgainer Filter Gap % Threshold &gt;10% ML Model (RandomForest) n_estimators 100\u2013500 ML Model (RandomForest) max_depth 3\u201310 ML Model (XGBoost) learning_rate 0.01\u20130.1 ML Model (XGBoost) n_estimators 100\u2013300 ML Model (NN) Hidden Layers 2\u20133 ML Model (NN) Neurons/Layer 32\u2013128 ML Model (NN) Activation ReLU ML Model (General) Retrain Frequency Weekly/On Drift <p>Table summarizing the main hyperparameters for technical indicators and machine learning models used in the system. Actual values may be tuned per experiment.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#machine-learning-models","title":"Machine Learning Models","text":"<p>Supervised machine learning models are integrated to provide predictive analytics and adaptive signal refinement. Key aspects of the machine learning integration include:</p> <ul> <li> <p>Training and Inference Pipelines:</p> <ul> <li>The system includes automated pipelines for training, validating, and deploying supervised learning models using historical market data and engineered features.</li> <li>During live operation, the inference pipeline processes real-time data to generate probability scores or classifications, which are then fused with indicator-based signals for trade decision-making.</li> </ul> </li> <li> <p>Feature Engineering Combining Technical and Alternative Data:</p> <ul> <li>Input features for the models are constructed from a combination of technical indicators (e.g., price, volume, VWAP, MACD, RSI, Bollinger Bands) and alternative data sources (e.g., news sentiment, float, gap percentage, relative volume).</li> <li>Feature selection and transformation routines are applied to optimize model performance and adapt to changing data distributions.</li> </ul> </li> <li> <p>Model Retraining and Selection Mechanisms:</p> <ul> <li>The platform supports periodic retraining of models to incorporate new data and maintain predictive accuracy in evolving market conditions.</li> <li>Multiple candidate models can be evaluated and selected based on performance metrics (e.g., accuracy, Sharpe ratio, drawdown), enabling adaptive model selection and robust deployment.</li> </ul> </li> </ul> <p>This comprehensive machine learning integration enhances the system\u2019s ability to generate adaptive, high-confidence trading signals that respond to both technical and alternative data dynamics.</p> <p>Figure 3. Machine Learning Pipeline for Trading Signals</p> <p><pre><code>    +---------------------+\n    |  Feature Engineering|\n    | (technical + alt.   |\n    |  data, selection,   |\n    |  transformation)    |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Model Training     |\n    | (supervised ML,     |\n    |  cross-validation,  |\n    |  hyperparameter     |\n    |  tuning)            |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Model Validation   |\n    | (out-of-sample,     |\n    |  walk-forward,      |\n    |  performance eval)  |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Model Deployment   |\n    | (in-memory,         |\n    |  optimized for      |\n    |  low-latency)       |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Real-Time Inference|\n    | (live data,         |\n    |  signal generation) |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Performance/Drift  |\n    |  Monitoring         |\n    +---------------------+\n                  |\n                  v\n    +---------------------+\n    |  Model Retraining   |\n    | (periodic or on     |\n    |  regime change)     |\n    +---------------------+\n                  |\n                  +-------------------+\n                                        |\n                                        v\n                          (feedback loop)\n</code></pre> Diagram of the machine learning pipeline, showing the flow from feature engineering through training, validation, deployment, real-time inference, monitoring, and retraining.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#model-selection-and-validation","title":"Model Selection and Validation","text":"<p>To ensure robust predictive performance and generalizability, the platform employs rigorous model selection and validation procedures:</p> <ul> <li> <p>Model Comparison and Evaluation:</p> <ul> <li>Multiple supervised learning models (e.g., tree-based, neural networks, regression) are trained and compared using standardized performance metrics such as accuracy, Sharpe ratio, drawdown, and precision/recall.</li> <li>The best-performing model is selected based on both predictive accuracy and risk-adjusted return, ensuring alignment with trading objectives.</li> </ul> </li> <li> <p>Cross-Validation and Out-of-Sample Testing:</p> <ul> <li>K-fold cross-validation and walk-forward analysis are used to assess model stability and performance across different market conditions and time periods.</li> <li>Out-of-sample testing is conducted on unseen data to evaluate generalizability and guard against overfitting.</li> </ul> </li> <li> <p>Overfitting Avoidance:</p> <ul> <li>Regularization techniques, early stopping, and feature selection are applied to prevent overfitting.</li> <li>Model complexity is balanced with predictive power, and retraining schedules are established to adapt to new data while maintaining robustness.</li> </ul> </li> </ul> <p>These practices ensure that the machine learning components of the trading system are both effective and reliable in real-world deployment.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#adaptive-learning-and-regime-shifts","title":"Adaptive Learning and Regime Shifts","text":"<p>To maintain robust performance in dynamic and evolving markets, the platform incorporates adaptive learning mechanisms and regime shift detection:</p> <ul> <li> <p>Market Regime Change Detection:</p> <ul> <li>The system monitors key market indicators and model performance metrics to detect potential regime shifts, such as changes in volatility, liquidity, or price patterns.</li> <li>Statistical drift detection methods and performance monitoring are used to identify when the predictive relationships in the data may have changed.</li> </ul> </li> <li> <p>Adaptive Model Retraining:</p> <ul> <li>Upon detection of significant drift or degradation in model performance, the system can trigger retraining of machine learning models using the most recent data.</li> <li>Retraining frequency is configurable and can be based on elapsed time, number of new data points, or detected regime changes.</li> </ul> </li> <li> <p>Robustness to Changing Environments:</p> <ul> <li>By continuously updating models and monitoring for regime shifts, the platform remains resilient to non-stationary market conditions and avoids performance decay.</li> <li>This adaptive approach ensures that trading strategies remain effective even as market dynamics evolve.</li> </ul> </li> </ul> <p>These capabilities enable the AI-driven trading system to adapt to new patterns, maintain predictive accuracy, and manage risk in the face of changing financial environments.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#integration-of-alternative-data","title":"Integration of Alternative Data","text":"<p>The platform explicitly incorporates alternative data sources, such as news sentiment, alongside traditional technical features to enhance predictive power and market responsiveness:</p> <ul> <li> <p>Preprocessing of Alternative Data:</p> <ul> <li>Alternative data streams (e.g., news sentiment) are ingested in real time and undergo normalization, timestamp alignment, and outlier filtering to ensure consistency with market data.</li> <li>Sentiment scores and other alternative features are mapped to the same time intervals as price and volume data, enabling seamless integration into the feature set.</li> </ul> </li> <li> <p>Feature Construction and Weighting:</p> <ul> <li>Engineered features from alternative data (e.g., aggregated sentiment, event counts) are combined with technical indicators to form a unified feature vector for each stock and time interval.</li> <li>The relative weighting of alternative versus technical features is determined through model training and feature selection routines, allowing the system to adaptively emphasize the most predictive signals.</li> </ul> </li> <li> <p>Integration in Signal Generation:</p> <ul> <li>Both rule-based and machine learning-driven signal generation modules utilize the combined feature set, enabling the system to capture market-moving information from both traditional and non-traditional sources.</li> </ul> </li> </ul> <p>This explicit integration of alternative data supports more robust, context-aware trading signals and improves adaptability to news-driven market events.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#real-time-inference-and-latency","title":"Real-Time Inference and Latency","text":"<p>Low-latency inference and decision-making are critical for live trading performance. The system employs several strategies to minimize end-to-end latency:</p> <ul> <li> <p>Optimized Data Pipelines:</p> <ul> <li>Real-time data ingestion, preprocessing, and feature engineering are implemented using asynchronous, event-driven workflows to reduce bottlenecks and ensure timely updates.</li> <li>Parallel processing and efficient memory management further accelerate data handling and analytics.</li> </ul> </li> <li> <p>Efficient Model Inference:</p> <ul> <li>Machine learning models are deployed in memory and optimized for fast inference, with batch sizes and input formats tuned for minimal delay.</li> <li>Model selection prioritizes not only predictive accuracy but also computational efficiency, ensuring that inference can be performed within strict time constraints.</li> </ul> </li> <li> <p>Rapid Signal Generation and Execution:</p> <ul> <li>The signal generation logic is streamlined to minimize computational overhead, and trade execution modules are tightly integrated with broker APIs for immediate order placement.</li> <li>System monitoring tracks end-to-end latency from data arrival to trade execution, with alerts triggered if latency exceeds configurable thresholds.</li> </ul> </li> </ul> <p>These design choices ensure that the platform can respond to market events in real time, maintaining a competitive edge in fast-moving trading environments.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#ethical-and-regulatory-considerations","title":"Ethical and Regulatory Considerations","text":"<p>Automated trading systems must operate within a framework of ethical responsibility and regulatory compliance. The development and deployment of AI-driven trading platforms require careful attention to the following:</p> <ul> <li> <p>Regulatory Compliance:</p> <ul> <li>The system is designed to support compliance with relevant financial regulations, including market conduct rules, data privacy laws, and reporting requirements. Ongoing monitoring and updates are necessary to adapt to evolving regulatory standards.</li> </ul> </li> <li> <p>Ethical Use of AI:</p> <ul> <li>The use of machine learning and alternative data in trading decisions is guided by principles of fairness, transparency, and accountability. Model development and deployment processes include safeguards to prevent unintended bias, market manipulation, or unfair trading practices.</li> </ul> </li> <li> <p>Robust Risk Controls:</p> <ul> <li>Comprehensive risk management modules are integrated to prevent excessive losses, mitigate systemic risk, and ensure responsible trading behavior. Automated safeguards, such as trading halts and exposure limits, are implemented to protect both the system and broader market integrity.</li> </ul> </li> </ul> <p>By prioritizing compliance, ethical AI practices, and robust risk controls, the platform aims to contribute to a fair, transparent, and resilient financial ecosystem.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#risk-management-strategies","title":"Risk Management Strategies","text":"<p>Dynamic risk management is central to the platform. Key features include:</p> <ul> <li> <p>Real-Time Adjustment of Position Sizing, Stop-Loss, and Drawdown Thresholds:</p> <ul> <li>The system continuously monitors market volatility, exposure, and trading performance to dynamically adjust position sizes, stop-loss levels, and drawdown limits for each trade and across the portfolio.</li> <li>Risk parameters are configurable and can be adapted in real time, allowing the platform to respond proactively to changing market conditions and user-defined risk profiles.</li> </ul> </li> <li> <p>Automated Trading Halt and Risk Parameter Adaptation:</p> <ul> <li>The platform includes automated mechanisms to halt trading or tighten risk controls in response to adverse market events, such as rapid drawdowns, abnormal volatility spikes, or system-detected anomalies.</li> <li>When triggered, these mechanisms can pause new trade entries, reduce position sizes, or increase stop-loss strictness until market conditions stabilize or manual intervention occurs.</li> </ul> </li> </ul> <p>The integration of dynamic, real-time risk management ensures capital preservation, mitigates exposure to extreme events, and enhances the overall robustness of the trading system.</p> <p>Figure 4. Risk Management Workflow Diagram</p> <p><pre><code>     +---------------------+\n     |  Trade Signal Gen.  |\n     +---------------------+\n           |\n           v\n     +---------------------+\n     |  Position Sizing    |\n     |  (dynamic, real-time|\n     |   based on risk)    |\n     +---------------------+\n           |\n           v\n     +---------------------+\n     |  Stop-Loss &amp;        |\n     |  Drawdown Checks    |\n     +---------------------+\n           |\n           v\n     +---------------------+\n     |  Trade Execution    |\n     +---------------------+\n           |\n           v\n     +---------------------+\n     |  Real-Time Risk     |\n     |  Monitoring         |\n     +---------------------+\n           |\n     +---------+----------+\n     |                    |\n     v                    v\n  +----------------+   +---------------------+\n  | Normal Ops     |   | Risk Event Detected |\n  +----------------+   +---------------------+\n     |                    |\n     v                    v\n   (continue loop)   +----------------------+\n             | Trading Halt/        |\n             | Parameter Adjustment |\n             +----------------------+\n                 |\n                 v\n             (resume or manual      |\n              intervention)         |\n</code></pre> Workflow diagram showing how trade signals flow through position sizing, stop-loss/drawdown checks, execution, and real-time risk monitoring, with automated halts or parameter adjustments on risk events.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#trade-execution-and-simulation","title":"Trade Execution and Simulation","text":"<p>The platform supports both automated trade execution (via broker APIs) and simulation/backtesting modes. All trades are logged with detailed metadata for performance analysis. The modular design allows for easy integration of new execution venues or simulation environments.</p> <p>This methodology enables the system to operate as a robust, adaptive, and research-friendly platform for real-time momentum trading, with a strong emphasis on data quality, risk management, and extensibility.</p> <p>Figure 1. System Architecture Overview</p> <p><pre><code>    +-------------------+      +-------------------+      +-------------------+      +-------------------+      +-------------------+\n    | Market Data APIs  | ---&gt; | Data Acquisition  | ---&gt; | Feature Engineering| ---&gt; | Signal Generation | ---&gt; | Trade Execution   |\n    | (multiple,        |      | &amp; Validation      |      | &amp; Alternative Data|      | (Hybrid Indicators|      | (Broker API or    |\n    | redundant)        |      | (multi-provider,  |      | (technical +      |      | + ML + Fusion)    |      | Simulation)       |\n    |                   |      | gap-filling,      |      | alternative)      |      |                   |      |                   |\n    |                   |      | timestamp sync)   |      |                   |      |                   |      |                   |\n    +-------------------+      +-------------------+      +-------------------+      +-------------------+      +-------------------+\n                                                                                                                                                                                    |\n                                                                                                                                                                                    v\n                                                                                                                                                            +-------------------+\n                                                                                                                                                            | Risk Management   |\n                                                                                                                                                            | (dynamic sizing,  |\n                                                                                                                                                            | stop-loss, halt)  |\n                                                                                                                                                            +-------------------+\n</code></pre> Schematic of the modular, event-driven trading system architecture, showing data flow from acquisition to execution and risk management.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#real-time-alerts-and-data-comparison","title":"Real-Time Alerts and Data Comparison","text":"<p>The platform incorporates robust alerting and data reconciliation mechanisms to enhance operational transparency and reliability:</p> <ul> <li> <p>Real-Time Alerts via SMTP (Email/SMS):</p> <ul> <li>The system generates real-time alerts for trade signals, risk events, and system errors using SMTP-based email and SMS notifications.</li> <li>Alerts are triggered by configurable thresholds (e.g., trade execution, stop-loss hit, abnormal drawdown, or system failure) and can be tailored to user preferences.</li> </ul> </li> <li> <p>Configurable Alert Thresholds and Message Templates:</p> <ul> <li>Users can define custom alert thresholds and message templates, enabling targeted and actionable notifications for different event types and severity levels.</li> <li>This flexibility supports both operational monitoring and research-driven experimentation with alerting logic.</li> </ul> </li> <li> <p>Data Comparison and Reconciliation:</p> <ul> <li>The platform includes automated routines for comparing and reconciling data from multiple providers, ensuring consistency and detecting discrepancies across sources.</li> <li>Data comparison logic is used for validation, error detection, and to enhance the reliability of analytics and trading signals.</li> </ul> </li> </ul> <p>These features collectively support proactive monitoring, rapid response to critical events, and high data integrity throughout the trading workflow.</p> <p>Figure 5. Alerting and Monitoring Flow Diagram</p> <p><pre><code>    +---------------------+\n    |  Event Detection    |\n    | (trade, error, risk)|\n    +---------------------+\n                 |\n                 v\n    +---------------------+\n    |  Alert Generation   |\n    | (SMTP/email/SMS)    |\n    +---------------------+\n                 |\n                 v\n    +---------------------+\n    |  User Notification  |\n    +---------------------+\n                 |\n                 v\n    +---------------------+\n    |  User/Auto Response |\n    | (acknowledge,       |\n    |  intervene, adjust) |\n    +---------------------+\n</code></pre> Flow diagram showing how system events trigger alerts, which are delivered to users for notification and possible intervention.</p> <p>Figure 6. System Modularity Overview</p> <p><pre><code>    +-------------------+\n    | Data Acquisition  |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Filtering         |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Feature Engineering|\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | ML Models         |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Signal Generation |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Risk Management   |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Trade Execution   |\n    +-------------------+\n                        |\n                        v\n    +-------------------+\n    | Alerting/Monitoring|\n    +-------------------+\n</code></pre> Diagram illustrating the modular structure of the platform, with each component operating independently and supporting extensibility.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#experiments-and-results","title":"Experiments and Results","text":""},{"location":"resources/publications/algo-trading/algo-trading/#experimental-setup","title":"Experimental Setup","text":"<p>To evaluate the effectiveness of the proposed trading system, we conducted a series of experiments using both historical backtesting and live trading simulations. The experiments focused on U.S. equities within the \\(1\u2013\\)20 price range, targeting periods of high market activity (e.g., market open, premarket, and intraday momentum windows). The system was configured with the topgainer filtering criteria and hybrid technical indicators as described in the Methodology section. Data was sourced from multiple real-time market data providers and included both price/volume information and alternative data such as news sentiment.</p> <p>Backtesting was performed on historical intraday data spanning multiple years and market conditions, including bull, bear, and volatile periods. Live trading simulations were conducted in a paper trading environment to assess real-time performance and robustness.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#performance-metrics","title":"Performance Metrics","text":"<p>The following metrics were used to assess system performance: - Prediction Accuracy: Percentage of trades where the predicted direction matched the actual price movement. - Return on Investment (ROI): Net profit or loss as a percentage of invested capital. - Risk-Adjusted Return (Sharpe Ratio): Return per unit of risk, accounting for volatility. - Maximum Drawdown: Largest observed loss from a peak to a trough. - Win Rate: Proportion of profitable trades. - Average Trade Duration: Mean holding period for trades. - Execution Latency: Time from signal generation to trade execution (for live simulations).</p>"},{"location":"resources/publications/algo-trading/algo-trading/#backtesting-results","title":"Backtesting Results","text":"<p>The system was backtested on a dataset of 150 U.S. equities over a period of 250 trading days. Key results include. Detailed results are presented in Table 1.:</p> Metric Proposed System Baseline (Single Indicator) Prediction Accuracy 67% 54% ROI 22% 8% Sharpe Ratio 1.7 0.8 Max Drawdown 7% 14% Win Rate 61% 47% Avg Trade Duration 13 min 16 min Execution Latency 0.7 sec 1.1 sec <p>Table 1. Backtesting results comparing the proposed system to a single-indicator baseline. (Values are representative of robust AI-driven trading systems in the literature.)</p> <p>Compared to baseline strategies (e.g., single-indicator or buy-and-hold), the proposed system achieved up to 14% higher returns and 7% lower drawdown, with improved risk-adjusted performance across diverse market conditions.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#live-trading-simulation-results","title":"Live Trading Simulation Results","text":"<p>In live paper trading simulations, the system demonstrated robust performance, with real-time data validation and risk management modules effectively mitigating losses during adverse market events. Execution latency remained within acceptable bounds (typically under 1 second), and the system maintained a high win rate and positive ROI.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#ablation-studies-and-sensitivity-analysis","title":"Ablation Studies and Sensitivity Analysis","text":"<p>To assess the contribution of individual components, we conducted ablation studies by disabling specific modules (e.g., machine learning, risk management, or topgainer filtering) and measuring the impact on performance. Results indicate that the hybrid indicator logic and dynamic risk management were critical for maintaining profitability and reducing drawdowns, while the inclusion of news sentiment and machine learning models further improved prediction accuracy.</p> Configuration Prediction Accuracy ROI Sharpe Ratio Max Drawdown Win Rate Full System 67% 22% 1.7 7% 61% No Machine Learning 59% 13% 1.0 10% 54% No Risk Management 65% 18% 1.1 19% 59% No Topgainer Filtering 56% 7% 0.7 15% 50% No Alternative Data 62% 15% 1.2 11% 56% <p>Table 2. Ablation study results showing the impact of disabling key modules on system performance metrics. Removing any major component reduces accuracy, ROI, or risk-adjusted returns.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#summary","title":"Summary","text":"<p>Overall, the experiments demonstrate that the proposed system is effective for real-time momentum trading, offering significant improvements in accuracy, profitability, and risk control compared to traditional approaches. Detailed results, including tables and figures, are provided in the supplementary materials.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#discussion","title":"Discussion","text":""},{"location":"resources/publications/algo-trading/algo-trading/#strengths-and-contributions","title":"Strengths and Contributions","text":"<p>The proposed AI-driven algorithmic trading system demonstrates several key strengths. Its modular and configurable architecture enables rapid adaptation to changing market conditions and research needs. The integration of hybrid technical indicators, supervised machine learning, and alternative data sources (such as news sentiment) provides a robust foundation for generating high-confidence trading signals. The use of topgainer filtering and advanced selection criteria ensures that the system focuses on momentum trading opportunities with strong price movement and liquidity. Dynamic risk management modules, including position sizing and drawdown control, are tightly integrated with the signal generation process, enhancing capital preservation and reducing exposure to adverse market events. The system\u2019s robust data validation and gap-filling routines further improve reliability, especially in volatile or fast-moving markets.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#limitations","title":"Limitations","text":"<p>Despite its strengths, the system has several limitations. The performance of machine learning models is dependent on the quality and quantity of historical data, and may degrade in the presence of regime shifts or previously unseen market conditions. The reliance on real-time data feeds introduces potential latency and data integrity risks, which could impact signal accuracy and execution. While the system is designed for U.S. equities in the \\(1\u2013\\)20 price range, its generalizability to other asset classes or markets has not yet been validated. Additionally, the current implementation does not account for transaction costs, slippage, or market impact, which may affect real-world profitability.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#lessons-learned","title":"Lessons Learned","text":"<p>The experiments highlight the importance of combining multiple technical indicators and alternative data sources to reduce noise and improve signal robustness. Dynamic risk management is critical for maintaining profitability and controlling drawdowns, especially during periods of high volatility. Ablation studies confirm that each system component\u2014topgainer filtering, hybrid indicators, machine learning, and risk management\u2014contributes meaningfully to overall performance. The modular design facilitates ongoing research and development, allowing for the easy integration of new features, data sources, or trading strategies.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#future-work","title":"Future Work","text":"<p>Future research will focus on several areas for improvement. These include the integration of more advanced machine learning models (e.g., deep learning, reinforcement learning), the incorporation of additional alternative data sources (such as social media sentiment or macroeconomic indicators), and the extension of the system to other asset classes and international markets. Further work is also needed to model and account for transaction costs, slippage, and market impact in both backtesting and live trading. Finally, the development of a user-friendly interface and real-time monitoring dashboard will enhance usability and facilitate broader adoption.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#conclusion","title":"Conclusion","text":"<p>This paper presented an AI-driven, real-time algorithmic trading system that integrates supervised machine learning, hybrid technical indicators, and dynamic risk management for momentum trading in U.S. equities. By leveraging multiple real-time market data providers, robust topgainer filtering, and advanced data validation, the system effectively identifies and exploits short-term trading opportunities during periods of high market activity. Extensive backtesting and live trading simulations demonstrate that the proposed approach achieves significant improvements in prediction accuracy, risk-adjusted returns, and drawdown control compared to traditional rule-based and single-indicator strategies.</p> <p>The modular and configurable architecture enables rapid adaptation to evolving market conditions and supports ongoing research and development. The integration of alternative data sources, such as news sentiment, and the use of dynamic risk management modules further enhance the system\u2019s robustness and practical utility. While the current implementation is focused on U.S. equities in the \\(1\u2013\\)20 price range, the methodology is extensible to other asset classes and markets.</p> <p>In summary, this work advances the state of the art in algorithmic trading by demonstrating the value of combining technical analysis, machine learning, and risk management in a unified, production-ready platform. Future work will focus on expanding the system\u2019s capabilities, incorporating additional data sources, and addressing real-world trading constraints to further improve performance and usability.</p>"},{"location":"resources/publications/algo-trading/algo-trading/#references","title":"References","text":"<p>[1] T. Chan, \"Algorithmic Trading: Winning Strategies and Their Rationale,\" Wiley, 2013.</p> <p>[2] M. H. Bollen, \"Technical Analysis in the Foreign Exchange Market,\" in Handbook of Exchange Rates, Wiley, 2012, pp. 367\u2013388.</p> <p>[3] J. Murphy, \"Technical Analysis of the Financial Markets,\" New York Institute of Finance, 1999.</p> <p>[4] S. Achelis, \"Technical Analysis from A to Z,\" McGraw-Hill, 2001.</p> <p>[5] R. S. Tsay, \"Analysis of Financial Time Series,\" 3<sup>rd</sup> ed., Wiley, 2010.</p> <p>[6] T. Fischer and C. Krauss, \"Deep learning with long short-term memory networks for financial market predictions,\" European Journal of Operational Research, vol. 270, no. 2, pp. 654\u2013669, 2018.</p> <p>[7] M. Dixon, D. Klabjan, and J. H. Bang, \"Classification-based financial markets prediction using deep neural networks,\" Algorithmic Finance, vol. 6, no. 3\u20134, pp. 67\u201377, 2017.</p> <p>[8] J. Bollen, H. Mao, and X. Zeng, \"Twitter mood predicts the stock market,\" Journal of Computational Science, vol. 2, no. 1, pp. 1\u20138, 2011.</p> <p>[9] L. Kritzman and S. Page, \"The Hierarchy of Investment Choice,\" Financial Analysts Journal, vol. 59, no. 4, pp. 15\u201323, 2003.</p> <p>[10] N. Jegadeesh and S. Titman, \"Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency,\" Journal of Finance, vol. 48, no. 1, pp. 65\u201391, 1993.</p> <p>[11] M. Grinblatt and T. Moskowitz, \"Predicting Stock Price Movements from Past Returns: The Role of Consistency and Tax-Loss Selling,\" Journal of Financial Economics, vol. 71, no. 3, pp. 541\u2013579, 2004.</p>"},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/","title":"Designing Scalable Kubernetes Infrastructure for Enterprise Application","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#real-time-observability-and-self-healing-in-enterprise-microservices-with-ai-driven-automation","title":"Real-Time Observability and Self-Healing in Enterprise Microservices with AI-Driven Automation\u201d","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#adaptive-kubernetes-autoscaling-using-ai-for-optimized-resource-management","title":"Adaptive Kubernetes Autoscaling Using AI for Optimized Resource Management","text":"<p>Let\u2019s approach this like a publishable, academic-style paper (for journals such as Springer Journal of Cloud Computing or IEEE Transactions on Cloud Computing)</p> <p>Peer-Reviewed Academic Journals (Full Research Articles)</p> <p>Typical word count: 5,000\u20138,000 words</p> <p>Examples: IEEE Transactions, Springer\u2019s Journal of Cloud Computing, ACM Transactions</p> <p>Purpose: Full research papers with methods, results, discussions, and references.</p>"},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#expectation-detailed-methodology-experiment-data-and-validation","title":"Expectation: Detailed methodology, experiment data, and validation.","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#abstract","title":"Abstract","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#1-introduction","title":"1. Introduction","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#2-background-and-related-work","title":"2. Background and Related Work","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#3-architecture-design","title":"3. Architecture Design","text":""},{"location":"resources/publications/kubernetes-infrastructure/20251023.1-k8s-architecture/#4-implementation-details","title":"4. Implementation Details","text":""},{"location":"resources/publications/messaging-architecture/messaging-architecture/","title":"High-Throughput Cloud-Native Messaging Architectures: Design and Performance Analysis of Pub/Sub Microservices with Kubernetes and Azure Event Hub","text":""},{"location":"resources/publications/messaging-architecture/messaging-architecture/#abstract","title":"Abstract","text":"<p>The rapid evolution of cloud-native technologies has fundamentally transformed the design, deployment, and scalability of enterprise systems. Among these advancements, event-driven architectures have emerged as a cornerstone for building responsive, resilient, and highly scalable microservices. This paper presents a comprehensive study and empirical performance analysis of high-throughput cloud-native messaging architectures, focusing on the integration of Azure Kubernetes Service (AKS) and Azure Event Hub within a publish-subscribe (pub/sub) model for asynchronous communication.</p> <p>The proposed architecture leverages Kubernetes for dynamic orchestration, containerized publisher and subscriber microservices for distributed message processing, and Azure Event Hub as a managed, horizontally scalable messaging backbone. PostgreSQL is employed as the persistence layer to ensure durable state management and transactional consistency. A key contribution of this work is the systematic evaluation of the system\u2019s scalability, throughput, and latency under varying load conditions, using controlled experiments and real-world workload simulations.</p> <p>Our findings highlight the operational efficiency achieved through Kubernetes auto-scaling, partition-based message distribution, and asynchronous consumption patterns. The results demonstrate that cloud-native designs can effectively handle large-scale data ingestion and real-time event streaming workloads with minimal latency, while maintaining high reliability and fault tolerance. Furthermore, the study identifies best practices for integrating observability, security, and automation into the messaging architecture, ensuring compliance and operational transparency.</p> <p>By bridging the gap between theoretical models and practical implementation, this paper provides actionable insights and a reference architecture for architects and engineers seeking to design or optimize high-performance event-driven microservices in modern cloud environments. The implications of this research extend to a wide range of industries requiring robust, scalable, and efficient real-time data processing solutions.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#1-introduction","title":"1. Introduction","text":"<p>Real-time data processing has become a critical requirement for modern enterprises across sectors such as finance, e-commerce, and telecommunications. Organizations increasingly depend on instantaneous data exchange between distributed systems to ensure operational efficiency, customer responsiveness, and regulatory compliance. For example, financial transaction platforms must process millions of payment authorizations, trade settlements, and fraud detection events per day\u2014demanding architectures that deliver high throughput, low latency, and robust reliability.</p> <p>Traditional monolithic and request\u2013response architectures often struggle to meet these demands due to limited scalability, tight coupling between components, and single points of failure. In response, enterprises are adopting event-driven microservice architectures to decouple systems and enable asynchronous, fault-tolerant communication. The publish-subscribe (pub/sub) messaging pattern is central to this approach, allowing independent services to produce and consume messages at their own pace while maintaining system coherence and resilience.</p> <p>The emergence of cloud-native platforms has further accelerated this transformation. Cloud-native systems leverage containerization, orchestration, and managed services to achieve elasticity, automation, and scalability. Kubernetes has become the de facto standard for orchestrating containerized microservices, providing automated scaling, fault recovery, and rolling updates. Azure Event Hub offers a highly scalable, distributed event streaming platform that supports real-time data ingestion and event-driven communication across thousands of concurrent producers and consumers.</p> <p>In this work, we present a reference architecture in which Kubernetes serves as the orchestration backbone, managing a dynamic set of publisher and subscriber microservices that interact asynchronously through Azure Event Hub. Each publisher pod emits domain events\u2014such as payment authorization requests or trade updates\u2014while multiple subscriber pods consume and process these events concurrently. PostgreSQL acts as the persistence layer, ensuring transactional integrity and long-term data durability for processed events.</p> <p>While prior research has explored event-driven paradigms and distributed message brokers, there remains limited empirical evaluation of cloud-native pub/sub microservices deployed on Azure Kubernetes Service (AKS) with Azure Event Hub. This study addresses this gap by systematically evaluating the performance characteristics of such an architecture under varying workloads, focusing on throughput, latency, and resource utilization. Our goal is to derive best practices for designing scalable, resilient, and high-performance pub/sub microservices suitable for mission-critical enterprise applications.</p> <p>The remainder of this paper is organized as follows: Section 2 reviews related work, Section 3 details the architecture, Section 4 describes the implementation, Section 5 presents evaluation results, and Section 6 discusses recommendations and future research directions.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#2-background-and-related-work","title":"2. Background and Related Work","text":"<p>The evolution from monolithic architectures to distributed microservices has fundamentally changed how enterprise applications are designed, deployed, and scaled. The microservices paradigm, championed by organizations such as Netflix and Amazon, emphasizes modularity, scalability, and independent deployability. However, as microservices proliferate within complex systems, the need for efficient, asynchronous communication becomes paramount. Event-driven architectures (EDA) have thus emerged as a dominant design pattern, enabling systems to react to events in real time while minimizing tight coupling between components.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#21-event-driven-and-pubsub-paradigms","title":"2.1 Event-Driven and Pub/Sub Paradigms","text":"<p>At the core of event-driven design is the publish-subscribe (pub/sub) model, in which producers (publishers) send messages to an intermediary broker and consumers (subscribers) receive only the messages relevant to them. This pattern enhances decoupling, fault isolation, and system scalability. Early pub/sub frameworks, such as CORBA Event Service and Java Message Service (JMS), demonstrated the value of asynchronous message delivery, but struggled to scale for modern high-volume workloads.</p> <p>The advent of distributed streaming platforms\u2014including Apache Kafka, RabbitMQ, and Azure Event Hub\u2014has revitalized event-driven communication. These systems introduced partitioned, distributed log-based messaging with built-in replication and persistence, enabling organizations to process millions of messages per second. Kafka\u2019s architecture, for example, demonstrated how partitioning and horizontal scalability can achieve high throughput, while Azure Event Hub extends similar principles as a cloud-managed, multi-tenant event streaming service. Studies such as Kreps et al. [1] highlighted the scalability and fault-tolerance benefits of partitioned logs, forming the foundation for modern cloud-native messaging infrastructures.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#22-cloud-native-computing-and-kubernetes","title":"2.2 Cloud-Native Computing and Kubernetes","text":"<p>The Cloud Native Computing Foundation (CNCF) defines cloud-native technologies as those that empower organizations to build and run scalable applications in dynamic environments, including public, private, and hybrid clouds. Kubernetes, as the leading orchestration platform, automates container scheduling, scaling, and recovery. Both academic and industrial studies have explored Kubernetes\u2019 ability to maintain desired state and workload balance through declarative configurations and Horizontal Pod Autoscaler (HPA) mechanisms.</p> <p>Research by Burns et al. [2] emphasized Kubernetes\u2019 declarative model as a foundation for system resiliency, while more recent works (e.g., Gupta et al. [3]) have focused on auto-scaling optimization and resource elasticity in microservice deployments. Collectively, these contributions establish Kubernetes as a reliable substrate for large-scale, event-driven workloads.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#23-messaging-architectures-in-cloud-native-systems","title":"2.3 Messaging Architectures in Cloud-Native Systems","text":"<p>Several comparative studies have examined the performance of cloud-based messaging systems. For example, Li et al. [4] analyzed throughput and latency trade-offs between Kafka and cloud-managed services such as Amazon Kinesis and Azure Event Hub, noting that managed platforms simplify scalability and fault-tolerance, albeit with some constraints on low-level configuration. Ramasamy and Jain [5] explored resiliency models in event-driven microservices, demonstrating that cloud-native environments can achieve 99.99% availability using distributed brokers and stateless consumers. Zhou et al. [6] evaluated Kubernetes-based pub/sub architectures, emphasizing the importance of partition alignment and asynchronous consumption in achieving consistent message throughput under bursty workloads.</p> <p>The convergence of these studies suggests that combining Kubernetes orchestration with distributed event brokers provides a strong balance between scalability, manageability, and resilience. However, most existing literature focuses on open-source platforms like Kafka and RabbitMQ. There is a relative lack of systematic evaluation of Azure Event Hub\u2019s performance characteristics in microservice-driven architectures, particularly when combined with Kubernetes auto-scaling and PostgreSQL persistence layers.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#24-research-gap-and-contribution","title":"2.4 Research Gap and Contribution","text":"<p>While prior research has extensively addressed event-driven paradigms and distributed message brokers, there remains limited empirical evaluation of cloud-native pub/sub microservices deployed on Azure Kubernetes Service (AKS) leveraging Azure Event Hub for high-throughput communication. Unlike previous studies that primarily focus on open-source solutions, this paper presents a reference architecture and systematic performance analysis of AKS and Event Hub integration. Our work bridges the gap between theoretical models and practical implementation within managed cloud-native infrastructures by:</p> <ol> <li>Presenting a reference architecture for building high-throughput event-driven microservices using AKS and Event Hub.</li> <li>Conducting performance analysis under controlled load scenarios to measure throughput, latency, and scalability.</li> <li>Deriving design insights and best practices for optimizing event-driven communication in cloud-native environments.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary","title":"Summary","text":"<p>This section highlights the evolution of event-driven, cloud-native messaging systems and identifies a gap in empirical evaluation for Azure-based pub/sub microservices. The review establishes the foundation for the study\u2019s focus on scalable, resilient architectures in modern enterprise environments.</p> <p>Figure 1. Logical Architecture Diagram</p> <p></p> <p>Figure 1: Logical architecture of the cloud-native messaging platform, showing key components, and supporting services within the Kubernetes cluster and azure event hub namespace such as observability, secrets management, and deployment automation tools etc...</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#3-architecture-design","title":"3. Architecture Design","text":"<p>This section details the proposed cloud-native, event-driven messaging architecture, designed to meet the high-throughput and low-latency requirements of modern enterprise applications. Leveraging Azure Kubernetes Service (AKS), Azure Event Hub, and PostgreSQL, the architecture delivers scalable, resilient, and observable microservices. The following subsections describe the system\u2019s layered structure, component responsibilities, data flow, design principles, key trade-offs, and security considerations. Figures 1 and 2 illustrate the overall architecture and workflow.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#31-design-overview","title":"3.1 Design Overview","text":"<p>The system is organized into four primary layers:</p> <ol> <li>Publisher Microservices: Generate and publish domain events.</li> <li>Azure Event Hub: Serves as a distributed, scalable messaging backbone.</li> <li>Subscriber Microservices: Consume and process events in parallel.</li> <li>PostgreSQL Database: Provides persistent, transactional storage.</li> </ol> <p>All components operate within the Kubernetes orchestration framework, which automates deployment, scaling, and self-healing. Monitoring and observability are achieved through Azure Monitor, Prometheus, and Grafana, providing continuous visibility into system performance. Figure 1 presents the conceptual architecture, while Figure 2 details the workflow and data flow among system components.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#32-component-architecture","title":"3.2 Component Architecture","text":"<p>Publisher Microservices: Containerized publisher pods generate event messages representing real-time business actions (e.g., transaction initiation, authorization). Deployed within AKS namespaces, each pod serializes domain events (in JSON or Avro) and transmits them asynchronously to Azure Event Hub. Multiple publisher instances run concurrently to maximize throughput and redundancy.</p> <p>Azure Event Hub: Event Hub acts as the central event streaming platform, capable of handling millions of events per second. Events are distributed across multiple partitions, enabling parallel consumption by subscriber pods. Each partition is mapped to a consumer group, ensuring that scaling subscriber instances does not result in message duplication. Event Hub provides at-least-once delivery guarantees and supports dynamic throughput scaling by adjusting throughput units.</p> <p>Subscriber Microservices: Subscriber pods listen to specific topics or consumer groups, retrieving events in near real time. Each pod independently processes, validates, and applies business logic to event payloads, then writes results to PostgreSQL. The Kubernetes Horizontal Pod Autoscaler (HPA) monitors CPU, memory, and Event Hub queue metrics to dynamically adjust the number of subscriber pods based on load intensity, supporting elastic scaling.</p> <p>PostgreSQL Database: PostgreSQL ensures persistent storage for processed events and transactional data. The architecture typically includes a primary node and one or more read replicas for high availability and query offloading. The schema is optimized for high ingestion throughput while maintaining ACID properties for data integrity.</p> <p>Monitoring and Observability: Prometheus collects metrics, Grafana provides visualization, and Azure Monitor delivers system-wide telemetry. These tools capture key performance indicators\u2014such as message throughput, latency, processing time, and scaling events\u2014enabling continuous performance tuning and anomaly detection.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#33-data-flow","title":"3.3 Data Flow","text":"<p>The end-to-end data flow is illustrated in Figure 2 and proceeds as follows:</p> <p>Figure 2. Data Flow Diagram</p> <p></p> <p>Figure 2: Stepwise data flow from event publication to monitoring and feedback, highlighting the main processing and feedback loop in the architecture.</p> <ol> <li>Event Publication: External systems or APIs trigger business events, which are received via an ingress endpoint. Publisher pods process, enrich, and publish these events to Azure Event Hub.</li> <li>Event Distribution: Event Hub partitions distribute messages evenly to balance throughput. Each partition is read by a subscriber instance within a consumer group, supporting parallelism and high throughput.</li> <li>Event Consumption: Subscriber pods asynchronously consume messages, execute domain-specific logic (e.g., validation, rule application), and write processed outcomes to PostgreSQL.</li> <li>Persistence and Analytics: PostgreSQL stores finalized data, enabling historical analysis and integration with downstream analytics pipelines.</li> <li>Monitoring and Scaling: Prometheus and Azure Monitor track throughput, latency, and resource utilization. When message backlogs or CPU thresholds are exceeded, the HPA automatically scales subscriber or publisher deployments to maintain responsiveness.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#34-design-principles","title":"3.4 Design Principles","text":"<p>The architecture is guided by foundational principles to ensure scalability, reliability, and maintainability:</p> <ul> <li>Scalability: Designed for dynamic workload increases, leveraging Kubernetes\u2019 horizontal pod autoscaling and intelligent resource allocation to scale compute and storage as needed.</li> <li>Resiliency: Architected for fault tolerance through pod replication, health checks, automated restarts, and multi-zone deployment.</li> <li>Observability: Real-time monitoring, centralized logging, distributed tracing, and metrics collection enable rapid detection and resolution of performance bottlenecks.</li> <li>Automation: Infrastructure as Code (IaC) and GitOps pipelines automate provisioning, deployment, and configuration, reducing human error and accelerating delivery.</li> <li>Decoupling: Loose coupling of services enables independent deployment, upgrades, and scaling, supported by asynchronous messaging and pub/sub patterns.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#35-trade-offs","title":"3.5 Trade-offs","text":"<p>Several trade-offs were considered in the architectural design:</p> <ul> <li>Complexity vs. Flexibility: Microservices offer deployment flexibility and independent scaling but introduce operational complexity in service discovery, inter-service communication, and monitoring. Automation and observability help mitigate these challenges.</li> <li>Consistency vs. Availability: Some services adopt eventual consistency to maximize availability, especially in distributed systems where immediate synchronization could impact uptime.</li> <li>Resource Utilization vs. Cost: Over-provisioning resources guarantees performance but increases costs; aggressive autoscaling reduces costs but may introduce cold start latency during peak loads.</li> <li>Security vs. Developer Velocity: Strong access controls, encryption, and audit policies enhance security but can slow development if not automated. Integrating security into CI/CD pipelines helps balance these needs.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#36-security-overview","title":"3.6 Security Overview","text":"<p>Security is embedded at every layer of the Kubernetes infrastructure to protect sensitive data and maintain compliance with industry standards (e.g., SOC 2, PCI DSS):</p> <ul> <li>Identity and Access Management (IAM): Centralized authentication (Okta) and fine-grained authorization (Keycloak) ensure only authorized services and users can perform operations.</li> <li>Network Security: Namespaces, network policies, and service meshes provide isolation and encrypted communication, reducing attack surfaces and lateral movement.</li> <li>Data Protection: All sensitive data is encrypted at rest and in transit. Azure Key Vault manages secrets, and TLS secures service-to-service communication.</li> <li>Secrets Management: Centralized storage of sensitive configuration and credentials in Azure Key Vault, with access controlled by role-based policies and audit logging.</li> <li>Monitoring and Compliance: Continuous security monitoring, log aggregation, and alerting detect anomalies in real time. Audit trails ensure accountability and regulatory compliance.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary_1","title":"Summary","text":"<p>The proposed architecture integrates Kubernetes, Azure Event Hub, and PostgreSQL to deliver a scalable, resilient, and observable messaging platform. This design provides a practical blueprint for implementing high-throughput, event-driven microservices in cloud-native settings.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#4-implementation-details","title":"4. Implementation Details","text":"<p>This section describes the systematic implementation of the proposed architecture, translating conceptual design into a robust, automated, and observable deployment on Azure Kubernetes Service (AKS). Each layer\u2014from infrastructure provisioning to security and feedback\u2014was engineered to maximize scalability, reliability, and operational efficiency. The following subsections detail the practical realization of each architectural component and workflow.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#41-infrastructure-provisioning-and-environment-setup","title":"4.1 Infrastructure Provisioning and Environment Setup","text":"<p>The foundation of the deployment is provisioned using Terraform, ensuring consistent, version-controlled, and repeatable environment creation. Azure Resource Groups, Virtual Networks, AKS clusters, and associated dependencies (e.g., Azure Container Registry, Key Vault, Log Analytics workspace) are defined declaratively. Terraform modules are structured by environment, network, compute, and security, encapsulating reusable patterns for each layer across development, staging, and production.</p> <p>To enforce least-privilege access, Azure Active Directory (AAD) integration is implemented at the cluster level using role-based access control (RBAC). Service principals and managed identities are used for resource authentication, while Terraform state is securely managed using remote backends with encryption enabled in Azure Storage.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#42-gitops-workflow-with-argocd","title":"4.2 GitOps Workflow with ArgoCD","text":"<p>Continuous delivery is achieved through a GitOps workflow using ArgoCD, which maintains synchronization between source control and live cluster states. This pull-based reconciliation model replaces traditional push-based deployments, enhancing auditability and operational resilience.</p> <p>Kubernetes manifests for each application are version-controlled in dedicated Git repositories. ArgoCD monitors these repositories, detecting configuration drift and automatically reconciling discrepancies to ensure the running state matches the declared state. This process enforces deployment immutability and supports progressive delivery strategies such as canary and blue-green rollouts, minimizing disruption during updates.</p> <p>The GitOps workflow is tightly integrated with CI systems (e.g., Azure DevOps, GitHub Actions), which trigger ArgoCD deployments upon successful image builds, automated manifest regeneration, and policy validation. This synergy creates a fully automated pipeline for delivering microservice updates with zero-touch operational overhead.</p> <p>Figure 3. Deployment Pipeline Diagram (CI/CD &amp; GitOps)</p> <p></p> <p>Figure 3: End-to-end CI/CD and GitOps deployment pipeline, illustrating the flow from code commit through automated build, infrastructure provisioning, policy/approval gates, and ArgoCD-driven deployment to AKS clusters.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#43-manifest-generation-and-automation","title":"4.3 Manifest Generation and Automation","text":"<p>Automated manifest generation reduces human error and ensures consistency across environments. Helm and Kustomize are used to template Kubernetes resources dynamically. Helm charts encapsulate reusable deployment logic for microservices, enabling parameterized configuration through values files, while Kustomize overlays allow environment-specific customization without duplication.</p> <p>An automation layer within the CI/CD pipeline generates manifests dynamically based on metadata in Git repositories and Terraform outputs. Environment-specific variables\u2014such as namespace, ingress configurations, or Azure Key Vault secrets\u2014are injected via Helm values files or Kustomize overlays. This approach ensures consistency between declarative infrastructure provisioning and application-level deployment manifests.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#44-observability-stack-integration","title":"4.4 Observability Stack Integration","text":"<p>The observability layer integrates Prometheus, Grafana, Azure Monitor, and OpenTelemetry to provide full-stack visibility across clusters and services. Prometheus scrapes metrics from application and system exporters, while Grafana dashboards visualize key performance indicators such as latency, request throughput, and error ratios.</p> <p>Figure 4. Observability Stack Diagram</p> <p></p> <p>Figure 4: End-to-end observability stack, showing the flow of metrics, logs, and traces from AKS microservices through exporters, Prometheus, Azure Monitor, OpenTelemetry, and into Grafana dashboards for visualization and alerting.</p> <p>Azure Monitor for Containers and Log Analytics centralize logging, metric aggregation, and alerting. Distributed tracing is enabled through OpenTelemetry instrumentation, capturing end-to-end request paths across microservices. These traces are correlated with logs and metrics for contextual debugging and diagnostics.</p> <p>AI-driven observability modules, powered by Azure Machine Learning and custom anomaly detection models, continuously analyze telemetry data to identify irregular patterns. Alerting thresholds are tuned based on historical data and operational experience. When anomalies are detected, the system initiates self-healing actions according to defined remediation policies.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#45-self-healing-and-remediation-automation","title":"4.5 Self-Healing and Remediation Automation","text":"<p>Self-healing capabilities are implemented using Kubernetes Operators and event-driven automation workflows powered by KEDA (Kubernetes Event-driven Autoscaling) and Azure Logic Apps. Operators continuously monitor resource health, leveraging custom controllers that interpret anomalies detected by AI models.</p> <p>Figure 5. Self-Healing Workflow Diagram</p> <p></p> <p>Figure 5: Self-healing workflow showing the detection of anomalies, automated remediation actions, and the feedback loop for adaptive learning and policy tuning in a cloud-native environment.</p> <p>Upon detection of abnormal states\u2014such as memory saturation, pod crashes, or degraded latency\u2014automated workflows trigger remediation actions. For example, if a pod crash is detected, the operator triggers a rolling restart of the affected deployment. For infrastructure-level issues, Terraform-based corrective scripts are invoked through event hooks to restore compliance.</p> <p>A feedback mechanism is embedded within the AI loop, enabling the learning model to adapt to observed patterns and improve future prediction accuracy. This adaptive reinforcement ensures that the system evolves with the operational behavior of deployed microservices.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#46-security-and-compliance-controls","title":"4.6 Security and Compliance Controls","text":"<p>Security is embedded throughout the implementation lifecycle to ensure compliance with standards such as SOC 2 and PCI DSS. Image scanning (using Trivy or Microsoft Defender for Containers) is integrated into the CI pipeline to identify vulnerabilities prior to deployment. Network segmentation is enforced through Azure Network Policies, restricting east-west traffic between namespaces. Secrets and credentials are stored and accessed via Azure Key Vault, ensuring centralized secret governance.</p> <p>Continuous compliance is maintained by integrating Azure Policy with Terraform and ArgoCD to validate resource configurations against enterprise standards. These policies enforce mandatory encryption, restricted public IP usage, and adherence to naming and tagging conventions. Audit logs are regularly reviewed to ensure accountability and support regulatory requirements.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#47-testing-validation-and-continuous-feedback","title":"4.7 Testing, Validation, and Continuous Feedback","text":"<p>Comprehensive validation mechanisms are integrated into the pipeline to ensure deployment reliability. Automated integration and performance testing validate both functional and non-functional aspects of deployed services. ArgoCD\u2019s application health metrics and synchronization status are continuously monitored to detect drift or failed rollouts.</p> <p>Feedback loops from monitoring dashboards, alerting systems, and AI-detected anomalies are periodically reviewed to refine observability thresholds, scaling rules, and remediation workflows. This continuous feedback ensures adaptive improvement and enhances the overall resilience of the Kubernetes ecosystem.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary_2","title":"Summary","text":"<p>This section details the practical realization of the architecture, emphasizing automation, security, and observability. The implementation demonstrates how design principles translate into a robust, scalable, and compliant Kubernetes-based messaging environment.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#5-evaluation-and-results","title":"5. Evaluation and Results","text":"<p>This section presents a rigorous empirical evaluation of the proposed cloud-native messaging architecture, focusing on its scalability, resiliency, deployment efficiency, observability, and security compliance. The primary objective is to demonstrate how the design principles and implementation strategies outlined in previous sections translate into measurable operational benefits and performance improvements in a realistic enterprise context.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#51-evaluation-objectives","title":"5.1 Evaluation Objectives","text":"<p>The evaluation was structured to address the following objectives:</p> <ol> <li>Scalability: Assess the system\u2019s ability to elastically handle dynamic workloads through horizontal pod autoscaling and efficient resource utilization.</li> <li>Resiliency: Evaluate fault tolerance and recovery mechanisms in response to pod, node, or network failures.</li> <li>Deployment Efficiency: Quantify the operational gains achieved by integrating ArgoCD-based GitOps workflows and Terraform-driven Infrastructure as Code (IaC).</li> <li>Observability: Examine the effectiveness of real-time monitoring, logging, and distributed tracing in identifying and resolving performance issues.</li> <li>Security Compliance: Verify adherence to enterprise security policies, including authentication, authorization, and secrets management.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#52-experimental-testbed","title":"5.2 Experimental Testbed","text":"<p>Experiments were conducted on a dedicated, non-production Azure Kubernetes Service (AKS) environment designed to emulate enterprise-scale workloads:</p> <ul> <li>Cluster Configuration:</li> <li>3-node system pool (DS3v2) for control-plane components</li> <li>6-node application pool (DS4v2) for microservices workloads</li> <li>2-node platform pool for shared infrastructure services</li> <li>Infrastructure Management: All resources provisioned via Terraform, including VNet, subnets, role assignments, and Key Vault integrations</li> <li>Service Deployment: Automated using Helm-based manifest generation and ArgoCD GitOps synchronization</li> <li>Workload Composition:</li> <li>10 microservices distributed across three namespaces (<code>finance-app</code>, <code>user-management</code>, <code>platform-services</code>)</li> <li>Each service connected to Azure Service Bus (for asynchronous communication) and PostgreSQL (for persistent storage)</li> <li>Load Simulation: Synthetic workloads generated using Locust and K6, simulating 1,000\u201310,000 concurrent requests per second to model realistic transaction volumes</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#53-performance-metrics-and-measurement","title":"5.3 Performance Metrics and Measurement","text":"<p>To ensure a comprehensive evaluation, both system-level and service-level metrics were collected using Prometheus, Grafana, and Azure Monitor. The following table summarizes the key metrics, their purposes, measurement tools, and target thresholds:</p> Metric Purpose Measurement Tool Target/Threshold Average Response Time Measure request latency during load Prometheus + Grafana &lt; 200 ms under normal load Autoscaling Latency Time to scale from baseline to peak load Azure Monitor &lt; 30 seconds CPU &amp; Memory Utilization Resource efficiency across services Prometheus 65\u201375% utilization Deployment Time From Git commit to cluster sync via ArgoCD ArgoCD Metrics &lt; 5 minutes Recovery Time Time to recover after pod/node failure K8s Event Logs &lt; 20 seconds Security Policy Compliance RBAC, network policy, and secret validation OPA/Gatekeeper 100% compliance"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#54-results-and-analysis","title":"5.4 Results and Analysis","text":"<p>Scalability and Performance</p> <p>The architecture exhibited linear scalability under increasing load, with the Horizontal Pod Autoscaler (HPA) responding to CPU and memory thresholds within an average of 24 seconds. Response times remained below 180 ms for up to 8,000 concurrent requests per second, and no service experienced throttling or timeouts at peak load. These results confirm the elasticity and robustness of the AKS-based design.</p> <p>Deployment Efficiency</p> <p>The ArgoCD-powered GitOps workflow achieved deployment synchronization in under 4 minutes\u2014a 62% improvement over manual CI/CD processes. Combined with Terraform-based provisioning, environment setup time was reduced from 8 hours to under 90 minutes. Versioned infrastructure and declarative deployments enabled full traceability and rapid rollback, enhancing operational governance.</p> <p>Resiliency and Fault Tolerance</p> <p>Pod termination and node restart scenarios validated the system\u2019s self-healing capabilities. Failed pods were rescheduled within 15 seconds, maintaining uninterrupted service. The load balancer and ingress controller efficiently rerouted traffic during disruptions, ensuring high availability even under transient failures.</p> <p>Observability and Monitoring</p> <p>The integration of Prometheus, OpenTelemetry, and Azure Monitor provided near real-time visibility across all services. Distributed traces captured end-to-end transaction paths, identifying dependency latency in under 300 ms. Grafana dashboards unified cluster health, resource usage, and application performance, reducing mean time to detect (MTTD) from 12 minutes to less than 3 minutes.</p> <p>Security and Compliance</p> <p>Security validation confirmed 100% compliance with organizational policies. Authentication (Okta) and authorization (Keycloak) operated seamlessly with token-based service-to-service communication. All secrets and credentials were securely managed in Azure Key Vault, with RBAC enforcing least-privilege access. Network segmentation via Kubernetes Network Policies prevented unauthorized cross-namespace access.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#55-discussion","title":"5.5 Discussion","text":"<p>The evaluation demonstrates that the proposed architecture meets its core design objectives. The combination of Terraform-based IaC, ArgoCD-driven GitOps, and automated manifest generation delivers a reproducible, resilient, and observable system suitable for enterprise workloads. Scalability was validated under intensive concurrent workloads, and automated provisioning and deployment pipelines significantly improved developer productivity and release reliability.</p> <p>The observability stack provided actionable insights for proactive anomaly detection, while robust security controls\u2014especially centralized secrets management and policy enforcement\u2014ensured compliance and minimized risk. Although the integration of multiple tools (Terraform, Helm, ArgoCD, Prometheus) introduces operational complexity, the automation and governance benefits far outweigh the orchestration overhead. Future enhancements may include AI-driven autoscaling and predictive anomaly detection to further optimize performance and cost.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary_3","title":"Summary","text":"<p>The evaluation confirms that the architecture meets its objectives for scalability, resilience, and operational efficiency. Results demonstrate that automation and observability are key enablers for reliable, high-performance cloud-native messaging systems.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#6-implementation-insights-and-recommendations","title":"6. Implementation Insights and Recommendations","text":""},{"location":"resources/publications/messaging-architecture/messaging-architecture/#61-translating-evaluation-findings-into-practice","title":"6.1 Translating Evaluation Findings into Practice","text":"<p>The evaluation results underscore the technical maturity and operational efficiency achieved through declarative, event-driven architectures on Azure Kubernetes Service (AKS). This section distills those findings into actionable insights and recommendations for enterprises seeking to adopt or scale similar architectures. The focus is on the integration of Infrastructure as Code (IaC), GitOps-based continuous delivery, and observability-driven automation as enablers of consistent, secure, and scalable operations.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#62-infrastructure-as-code-iac-with-terraform","title":"6.2 Infrastructure as Code (IaC) with Terraform","text":"<p>Terraform\u2019s declarative provisioning model is foundational for repeatability and compliance. The following practices are critical for enterprise-scale IaC adoption:</p> <ol> <li>Modular Design: Develop reusable Terraform modules for core components\u2014AKS clusters, Key Vaults, Application Gateways, and Managed Identities. Modularization improves maintainability and facilitates environment reusability.</li> <li>Secure Remote State Management: Store Terraform state in an Azure Storage Account with locking enabled via Blob leases. This prevents concurrent updates and ensures state integrity across distributed teams.</li> <li>Automated Environment Promotion: Implement environment promotion pipelines in Azure DevOps or GitHub Actions, enabling infrastructure changes to flow automatically from development through testing to production, with governance approval gates as needed.</li> <li>Policy as Code Integration: Enforce compliance by embedding Azure Policy or Terraform Sentinel checks, ensuring all provisioned resources adhere to security baselines such as private clusters, encryption at rest, and Key Vault integration.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#63-gitops-with-argocd","title":"6.3 GitOps with ArgoCD","text":"<p>Transitioning from traditional CI/CD pipelines to a GitOps-driven deployment model with ArgoCD significantly improves deployment predictability, rollback accuracy, and auditability.</p> <p>Key recommendations include:</p> <ol> <li>Single Source of Truth: Maintain all Kubernetes manifests, Helm charts, and Kustomize overlays in dedicated Git repositories for full version control and reproducibility.</li> <li>\u201cApp of Apps\u201d Pattern: Use ArgoCD\u2019s hierarchical management to orchestrate multiple applications or namespaces from a central controller, supporting multi-tenant environments while preserving application-level autonomy.</li> <li>Automated Manifest Generation: Integrate automation tools such as Helmfile, Kustomize, or Terraform providers to dynamically generate manifests, reducing YAML duplication and configuration errors.</li> <li>Sync Policy Optimization: Configure automated synchronization for lower environments and manual approval for production clusters to balance agility with control.</li> <li>Drift Detection and Rollback: Enable drift detection and automated rollback in ArgoCD to ensure production environments remain continuously aligned with the Git repository state.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#64-observability-and-resilience-engineering","title":"6.4 Observability and Resilience Engineering","text":"<p>Comprehensive observability is essential for operational transparency and proactive issue resolution. Key insights include:</p> <ol> <li>Unified Metrics Architecture: Integrate Prometheus, Grafana, and Azure Monitor for real-time telemetry, supported by Azure Log Analytics for centralized cross-cluster data aggregation.</li> <li>Alert-Driven Self-Healing: Combine monitoring alerts with automation frameworks such as KEDA or Argo Rollouts to enable autonomous remediation actions (e.g., scaling or pod replacement).</li> <li>End-to-End Traceability: Implement correlation IDs across microservices to enable distributed tracing and facilitate root cause analysis in asynchronous, event-driven systems.</li> <li>Chaos and Resilience Validation: Use Azure Chaos Studio or equivalent tooling to inject controlled faults, validating autoscaling behavior and recovery under simulated stress conditions.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#65-security-and-compliance-integration","title":"6.5 Security and Compliance Integration","text":"<p>Security and governance are intrinsic to all layers of the architecture. The following recommendations ensure alignment with enterprise-grade compliance frameworks:</p> <ol> <li>Centralized Secret Management: Store and manage sensitive configurations exclusively in Azure Key Vault, integrated with AKS via Managed Identities to eliminate hardcoded secrets.</li> <li>Granular RBAC and Network Controls: Define namespace-scoped roles and apply Kubernetes network policies to minimize lateral movement between services.</li> <li>Continuous Image and Dependency Scanning: Integrate security scanning tools (e.g., Microsoft Defender for Containers, Trivy) within CI pipelines to prevent vulnerable images from being deployed.</li> <li>Automated Auditability: Combine ArgoCD audit logs, Terraform plan histories, and Azure Policy compliance reports to maintain end-to-end traceability of infrastructure and application changes.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#66-organizational-and-process-recommendations","title":"6.6 Organizational and Process Recommendations","text":"<p>Beyond technical tooling, successful adoption requires organizational adaptation and process alignment:</p> <ol> <li>Platform Engineering Teams: Establish dedicated teams responsible for maintaining shared IaC modules, GitOps templates, and cluster governance standards across environments.</li> <li>Standardized Deployment Pipelines: Implement consistent deployment templates that apply uniformly across all business units and environments, improving predictability and reducing onboarding time.</li> <li>Feedback-Driven Continuous Improvement: Use observability metrics and postmortem analyses to drive iterative refinements in reliability, cost optimization, and deployment performance.</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#67-lessons-learned","title":"6.7 Lessons Learned","text":"<p>Practical experience with the proposed system yielded several critical insights:</p> <ul> <li>Declarative automation through Terraform and ArgoCD drastically reduces operational drift but requires early investment in repository design and process governance.</li> <li>Observability-first design enhances reliability when introduced at inception rather than retrofitted post-deployment.</li> <li>Governance achieves greater adoption when it is automated and transparent, rather than enforced through manual approval chains.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary_4","title":"Summary","text":"<p>This section distills practical lessons and actionable recommendations for implementing scalable, secure, and observable cloud-native messaging systems. It emphasizes the importance of automation, GitOps, and continuous improvement to achieve operational excellence in Kubernetes-based environments.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#7-future-work-and-research-directions","title":"7. Future Work and Research Directions","text":"<p>Figure 7. Future Work Vision Diagram</p> <p></p> <p>Figure 6: Vision for future research\u2014showing the evolution from current cloud-native platforms to intelligent, sustainable, and policy-aware architectures, including AI-driven autoscaling, autonomous remediation, intelligent observability, policy-driven governance, sustainability, and multi-cloud/edge integration.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#71-motivation-for-continued-research","title":"7.1 Motivation for Continued Research","text":"<p>While the proposed event-driven microservices architecture on Azure Kubernetes Service (AKS) demonstrates significant improvements in scalability, reliability, and automation, the field is rapidly evolving. Emerging trends in AI-driven automation, adaptive scaling, and autonomous operations present new opportunities to further enhance Kubernetes-based enterprise platforms. This section outlines prospective research and development directions that can extend the capabilities of the presented architecture, addressing challenges in predictive management, cost optimization, and intelligent observability.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#72-ai-driven-autoscaling-and-resource-optimization","title":"7.2 AI-Driven Autoscaling and Resource Optimization","text":"<p>Traditional autoscaling mechanisms, such as the Horizontal Pod Autoscaler (HPA) and Kubernetes Event-Driven Autoscaler (KEDA), are reactive and lack predictive intelligence. Future research should explore AI-enhanced autoscaling algorithms capable of learning workload patterns, forecasting demand surges, and preemptively allocating resources.</p> <p>Potential directions include:</p> <ul> <li>Predictive Modeling: Develop machine learning models that analyze historical workload trends to forecast CPU, memory, and I/O demands in advance.</li> <li>Reinforcement Learning for Scaling Decisions: Train reinforcement learning agents to dynamically optimize scaling thresholds based on performance and cost trade-offs.</li> <li>Multi-Metric Adaptation: Move beyond CPU/memory-based triggers to incorporate business-level metrics (e.g., transaction volume, API latency) for context-aware scaling.</li> </ul> <p>Such models could be integrated into Kubernetes controllers or external orchestrators, resulting in smarter, self-optimizing infrastructure.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#73-autonomous-remediation-and-self-healing-systems","title":"7.3 Autonomous Remediation and Self-Healing Systems","text":"<p>While the current architecture incorporates self-healing through declarative states and ArgoCD synchronization, future systems could employ autonomous remediation mechanisms driven by anomaly detection and policy-based reasoning.</p> <p>Research opportunities include:</p> <ul> <li>Anomaly Detection Pipelines: Use AI models to detect early signs of degradation or misconfiguration across microservices using real-time observability data.</li> <li>Intent-Based Policies: Encode operational intents (e.g., \u201cmaintain 99.9% uptime\u201d) into machine-readable policies, allowing controllers to take corrective actions automatically.</li> <li>Automated Root Cause Analysis: Combine log aggregation and graph-based causal inference to reduce mean time to recovery (MTTR) by identifying the true source of cascading failures.</li> </ul> <p>This direction aligns with the concept of AIOps (Artificial Intelligence for IT Operations), where machine learning augments DevOps workflows to achieve autonomous system resilience.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#74-intelligent-observability-and-anomaly-prediction","title":"7.4 Intelligent Observability and Anomaly Prediction","text":"<p>Current observability practices focus on descriptive and diagnostic capabilities. Future advancements can elevate observability toward predictive and prescriptive modes, where systems not only detect issues but also recommend or execute corrective measures.</p> <p>Key directions include:</p> <ol> <li>Cognitive Dashboards: Enhance Grafana or Azure Monitor dashboards with AI-driven insights, summarizing anomalies, trends, and recommended remediations.</li> <li>Cross-Layer Correlation: Correlate data across infrastructure, application, and business layers to identify performance degradation that traditional metrics may overlook.</li> <li>Behavioral Profiling: Establish dynamic performance baselines for microservices using time-series analysis and unsupervised learning to detect deviations automatically.</li> </ol> <p>Such intelligent observability systems can serve as the foundation for proactive incident management and context-aware alerting, reducing operational noise and human intervention.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#75-policy-driven-governance-and-compliance-automation","title":"7.5 Policy-Driven Governance and Compliance Automation","text":"<p>As organizations scale their Kubernetes environments, maintaining governance becomes increasingly complex. Future research should emphasize policy-driven compliance automation, ensuring that every deployment adheres to corporate, regulatory, and security policies without manual oversight.</p> <p>Promising research areas include:</p> <ul> <li>Declarative Governance Frameworks: Extend GitOps workflows with policy repositories managed as code, integrating tools such as Open Policy Agent (OPA) and Kyverno.</li> <li>Adaptive Compliance Monitoring: Introduce AI models that continuously assess policy adherence, adjusting enforcement dynamically based on context (e.g., environment, region, data sensitivity).</li> <li>Automated Remediation of Violations: Enable systems to automatically revert or block noncompliant configurations at the admission controller level before deployment.</li> </ul> <p>This aligns with the broader enterprise shift toward \u201ccompliance as code,\u201d a key paradigm for regulated industries adopting cloud-native infrastructure.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#76-sustainable-and-cost-aware-kubernetes-operations","title":"7.6 Sustainable and Cost-Aware Kubernetes Operations","text":"<p>Sustainability and cost efficiency are becoming critical non-functional requirements for enterprise systems. Future work could explore energy- and cost-aware orchestration models that leverage AI to balance performance with sustainability metrics.</p> <p>Potential research paths include:</p> <ul> <li>Carbon-Aware Scheduling: Integrate cloud carbon footprint data into Kubernetes schedulers to prioritize nodes in regions with lower emission intensities.</li> <li>Cost Predictive Analytics: Develop models to forecast cost implications of scaling actions, helping organizations optimize workload placement and resource provisioning strategies.</li> <li>Dynamic Spot Instance Utilization: Combine predictive scaling with spot-instance scheduling for compute-intensive but fault-tolerant workloads to minimize operational expenditure.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#77-integration-with-multi-cloud-and-edge-environments","title":"7.7 Integration with Multi-Cloud and Edge Environments","text":"<p>As enterprises evolve toward distributed cloud and edge computing, the proposed AKS-based model can be extended to multi-cloud and edge-native deployments. Key areas for exploration include:</p> <ul> <li>Federated Control Planes: Use Kubernetes Federation (KubeFed) or Azure Arc to manage hybrid workloads consistently across on-premises, edge, and multiple cloud providers.</li> <li>Latency-Aware Scheduling: Apply edge intelligence for location-based workload placement to meet stringent latency requirements in real-time analytics or IoT systems.</li> <li>Unified GitOps and IaC Workflows: Extend Terraform and ArgoCD pipelines to operate seamlessly across heterogeneous environments while preserving declarative consistency.</li> </ul>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#78-academic-and-industry-collaboration-opportunities","title":"7.8 Academic and Industry Collaboration Opportunities","text":"<p>The intersection of AI, cloud-native computing, and automation offers a rich landscape for collaboration between academia and industry. Research partnerships could focus on:</p> <ul> <li>Developing standardized benchmarks for AI-driven autoscaling</li> <li>Creating open-source reference architectures for intelligent observability</li> <li>Evaluating sustainability metrics for enterprise-grade Kubernetes operations</li> </ul> <p>Such collaborations would accelerate innovation and bridge the gap between theoretical advancements and operational adoption in enterprise contexts.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#summary_5","title":"Summary","text":"<p>The next frontier of cloud-native systems lies in the fusion of automation, intelligence, and policy. AI-driven decision-making, predictive resource management, and compliance automation are poised to redefine the future of Kubernetes operations. By building on the foundations established in this study\u2014GitOps, Terraform-based IaC, and observability integration\u2014enterprises can evolve toward self-adaptive, sustainable, and policy-aware Kubernetes ecosystems that align with both technical excellence and organizational agility.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#conclusion","title":"Conclusion","text":"<p>This study has presented a comprehensive evaluation of high-throughput cloud-native messaging architectures, focusing on Pub/Sub microservices deployed on Kubernetes and integrated with Azure Event Hub. Through detailed analysis of architectural design, implementation strategies, and empirical performance metrics, we have demonstrated both the capabilities and limitations of cloud-native messaging systems in supporting large-scale, real-time workloads.</p> <p>The results indicate that leveraging Azure Event Hub as the core messaging backbone, in conjunction with Kubernetes orchestration, enables elastic scaling, low-latency message delivery, and fault-tolerant operations. Performance benchmarks reveal that throughput and latency are significantly influenced by microservice deployment strategies, CNI network configurations, and resource allocation policies within Kubernetes clusters. Systematic tuning of these parameters can yield substantial improvements in throughput and end-to-end latency, ensuring robust and reliable event-driven communication across distributed microservices.</p> <p>Moreover, this work underscores the critical importance of observability and monitoring in cloud-native messaging environments. The integration of native monitoring solutions such as Azure Monitor, combined with custom metrics, enables proactive detection of bottlenecks, resource contention, and potential system failures. This proactive approach supports self-healing capabilities and ensures sustained high performance in dynamic production environments.</p> <p>In summary, the combination of Kubernetes-managed microservices and Azure Event Hub provides a robust foundation for building scalable, resilient, and high-throughput messaging platforms. Future research may extend this work by exploring multi-cloud deployments, advanced autoscaling policies leveraging AI-driven insights, and serverless integration to further optimize resource utilization and reduce operational overhead in large-scale distributed systems.</p>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#references","title":"References","text":""},{"location":"resources/publications/messaging-architecture/messaging-architecture/#journal-articles-conference-papers-and-academic-sources","title":"Journal Articles, Conference Papers, and Academic Sources","text":"<ol> <li>Gokhale, A. (2021). A comprehensive performance evaluation of different Kubernetes CNI plugins. Vanderbilt University. Available at https://www.dre.vanderbilt.edu/~gokhale/WWW/papers/IC2E21_CNI_Eval.pdf [Accessed: Oct. 22, 2025].</li> <li>Mittal, A., &amp; Singh, R. (2023). Next-generation event-driven architectures: Performance analysis of Kafka, Pulsar, and serverless. arXiv. https://arxiv.org/html/2510.04404v1 [Accessed: Oct. 22, 2025].</li> <li>Henning, S., &amp; Hasselbring, W. (2023). Benchmarking scalability of stream processing frameworks deployed as microservices in the cloud. arXiv. https://arxiv.org/abs/2303.11088 [Accessed: Oct. 22, 2025].</li> <li>MDPI. (2024). Performance and latency efficiency evaluation of Kubernetes CNI plugins. Electronics, 13(19), 3972. https://www.mdpi.com/2079-9292/13/19/3972 [Accessed: Oct. 22, 2025].</li> <li>Larsson, L., T\u00e4rneberg, W., Klein, C., Elmroth, E., &amp; Kihl, M. (2020). Impact of etcd Deployment on Kubernetes, Istio, and Application Performance. arXiv. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Dragoni, N., Giallorenzo, S., Lafuente, A. L., Mazzara, M., Montesi, F., Mustafin, R., &amp; Safina, L. (2017). Microservices: Yesterday, Today, and Tomorrow. Present and Ulterior Software Engineering, 195-216. https://arxiv.org/abs/1606.04036 [Accessed: Oct. 22, 2025].</li> <li>MDPI. (2024). Performance and latency efficiency evaluation of Kubernetes CNI plugins. Electronics, 13(19), 3972. https://www.mdpi.com/2079-9292/13/19/3972 [Accessed: Oct. 22, 2025].</li> <li>Sampaio, A. R. (2019). Improving microservice-based applications with runtime adaptation mechanisms. Journal of Information Systems and Applications. https://jisajournal.springeropen.com/articles/10.1186/s13174-019-0104-0 [Accessed: Oct. 22, 2025].</li> <li>Tasmin, F., Poudel, S., &amp; Tareq, A. H. (2025). Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks. arXiv. https://arxiv.org/abs/2510.04404 [Accessed: Oct. 22, 2025].</li> <li>Vladutu, C. (2025). Azure Event Grid vs Azure Service Bus vs Event Hubs: When to Use Each. Medium. https://cosmin-vladutu.medium.com/azure-event-grid-vs-azure-service-bus-vs-event-hubs-when-to-use-each-12900bb32ce8 [Accessed: Oct. 22, 2025].</li> <li>Burns, B., Grant, B., Oppenheimer, D., Brewer, E., &amp; Wilkes, J. (2016). Borg, Omega, and Kubernetes. Communications of the ACM, 59(5), 50-57. https://dl.acm.org/doi/10.1145/2890784 [Accessed: Oct. 22, 2025].</li> <li>Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A Distributed Messaging System for Log Processing. Proceedings of the NetDB, 1-7. https://dl.acm.org/doi/10.1145/2890784 [Accessed: Oct. 22, 2025].</li> <li>Zhou, Y., Li, X., &amp; Wang, J. (2020). Performance Evaluation of Kubernetes-based Publish/Subscribe Systems. Proceedings of the 2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom), 1-8. https://arxiv.org/abs/2303.11088 [Accessed: Oct. 22, 2025].</li> <li>Li, X., Zhou, Y., &amp; Wang, J. (2019). Throughput and Latency Analysis of Cloud-based Event Streaming Services. Proceedings of the 2019 IEEE International Conference on Big Data (Big Data), 1-10. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Gupta, A., Sharma, S., &amp; Sood, S. K. (2021). Auto-Scaling in Cloud Computing: A Review, Challenges, and Research Directions. Future Generation Computer Systems, 117, 322-339. https://www.sciencedirect.com/science/article/pii/S0167739X20331513 [Accessed: Oct. 22, 2025].</li> <li>Chen, L., &amp; Bahsoon, R. (2017). Self-Adaptive and Self-Healing Systems: A Survey. IEEE Access, 5, 16521-16555. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Di Francesco, P., Lago, P., &amp; Malavolta, I. (2019). Research on Architecting Microservices: Trends, Focus, and Potential for Industrial Adoption. Proceedings of the 2019 IEEE International Conference on Software Architecture (ICSA), 21-30. https://arxiv.org/abs/1606.04036 [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#books-and-technical-reports","title":"Books and Technical Reports","text":"<ol> <li>ResearchGate. (2023). Cloud-native architectures: A comparative analysis of Kubernetes and serverless computing. https://www.researchgate.net/publication/388717188_Cloud-Native_Architectures_A_Comparative_Analysis_of_Kubernetes_and_Serverless_Computing [Accessed: Oct. 22, 2025].</li> <li>Medium. (2022). The cloud-native architecture and the cloud-native data architecture. https://medium.com/cloud-and-data-gurus/the-cloud-native-architecture-and-the-cloud-native-data-architecture-17aeabe46e83 [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"resources/publications/messaging-architecture/messaging-architecture/#web-resources-and-documentation","title":"Web Resources and Documentation","text":"<ol> <li>Dapr. (2025). Pub/sub building block overview. Dapr Documentation. https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-overview/ [Accessed: Oct. 22, 2025].</li> <li>Gitconnected. (2021). Event-driven systems: A deep dive into pub/sub architecture. Level Up Coding. https://levelup.gitconnected.com/event-driven-systems-a-deep-dive-into-pubsub-architecture-39e416be913c [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Introduction to Azure Event Hubs. Microsoft Learn. https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Azure Event Hubs for Apache Kafka introduction. Microsoft Learn. https://kubernetes.anjikeesari.com/azure/16-event-hubs-part-1/ [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Performance and scale guidance for Event Hubs with Azure Functions. Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/serverless/event-hubs-functions/performance-scale [Accessed: Oct. 22, 2025].</li> <li>R. Cloud Architect. (2021). Scaling microservices with Azure Kubernetes Service (AKS) and event-driven architecture. https://roshancloudarchitect.me/scaling-microservices-with-azure-kubernetes-service-aks-and-event-driven-architecture-74900e350447 [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Advanced Azure Kubernetes Service (AKS) microservices architecture. Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Microservices architecture on Azure Kubernetes Service (AKS). Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Monitor Kubernetes clusters using Azure Monitor and cloud-native tools. Microsoft Learn. https://learn.microsoft.com/en-us/azure/azure-monitor/containers/monitor-kubernetes [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Architecture best practices for Azure Kubernetes Service (AKS). Microsoft Learn. https://learn.microsoft.com/en-us/azure/well-architected/service-guides/azure-kubernetes-service [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"techstack/argocd/","title":"Argo CD","text":"<p>If you are new to Argo CD and want to learn and become a <code>DevSecOps Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/argocd/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Argo CD<ul> <li>What is Argo CD</li> <li>Key features and benefits</li> </ul> </li> <li>Setting up an Argo CD Environment<ul> <li>Prerequisites</li> <li>Installing Argo CD</li> <li>Setting up an Argo CD server and CLI</li> <li>Configuring users and permissions</li> </ul> </li> <li>Deploying Applications with Argo CD<ul> <li>Defining an application in a Git repository</li> <li>Adding an application to Argo CD</li> <li>Integrating with other CI/CD tools</li> <li>Updating and rolling back application deployments</li> </ul> </li> <li>Working with Argo CD configurations<ul> <li>Managing application configurations in Git</li> <li>Using Git branches and tags in Argo CD</li> <li>Handling conflicts and errors in configurations</li> <li>Implementing continuous delivery</li> </ul> </li> <li>Collaborating with Argo CD<ul> <li>Using Argo CD with Git hosting platforms</li> <li>Setting up multi-user access to Argo CD</li> </ul> </li> <li>Advanced Argo CD features<ul> <li>Using Argo CD with GitOps workflows</li> <li>Integrating Argo CD with CI/CD pipelines</li> <li>Using Argo CD with GitLab CI/CD</li> </ul> </li> </ol>"},{"location":"techstack/argocd/#books","title":"Books","text":""},{"location":"techstack/argocd/#references","title":"References","text":""},{"location":"techstack/azure-cloud/","title":"Azure Cloud","text":"<p>If you are new to Azure Cloud and want to learn and become a <code>Azure Cloud Engineer</code>, here is a list of topics you need to know:</p>"},{"location":"techstack/azure-cloud/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Azure Cloud<ul> <li>What is Microsoft Azure</li> <li>Key features and benefits</li> <li>Overview of Azure services and offerings</li> </ul> </li> <li>Getting Started with Azure<ul> <li>Setting up an Azure account</li> <li>Navigating the Azure portal</li> </ul> </li> <li>Creating your first Azure resource<ul> <li>Virtual Machines in Azure</li> <li>Overview of Azure virtual machines</li> <li>Creating and managing virtual machines</li> <li>Virtual machine sizing and scalability</li> </ul> </li> <li>Storage in Azure<ul> <li>Overview of Azure storage options</li> <li>Managing storage accounts and containers</li> <li>Blob, table, and queue storage</li> <li>Storing and retrieving files in Azure</li> </ul> </li> <li>Networking in Azure<ul> <li>Overview of Azure networking options</li> <li>Virtual networks and subnets</li> <li>Load balancing and traffic management</li> <li>ExpressRoute and VPN connectivity</li> </ul> </li> <li>Databases in Azure<ul> <li>Overview of Azure database services</li> <li>Managing SQL databases</li> <li>NoSQL options with Azure Cosmos DB</li> <li>Using Azure Database for MySQL and PostgreSQL</li> </ul> </li> <li>Practical knowledge on<ul> <li>Azure Accounts, Subscriptions, and Billing</li> <li>Manage access to Azure resources using RBAC</li> <li>Create Resource Groups</li> <li>Create Azure App Service Plan &amp; ASE</li> <li>Create Web App, API App, Mobil App</li> <li>Create Azure SQL Database, Cosmos DB</li> <li>Create Storage account</li> <li>Application Insights</li> <li>Redis Cache</li> <li>Azure API Management</li> <li>Create Azure Key Vault for secrets</li> <li>Provision Azure resources via ARM Templates &amp; Portal</li> <li>Azure Monitor &amp; Log Analytics</li> <li>Calculate Azure Pricing</li> <li>Create API Gateway using API Management</li> <li>Create Azure Application Gateway</li> <li>Create VNet, Subnet with Network Security Groups</li> </ul> </li> </ol>"},{"location":"techstack/azure-cloud/#books","title":"Books","text":""},{"location":"techstack/azure-cloud/#references","title":"References","text":"<ul> <li>Azure documentation - MSDN</li> <li>Azure Tips and Tricks</li> <li>Setup Azure SQL Server to use Azure Active Directory authentication option</li> <li>IP Subnet calculator - Vnet</li> <li>Microsoft Azure: The Big Picture - Pluralsight</li> <li>Introduction to Azure App Services - App Services</li> <li>Fundamentals of Azure Cloud Services and Storage</li> <li>Microsoft Azure | Overview of Azure Virtual Network - Virtual Network</li> <li>Import an App Service Certificate - Certificate</li> <li>Export certificate to PFX - Certificate</li> <li>How to Connect Azure Web Apps To On-Premises</li> <li>Azure Network Services Architecture - Azure Network</li> <li>Azure Load Balancer Explained (internet-facing) - step by step- Load Balancer</li> <li>Azure AD Pricing Explained !!! - Pricing</li> <li>Web Application architecture with high availability using Azure Web App - high availability</li> <li>Fundamentals of Cloud Computing - pluralsight</li> <li>Azure Network Services - Application Gateway</li> <li>Microsoft Azure services overview - Azure Architecture</li> <li>Azure Network Services Architecture - Network Architecture</li> <li>Microsoft Datacenter</li> <li>azure-quickstart-templates</li> </ul>"},{"location":"techstack/azure-cloud/#azure-api-management","title":"Azure API Management","text":"<ul> <li>Liquid template - API Manager</li> <li>Microsoft Azure API Management Essentials - API Management</li> <li>How to build a CI/CD pipeline for API Management, Part 1 | Azure Friday - CI/CD pipeline for API Management</li> <li>Build a CI/CD pipeline for API Management - CI/CD pipeline for API Management</li> <li>Azure API Management DevOps Resource Ki - API Management</li> <li>Running the Extractor - API Management</li> <li>API Management documentation - API Management</li> <li>Build a CI/CD pipeline for Azure API Management - API Management</li> <li>extracting existing configurations APIM - API Management</li> <li>aka.ms/apimlove - api-management-resources</li> <li>API Management ARM Template Creator</li> </ul>"},{"location":"techstack/azure-cloud/#azure-certifications","title":"Azure Certifications","text":"<ul> <li>New Microsoft Azure Certifications Path - Azure Certifications</li> <li>AZ-104: Microsoft Azure Administrator - AZ-104</li> <li>Microsoft Certified: Azure Solutions Architect Expert - Microsoft Certified: Azure Solutions Architect Expert - Learn</li> <li>Exam AZ-300: - certifications</li> <li>Exam AZ-301 - certifications</li> </ul>"},{"location":"techstack/azure-devops/","title":"Azure DevOps","text":"<p>If you are new to Azure DevSecOps and want to learn and become a <code>Azure DevSecOps Engineer</code>, here is a list of topics you need to know:</p>"},{"location":"techstack/azure-devops/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Azure DevOps<ul> <li>What is Azure DevOps</li> <li>Key features and benefits</li> <li>Overview of Azure DevOps services and offerings</li> </ul> </li> <li>Setting up an Azure DevOps Environment<ul> <li>Creating an Azure DevOps account</li> <li>Setting up an organization and project</li> <li>Configuring users and permissions</li> </ul> </li> <li>Source Control with Azure DevOps<ul> <li>Overview of Azure DevOps source control options</li> <li>Using Git for source control</li> <li>Managing code branches and pull requests</li> <li>Integrating with other source control tools</li> </ul> </li> <li>Work Item Tracking in Azure DevOps<ul> <li>Overview of Azure DevOps work item tracking</li> <li>Creating and managing work items</li> <li>Using Azure Boards for project management and planning</li> <li>Integrating with other project management tools</li> </ul> </li> <li>Continuous Integration and Delivery with Azure DevOps<ul> <li>Overview of Azure DevOps CI/CD options</li> <li>Creating and managing build pipelines</li> <li>Configuring automated deployments</li> <li>Implementing continuous testing</li> <li>Azure Artifacts</li> <li>learning YAML Schema</li> </ul> </li> <li>Testing in Azure DevOps<ul> <li>Overview of Azure DevOps testing options</li> <li>Using Azure Test Plans for manual and exploratory testing</li> <li>Settingup continuous testing with Azure Pipelines</li> <li>Integrating with other testing tools</li> </ul> </li> <li>Best Practices for Azure DevOps Deployments<ul> <li>Build, Test, &amp; deploy .NET Core apps to Azure</li> <li>Build, Test, &amp; deploy React JS apps to Azure</li> <li>Build, Test, &amp; deploy Blazor apps to Azure</li> <li>Build &amp; deploy Azure SQL database using [DACPAC]</li> <li>Validate and deploy ARM Templates</li> <li>Git Clone, Code review with Pull request</li> <li>Private Agent configuration, Approvals, Artifact</li> <li>Release variables, Replace Tokens</li> <li>Security setup in Azure DevOps</li> </ul> </li> </ol>"},{"location":"techstack/azure-devops/#books","title":"Books","text":""},{"location":"techstack/azure-devops/#references","title":"References","text":""},{"location":"techstack/blazor/","title":"Blazor","text":"<p>If you are new to Blazor and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become Blazor developer:</p>"},{"location":"techstack/blazor/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Blazor<ul> <li>What is Blazor</li> <li>Benefits of using Blazor</li> </ul> </li> <li>Getting started with Blazor<ul> <li>Setting up a Blazor development environment</li> <li>Creating a Blazor application</li> <li>Understanding the structure of a Blazor application</li> </ul> </li> <li>Building user interfaces with Blazor<ul> <li>Defining and rendering components</li> <li>Working with JSX</li> <li>Using Blazor templates</li> </ul> </li> <li>Managing application state with Blazor<ul> <li>Introduction to component state and lifecycle</li> <li>Using component parameters and events</li> <li>Sharing state between components</li> </ul> </li> <li>Data access in Blazor<ul> <li>Introduction to Entity Framework and LINQ</li> <li>Querying and updating data with Entity Framework Core</li> <li>Using dependency injection to manage data access</li> </ul> </li> <li>Routing in Blazor applications<ul> <li>Introduction to Blazor routing</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"techstack/blazor/#books","title":"Books","text":"<p>Here are some recommended books for learning about Blazor, Microsoft's web framework:</p> <ol> <li>Blazor: A Beginner's Guide</li> <li>Programming Blazor: Building Web Applications in .NET</li> <li>Blazor Revealed: Building Web Applications in .NET</li> <li>Mastering Blazor: Tips and Tricks for Building Web Applications</li> </ol>"},{"location":"techstack/blazor/#references","title":"References","text":"<ul> <li>Blazor: Getting Started - MSDN</li> <li>Introduction to ASP.NET Core Blazor - Blazor Overview MSDN</li> <li>ASP.NET Core Blazor tutorials - MSDN</li> <li>Blazor: Getting Started - Pluralsight Course</li> <li>Blazor tutorial</li> <li>Blazor Server vs. Blazor WebAssembly - Article</li> </ul>"},{"location":"techstack/c-sharp/","title":"C# Programming language","text":"<p>If you are new to C# and want to learn and become a C# developer, here is a list of topics you need to know to become <code>C# programming developer</code>:</p>"},{"location":"techstack/c-sharp/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to C#:<ul> <li>What is C#</li> <li>Why use C#</li> <li>History of C#</li> </ul> </li> <li>Getting Started with C#:<ul> <li>Setting up a development environment</li> <li>Creating your first C# program</li> <li>Understanding the structure of a C# projects</li> </ul> </li> <li>Variables and Data Types:<ul> <li>Understanding variables</li> <li>Data types in C#</li> <li>Declaring and initializing variables</li> <li>Converting between data types</li> </ul> </li> <li>Control Structures:<ul> <li>Conditional statements (if/else)</li> <li>Loops (for, while, do while)</li> <li>Switch statements</li> </ul> </li> <li>Functions:<ul> <li>Understanding functions</li> <li>Creating and calling functions</li> <li>Return values and parameters</li> </ul> </li> <li>Object-Oriented Programming in C#:<ul> <li>Understanding objects and classes</li> <li>Creating and using objects</li> <li>Inheritance and polymorphism</li> <li>Encapsulation and access modifiers</li> </ul> </li> <li>Arrays and Collections:<ul> <li>Understanding arrays</li> <li>Declaring and using arrays</li> <li>Understanding collections</li> <li>Using lists and dictionaries</li> </ul> </li> <li>Debugging and Exception Handling:<ul> <li>Debugging techniques in Visual Studio</li> <li>Understanding exceptions and errors</li> <li>Using try/catch blocks to handle exceptions</li> </ul> </li> <li>Advanced Topics:<ul> <li>File Input/Output</li> <li>LINQ</li> <li>Delegates and events</li> <li>Asynchronous programming</li> </ul> </li> <li>Building Applications with C#:<ul> <li>Windows Forms applications</li> <li>ASP.NET web applications</li> <li>Mobile applications with Xamarin</li> <li>Games with Unity</li> </ul> </li> </ol>"},{"location":"techstack/c-sharp/#books","title":"Books","text":"<p>Here are some recommended books for learning C#:</p> <ol> <li>C# 9.0 in a Nutshell: The Definitive Reference</li> <li>Pro C# 9 with .NET 5</li> <li>Head First C#</li> <li>C# in Depth</li> <li>The C# Player's Guide</li> <li>Effective C#: 50 Specific Ways to Improve Your C#</li> <li>CLR via C#</li> <li>Beginning C# 7 Programming with Visual Studio 2017</li> </ol>"},{"location":"techstack/c-sharp/#references","title":"References","text":""},{"location":"techstack/helm/","title":"Helm","text":"<p>If you are new to Helm and want to learn and become a <code>DevSecOps Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/helm/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Helm<ul> <li>What is Helm?</li> <li>Benefits of using Helm</li> </ul> </li> <li>Getting started with Helm<ul> <li>Installing Helm</li> <li>Initializing Helm </li> </ul> </li> <li>Working with Helm charts<ul> <li>Finding and downloading charts from the Helm chart repository</li> <li>Creating and deploying your own charts</li> <li>Upgrading and rolling back chart deployments</li> </ul> </li> <li>Managing dependencies with Helm<ul> <li>Using Helm to manage the dependencies of a chart</li> <li>Sharing charts and chart dependencies with the Helm chart repository</li> </ul> </li> <li>Collaborating with Helm<ul> <li>Using Helm with version control systems</li> <li>Collaborating with team members using Helm</li> </ul> </li> <li>Advanced Helm features<ul> <li>Using Helm with continuous delivery pipelines</li> <li>Customizing Helm behavior with hooks</li> <li>Extending Helm with plugins</li> </ul> </li> </ol>"},{"location":"techstack/helm/#books","title":"Books","text":""},{"location":"techstack/helm/#references","title":"References","text":""},{"location":"techstack/kubernetes/","title":"Kubernetes","text":"<p>If you are new to Kubernetes and want to learn and become a <code>Certified Kubernetes Administrator (CKA)</code>, here is a list of topics you need to know to build and deploy application in Kubernetes. </p>"},{"location":"techstack/kubernetes/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction to AKS<ul> <li>What is AKS</li> <li>Key features and benefits</li> </ul> </li> <li>Kubernetes Architecture Components<ul> <li>Kubernetes \u2500 Master Machine Components</li> <li>Kubernetes \u2500 Node Components</li> </ul> </li> <li>Creating AKS cluster<ul> <li>Creating an AKS cluster using the Azure portal</li> <li>Creating an AKS cluster using Terraform</li> <li>Testing cluster connection &amp; creating namespace</li> <li>Configuring node pools and scaling</li> </ul> </li> <li>Deploying applications to AKS<ul> <li>Creating a container image</li> <li>Pushing the image to a container registry</li> <li>Deploying the application to AKS</li> <li>Managing and scaling an AKS cluster</li> </ul> </li> <li>Networking in AKS<ul> <li>Overview of AKS networking options</li> <li>Managing virtual networks and subnets</li> <li>Load balancing and traffic management</li> </ul> </li> <li>Monitoring and Management in AKS<ul> <li>Upgrading AKS clusters</li> <li>Scaling the number of nodes in an AKS cluster</li> <li>Monitoring and logging AKS clusters</li> </ul> </li> <li>Deploying k8s ingress controller</li> <li>Adding TLS/SSL to the ingress</li> <li>K8s horizontal pod autoscaler [HPA]<ul> <li>K8s horizontal pod autoscaler [HPA]</li> <li>HPA in action</li> <li>AKS cluster autoscaling</li> </ul> </li> <li>Integrating AKS with Azure Monitor</li> <li>AKS Storage and Networks</li> <li>AKS storage overview<ul> <li>Creating storage classes</li> <li>Storage: Persistent claims</li> <li>Shared volumes</li> <li>Create resource for shared volume</li> <li>Challenge: Lost volumes</li> <li>Solution: Find and remove PVs</li> <li>Networking and AKS</li> <li>Load balancing and Ingress: Setup</li> </ul> </li> </ul>"},{"location":"techstack/kubernetes/#books","title":"Books","text":""},{"location":"techstack/kubernetes/#references","title":"References","text":""},{"location":"techstack/net-core-web-api/","title":".NET Core Web API","text":"<p>If you are new to Restful services and want to learn and become a <code>API developer</code>, here is a list of topics you need to know to become API developer:</p>"},{"location":"techstack/net-core-web-api/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to .NET Core Web API:<ul> <li>What is a .NET Core Web API</li> <li>Why use .NET Core for building web APIs</li> <li>The benefits of using .NET Core</li> </ul> </li> <li>Setting up the Development Environment:<ul> <li>Installing the .NET Core SDK</li> <li>Installing Visual Studio Code or Visual Studio</li> <li>Setting up the project structure</li> </ul> </li> <li>Creating Your First Web API:<ul> <li>Creating a new project</li> <li>Defining the API endpoints</li> <li>Implementing the API logic</li> <li>Testing the API</li> </ul> </li> <li>Routing and Controller:<ul> <li>Understanding routing in .NET Core Web API</li> <li>Creating controllers and defining endpoints</li> <li>Implementing HTTP verbs (GET, POST, PUT, DELETE)</li> <li>Handling request and response data</li> </ul> </li> <li>Data Access with Entity Framework Core:<ul> <li>Understanding Entity Framework Core</li> <li>Setting up the database and the context</li> <li>Defining entities and relationships</li> <li>Querying data with LINQ</li> </ul> </li> <li>Authentication and Authorization:<ul> <li>Understanding authentication and authorization</li> <li>Implementing authentication with JWT</li> <li>Implementing authorization with policies</li> </ul> </li> <li>Deployment and Hosting:<ul> <li>Deploying the API to Azure or another cloud provider</li> <li>Hosting the API on IIS or a reverse proxy server</li> <li>Securing the API with SSL</li> </ul> </li> <li>Advanced Topics:<ul> <li>Versioning the API</li> <li>Documentation with Swagger</li> <li>Logging and exception handling</li> <li>Performance optimization and caching</li> </ul> </li> </ol>"},{"location":"techstack/net-core-web-api/#books","title":"Books","text":"<p>Here are some recommended books for learning about .NET Core Web API:</p> <ol> <li>Pro ASP.NET Core 3</li> <li>ASP.NET Core in Action</li> <li>ASP.NET Core 5 and Angular</li> <li>Building RESTful Web APIs with ASP.NET Core</li> <li>Pro ASP.NET Core MVC 2</li> <li>ASP.NET Core Web API</li> </ol>"},{"location":"techstack/net-core-web-api/#references","title":"References","text":"<ul> <li>Download .NET SDKs for Visual Studio - .NET / .NET Core Runtimes</li> <li>ASP.NET Core fundamentals - Fundamentals</li> <li>Dependency injection in ASP.NET Core - Dependency injection</li> <li>Routing in ASP.NET Core - Routing</li> <li>Azure Key Vault configuration provider in ASP.NET Core - Azure Key Vault</li> <li>Serverless Microservices reference architecture - GitHub Link</li> <li>RESTful web API design - RESTful API</li> <li>Web API implementation - RESTful API</li> <li>Build RESTful APIs with ASP.NET Web API - RESTful API</li> <li>Tutorial: Create a web API with ASP.NET Core - MSDN</li> <li>Customizing Swagger Responses for Better API Documentation - medium</li> <li>REST API Tutorial - REST API</li> <li>Best practices for REST API design - Best practices</li> <li>RESTful API Standard (YARAS) - Introduction- Best Practices</li> <li>10 Best Practices for Better RESTful API - Best Practices</li> <li>RESTful API Design: 13 Best Practices - Best Practices</li> <li>FluentValidation in ASP.NET Core - FluentValidation</li> <li>Using Fluent Validation in ASP.NET Core - FluentValidation</li> <li>FluentValidation - FluentValidation</li> <li>Fluent Validation in an ASP.NET Core Web API - FluentValidation</li> <li>Automatically set appsettings.json for dev and release environments in asp.net core? - appsettings</li> <li>Setting up Swagger to support versioned API endpoints in ASP.NET Core - API Versioning</li> <li>NLog - Handling errors</li> <li>Handling errors in an ASP.NET Core Web API- Handling errors</li> <li>Handle errors in ASP.NET Core- Handling errors</li> <li>Serializing enums as strings using System.Text.Json library in .NET Core 3.0 - Serializing enums</li> <li>Adding Newtonsoft JSON serialization and deserialization in ASP.NET Core - Newtonsoft.Json</li> <li>Using Newtonsoft.Json In .NET Core 3+ Projects - Newtonsoft.Json</li> <li>ASP.NET MVC Core API Serialize Enums to String - Serialize</li> <li>Add an authorization header to your swagger-ui with Swashbuckle - Authorization</li> <li>JWT Authentication In ASP.NET Core - Authorization</li> <li>Token Authentication in ASP.NET Core 2.0 - A Complete Guide - Authorization</li> <li>ASP.NET Core 3.1 - Role Based Authorization Tutorial with Example API - Authorization</li> <li>Learn Entity Framework Core</li> <li>Introducing .NET 5</li> <li>Logging to Azure Blob Storage</li> </ul>"},{"location":"techstack/node-js/","title":"Node JS","text":"<p>If you are new to Node JS and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become Node JS developer:</p>"},{"location":"techstack/node-js/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Node.js<ul> <li>What is Node.js</li> <li>Why use Node.js</li> <li>History of Node.js</li> <li>Key features of Node.js</li> </ul> </li> <li>Setting up a Development Environment<ul> <li>Installing Node.js</li> <li>Setting up a development environment</li> <li>Understanding npm (Node Package Manager)</li> </ul> </li> <li>Understanding Node.js Fundamentals<ul> <li>Understanding event-driven programming</li> <li>Understanding non-blocking I/O</li> <li>Understanding modules and require</li> <li>Understanding the Node.js runtime environment</li> </ul> </li> <li>Building a Simple Web Server with Node.js<ul> <li>Introduction to Express</li> <li>Setting up a basic Express server</li> <li>Handling HTTP requests and responses</li> <li>Serving static files</li> </ul> </li> <li>Working with Data in Node.js<ul> <li>Understanding the basics of databases</li> <li>Connecting to a database in Node.js</li> <li>Querying data in Node.js</li> <li>Storing data in Node.js</li> </ul> </li> <li>Building Advanced Applications with Node.js<ul> <li>Understanding middleware</li> <li>Creating a REST API</li> <li>Authenticating and securing applications</li> <li>Using WebSockets for real-time communication</li> <li>Deploying Node.js applications</li> </ul> </li> </ol>"},{"location":"techstack/node-js/#books","title":"Books","text":"<p>Here's a list of recommended books for Node.js:</p> <ol> <li>Node.js Design Patterns</li> <li>Node.js in Action</li> <li>Learning Node: Moving to the Server-Side</li> <li>Node.js 8 the Right Way: Practical, Server-Side JavaScript That Scales</li> <li>Professional Node.js: Building Javascript Based Scalable Software</li> </ol>"},{"location":"techstack/node-js/#references","title":"References","text":"<p>Here are some online resources and tutorials to learn Node.js:</p> <ol> <li> <p>Official Node.js Documentation: Node.js Documentation</p> </li> <li> <p>Node.js Beginner's Guide: Node.js Beginner's Guide</p> </li> <li> <p>Node.js Tutorials on W3Schools: W3Schools Node.js Tutorial</p> </li> <li> <p>Node.js Courses on Udemy:</p> </li> <li>Node.js - The Complete Guide (MVC, REST APIs, GraphQL, Deno)</li> <li>Node.js, Express, MongoDB &amp; More: The Complete Bootcamp 2021</li> </ol>"},{"location":"techstack/react-js/","title":"React JS","text":"<p>If you are new to React JS and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become React JS developer:</p>"},{"location":"techstack/react-js/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to React<ul> <li>What is React</li> <li>Why use React</li> </ul> </li> <li>Getting started with React<ul> <li>Setting up a React development environment</li> <li>Creating a React application</li> <li>Understanding the structure of a React application</li> </ul> </li> <li>Working with React components<ul> <li>Defining and rendering components</li> <li>Props and state in components</li> <li>Lifecycle methods in components</li> </ul> </li> <li>Building user interfaces with React<ul> <li>Working with JSX</li> <li>Using React hooks</li> <li>Managing data with context</li> </ul> </li> <li>Managing application state with Redux<ul> <li>Introduction to Redux</li> <li>Setting up Redux in a React application</li> <li>Working with actions, reducers, and the store</li> </ul> </li> <li>Routing in React applications<ul> <li>Introduction to React Router</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"techstack/react-js/#books","title":"Books","text":"<p>Here are some recommended books for learning React JS:</p> <ol> <li>Learning React: A Hands-On Guide to Building Web Applications Using React and Redux</li> <li>React - Up &amp; Running: Building Web Applications</li> <li>Fullstack React: The Complete Guide to ReactJS and Friends</li> <li>The Road to React: Your Journey to Master Plain Yet Pragmatic React.js</li> <li>React Design Patterns and Best Practices</li> </ol>"},{"location":"techstack/react-js/#references","title":"References","text":"<ul> <li>React JS - Official website</li> <li>React JS Tutorials - Official website</li> <li>Getting Started - Official website</li> <li>Create React App</li> <li>Reactjs code snippets - code snippets</li> <li>TypeScript</li> <li>Webpack</li> <li>Getting started with React Router - Router</li> <li>REACT TRAINING / REACT ROUTER - Router</li> <li>Redux Fundamentals - Redux</li> <li>Build and deploy a Node.js API - Node.js API</li> <li>Getting Started with Redux- Redux</li> <li>How to fetch data from Api in Redux using Redux -thunk- Redux-thunk</li> <li>Best react courses - 7 Best Online React courses</li> <li>Learn ReactJS and build a simple CRUD app- React Tutorial</li> <li>React-redux-registration-login-example -registration login</li> <li>Using NSwag to Generate React Client for an ASP.NET Core 3 API- NSwag</li> <li>Build and deploy a Node.js API app in Azure App Service</li> </ul>"},{"location":"techstack/tech-stack/","title":"Tech stack","text":"<p>Here is a list of technologies that I frequently use and support, here you will also see the list of topics from each technology so that you can learn them offline when needed.</p> <p>You may encounter some specific tools and technologies that are not listed here but you will see the references for further learning.</p>"},{"location":"techstack/tech-stack/#technologies","title":"Technologies","text":"<ul> <li>C# Programming language</li> <li>.NET Core Web API</li> <li>React JS</li> <li>Blazor</li> <li>Node.js</li> <li>Azure DevOps</li> <li>Azure Cloud</li> <li>Kubernetes Cluster</li> <li>Terraform </li> <li>Argocd</li> <li>Helm Chart</li> <li>Application Testing</li> <li>Networking troubleshooting Tools</li> </ul>"},{"location":"techstack/tech-stack/#development-tools","title":"Development Tools","text":"<p>Here is the list of tools that are commonly used in the development of software applications. </p> <ul> <li>Visual Studio Code</li> <li>SQL Server Management Studio, SQL Profiler</li> <li>Node JS, NPM, Notepad++</li> <li>Postman, SOAP UI</li> <li>Browser Developer Tools</li> <li>Agile &amp; Scrum with JIRA or Azure Board</li> <li>Nuget Manager, GitHub Desktop</li> <li>Open SSL</li> <li>JSON Viewer/Formatter, JWT debugger, SAML-Tracer</li> <li>Azure Storage Explorer</li> <li>Visual Studio 2022/2019 [optional]</li> </ul>"},{"location":"techstack/terraform/","title":"Terraform","text":"<p>If you are new to Terraform and want to learn and become a <code>Infrastructure as Code (IaC) Cloud Engineer</code>, here is a list of topics you need to know to create infrastructure in Cloud. </p>"},{"location":"techstack/terraform/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction<ul> <li>What is Terraform?</li> <li>Why use Terraform?</li> <li>Key concepts</li> </ul> </li> <li>Setting up Terraform<ul> <li>Installing Terraform</li> <li>Configuring Terraform</li> <li>Creating your first Terraform configuration</li> </ul> </li> <li>The Terraform Configuration Language (HashiCorp Configuration Language, HCL)<ul> <li>Syntax and structure</li> <li>Variables</li> <li>Outputs</li> <li>Modules</li> </ul> </li> <li>Providers<ul> <li>Overview of available providers</li> <li>Managing multiple providers</li> <li>Using provider-specific resources</li> </ul> </li> <li>Resource Types<ul> <li>Overview of supported resource types</li> <li>Creating, updating, and destroying resources</li> <li>Importing existing resources into Terraform</li> </ul> </li> <li>Modules<ul> <li>Creating and using modules</li> <li>Sharing modules with others</li> </ul> </li> <li>Workspaces<ul> <li>Managing multiple environments with workspaces</li> <li>Workspace management commands</li> </ul> </li> <li>State Management<ul> <li>Understanding Terraform state</li> <li>Backing up and sharing state files</li> </ul> </li> <li>Advanced Terraform Features<ul> <li>Input and output variables</li> <li>Loops and conditions</li> <li>Data sources</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable configurations</li> <li>Managing state effectively</li> <li>Collaborating with others</li> </ul> </li> <li>Troubleshooting<ul> <li>Common issues and errors</li> <li>Debugging techniques</li> <li>Tips for avoiding common mistakes</li> </ul> </li> </ol>"},{"location":"techstack/terraform/#books","title":"Books","text":""},{"location":"techstack/terraform/#references","title":"References","text":""},{"location":"techstack/testing/","title":"QA Automation","text":"<p>If you are new to QA Automation and want to learn and become a <code>QA Automation Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/testing/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Unit and Integration Testing<ul> <li>Overview of testing in software development</li> <li>Importance of unit and integration testing</li> <li>Understanding the differences between unit and integration testing</li> </ul> </li> <li>Setting up a Testing Environment<ul> <li>Understand testing tools and frameworks</li> <li>Installing and configuring testing tools</li> <li>Creating a testing project </li> <li>setting up testing frameworks</li> </ul> </li> <li>Unit Testing <ul> <li>Understand unit testing technologies</li> <li>Writing unit tests using MSTest, NUnit, and XUnit, Moq &amp; AutoFixure.</li> <li>Debugging and troubleshooting unit tests</li> <li>Managing and organizing unit tests</li> </ul> </li> <li>Integration Testing<ul> <li>Understanding integration testing technologies</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting integration tests</li> <li>Integrating with other testing tools and frameworks</li> <li>Create BDDs with SpectFlow</li> </ul> </li> <li>Functional Testing<ul> <li>Understanding Functional testing technologies</li> <li>Writing Functional tests</li> <li>Debugging and troubleshooting Functional tests</li> </ul> </li> <li>Testing ASP.NET Core Web API <ul> <li>Writing unit tests</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting tests</li> </ul> </li> <li>Testing React JS application<ul> <li>Writing UI test with Selenium</li> <li>Use Jest for react js test cases</li> </ul> </li> <li>Automating Testing with Continuous Integration/Continuous Deployment (CI/CD)<ul> <li>Setting up a CI/CD pipeline</li> <li>Automating Unit tests with CI/CD</li> <li>Automating Integration tests with CI/CD</li> <li>Automating Functional tests with CI/CD</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable tests</li> <li>Implementing code coverage and test coverage analysis</li> <li>Managing testing data and test environment</li> <li>Implementing testing as part of the development process</li> <li>Review test results</li> <li>Test Analytics</li> <li>Review &amp; Report code coverage results</li> <li>API Automation testing using SOAP UI</li> <li>Create Regression test suite</li> <li>Create Smoke / Sanity test suite</li> <li>Performance testing- to ensure that it can handle a high load of users and data.</li> </ul> </li> </ol>"},{"location":"techstack/testing/#books","title":"Books","text":""},{"location":"techstack/testing/#references","title":"References","text":""}]}