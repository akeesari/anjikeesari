{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"blogs/","title":"Blogs","text":"<p>Welcome to our blog page! Here, you'll find a collection of informative and insightful articles covering various topics related to the Microsoft technology stack. Our blog posts aim to provide valuable knowledge, tutorials, and best practices to help you navigate the world of Microsoft technologies.</p>"},{"location":"blogs/#latest-blog-posts","title":"Latest Blog Posts","text":"<p>Below are our most recent blog posts:</p> <p>Stay tuned for regular updates as we continue to share valuable content to help you stay at the forefront of Microsoft technologies!</p>"},{"location":"about/","title":"Cloud Architect, Researcher &amp; Technology Leader","text":""},{"location":"about/#building-the-future-of-cloud-computing","title":"Building the Future of Cloud Computing","text":"<p>I'm Anji Keesari, a Cloud Architect and technology leader with over 23 years of experience transforming how organizations build and operate in the cloud. Currently based in San Ramon, California, I combine deep technical expertise in Microsoft Azure, Kubernetes, AI/ML, and Infrastructure as Code with a passion for innovation, team empowerment, and driving industry-wide change through knowledge sharing.</p> <p>What excites me most is not just building infrastructure, but reimagining how technology can solve complex business challenges, mentoring teams to reach their full potential, and exploring emerging paradigms like AI agents and intelligent automation that will shape the next decade of cloud computing.</p> <ul> <li> <p>\ud83c\udfd7\ufe0f Technical Leadership</p> <p>As a Cloud Architect in the financial services sector, I lead the technical vision for enterprise cloud transformation:</p> <ul> <li>Championing cloud-native thinking across organizations</li> <li>Pioneering adoption of emerging technologies (AI agents, Azure AI Foundry, LLMs)</li> <li>Architecting scalable, event-driven platforms</li> <li>Building proof-of-concepts that demonstrate business value</li> <li>Balancing innovation with stability and security</li> </ul> </li> <li> <p>\ud83d\udcda Research &amp; Publications</p> <p>Published peer-reviewed research in international journals:</p> <ul> <li> <p>AI-Driven Algorithmic Trading (IJECS, 2025)   Machine learning integration with technical indicators</p> </li> <li> <p>Cloud-Native Messaging Architectures (IJARCSMS, 2025)   High-throughput Kubernetes and Azure Event Hub analysis</p> </li> </ul> <p>Research Focus: Cloud architecture, AI/ML integration, distributed systems, algorithmic trading</p> <p>View Research \u2192</p> </li> <li> <p>\ud83e\udd1d Team Empowerment</p> <p>Building high-performing teams through mentorship and collaboration:</p> <ul> <li>Mentoring developers on cloud-native patterns and IaC</li> <li>Building bridges between dev, ops, and security teams</li> <li>Creating collaborative problem-solving cultures</li> <li>Translating technical concepts for executive stakeholders</li> <li>Connecting technology decisions to business outcomes</li> </ul> </li> <li> <p>\ud83c\udf0d Knowledge Sharing</p> <p>Operating anjikeesari.com for 7+ years as a free educational platform:</p> <ul> <li>87 countries global reach</li> <li>80+ technical articles on Azure, Kubernetes, AI/ML</li> <li>2 free e-books downloaded by thousands</li> <li>2,300+ YouTube subscribers</li> <li>243 Medium followers</li> </ul> <p>Democratizing enterprise cloud expertise worldwide</p> </li> <li> <p>\ud83c\udfc6 Professional Recognition</p> <p>Distinguished memberships and judging roles:</p> <ul> <li>IEEE Member - Institute of Electrical and Electronics Engineers</li> <li>SAS Fellow - International Society for Academic &amp; Scientific Society</li> <li>IOASD Royal Fellow - International Organization for Academic Development</li> <li>Globee Awards Judge (2025) - Impact, Leaders, Business</li> <li>Stevie Awards Member - Technology and innovation focus</li> </ul> <p>View Awards \u2192</p> </li> <li> <p>\ud83d\ude80 Innovation Focus</p> <p>Exploring cutting-edge technologies and emerging paradigms:</p> <ul> <li>AI agents and AI Agent Frameworks for cloud operations</li> <li>Azure AI Foundry and Azure OpenAI Service integration</li> <li>Large Language Models (LLMs) and Model Context Protocol (MCP) servers</li> <li>Infrastructure as Code best practices (Terraform, Bicep)</li> <li>Kubernetes with ArgoCD and Helm Charts for GitOps workflows</li> </ul> </li> </ul>"},{"location":"about/#core-technical-expertise","title":"Core Technical Expertise","text":"<p>Cloud Architecture &amp; Platforms: Microsoft Azure \u2022 Azure Kubernetes Service (AKS) \u2022 Azure OpenAI Service \u2022 Azure AI Foundry \u2022 Azure Event Hub \u2022 Infrastructure as Code</p> <p>AI/ML &amp; Emerging Technologies: Artificial Intelligence \u2022 Machine Learning \u2022 AI Agents \u2022 AI Agent Frameworks \u2022 Large Language Models (LLMs) \u2022 Model Context Protocol (MCP) Servers \u2022 Intelligent Automation</p> <p>Container Orchestration &amp; Microservices: Kubernetes \u2022 Docker \u2022 ArgoCD \u2022 Helm Charts \u2022 Microservices Architecture \u2022 Event-Driven Systems</p> <p>Development &amp; Automation: Infrastructure as Code (IaC) \u2022 Terraform \u2022 Bicep \u2022 ARM Templates \u2022 CI/CD \u2022 Azure DevOps \u2022 C# \u2022 .NET \u2022 React JS</p> <p>Enterprise Solutions: Cloud-Native Applications \u2022 Distributed Systems \u2022 High-Throughput Architectures \u2022 Security &amp; Compliance (SOC 2, FINRA, SEC)</p> <p></p>"},{"location":"about/#professional-philosophy","title":"Professional Philosophy","text":"<p>\"The best architectures solve today's problems while preparing for tomorrow's opportunities. But technical excellence alone isn't enough\u2014I believe in multiplying impact by sharing what I learn freely. Every tutorial I write, every pattern I document, and every engineer I mentor creates ripples that extend far beyond any single project. That's how we advance not just our careers, but our entire field.\"</p>"},{"location":"about/#beyond-technology","title":"Beyond Technology","text":"<p>During free time, I find joy in playing soccer, hiking through California trails, exploring new places, and most importantly, spending quality time with family and loved ones. These activities provide balance and fresh perspectives that fuel creativity in problem-solving.</p>"},{"location":"about/#lets-connect","title":"Let's Connect","text":"<p>Interested in cloud architecture, AI/ML integration, or collaboration opportunities? Get in touch \u2192</p>"},{"location":"about/accomplishments/","title":"Technical Accomplishments","text":""},{"location":"about/accomplishments/#technical-delivery-highlights","title":"Technical Delivery Highlights","text":""},{"location":"about/accomplishments/#cloud-infrastructure-provisioning-azure-terraform","title":"Cloud Infrastructure &amp; Provisioning (Azure + Terraform)","text":"<ul> <li> <p>Designed and implemented a Terraform foundation to automate infrastructure deployment across Azure Cloud and Azure DevOps, significantly streamlining provisioning processes.</p> </li> <li> <p>Built a Hub &amp; Spoke VNet model to isolate resources and improve network security for enterprise-grade applications.</p> </li> <li> <p>Provisioned Virtual Machines, Azure Bastion Host, Private Endpoints, Private Link, DNS Resolver, and Azure Storage Accounts using Terraform for secure communication and state management.</p> </li> <li> <p>Deployed and configured Azure Container Registry (ACR) for secure storage of private Docker images.</p> </li> </ul>"},{"location":"about/accomplishments/#networking-gateway-dns","title":"Networking, Gateway &amp; DNS","text":"<ul> <li> <p>Configured Azure Application Gateway as the public entry point with reverse proxy, SSL/TLS termination, load balancing, custom listeners, and backend pools to optimize request routing.</p> </li> <li> <p>Set up Azure DNS for new domains and managed DNS records via Terraform for seamless resolution across environments.</p> </li> </ul>"},{"location":"about/accomplishments/#kubernetes-containerization-aks-helm-gitops","title":"Kubernetes &amp; Containerization (AKS + Helm + GitOps)","text":"<ul> <li> <p>Deployed containerized microservices on Azure Kubernetes Service (AKS), ensuring scalability and high availability.</p> </li> <li> <p>Configured NGINX Ingress Controller, Persistent Volumes (PV), and Persistent Volume Claims (PVC) to support application routing and storage needs.</p> </li> <li> <p>Created custom Helm charts and used Helm hooks for deploying services like databases, jobs, and Keycloak containers.</p> </li> <li> <p>Deployed pgAdmin4, MinIO, Jaeger, and KEDA on AKS using Helm and Terraform for database management, object storage, observability, and autoscaling.</p> </li> </ul>"},{"location":"about/accomplishments/#cicd-devops-automation","title":"CI/CD &amp; DevOps Automation","text":"<ul> <li> <p>Developed YAML-based Azure DevOps pipelines for .NET Core, React, and Node.js applications, automating build and release workflows.</p> </li> <li> <p>Integrated Azure DevOps with ACR, AKS, and other services through service connections and DevOps library variable groups, pulling secrets securely from Azure Key Vault.</p> </li> <li> <p>Organized Git repositories and standardized code structure for microservices to improve team collaboration and code reuse.</p> </li> <li> <p>Implemented Helm rollback strategies to quickly recover from deployment issues with minimal downtime.</p> </li> </ul>"},{"location":"about/accomplishments/#gitops-application-lifecycle-management","title":"GitOps &amp; Application Lifecycle Management","text":"<ul> <li> <p>Installed and configured ArgoCD for automated GitOps deployment to AKS clusters.</p> </li> <li> <p>Registered AKS clusters with ArgoCD, created ArgoCD Projects, and deployed Helm charts and applications with lifecycle automation and RBAC enforcement.</p> </li> </ul>"},{"location":"about/accomplishments/#security-secrets-management","title":"Security &amp; Secrets Management","text":"<ul> <li> <p>Integrated Azure Key Vault with AKS using the CSI driver to manage secrets, keys, and certificates securely.</p> </li> <li> <p>Configured RBAC across Kubernetes and ArgoCD to enable fine-grained access control, aligning with security and compliance requirements.</p> </li> </ul>"},{"location":"about/accomplishments/#messaging-eventing-observability","title":"Messaging, Eventing &amp; Observability","text":"<ul> <li> <p>Configured Azure Event Hubs and Kafka topics for producer-consumer applications, enabling seamless asynchronous communication.</p> </li> <li> <p>Integrated OpenTelemetry with Jaeger to monitor .NET microservices, providing end-to-end tracing and performance insights.</p> </li> </ul>"},{"location":"about/accomplishments/#data-analytics","title":"Data &amp; Analytics","text":"<ul> <li> <p>Deployed PostgreSQL Flexible Server for backend databases and managed access via pgAdmin in Kubernetes.</p> </li> <li> <p>Created an Azure Synapse Analytics workspace using Terraform to enable advanced analytics and big data capabilities.</p> </li> </ul>"},{"location":"about/accomplishments/#architecture-best-practices","title":"Architecture &amp; Best Practices","text":"<ul> <li> <p>Contributed to the design of a multi-tenant architecture, enabling efficient resource segmentation for multiple clients within a shared AKS environment.</p> </li> <li> <p>Authored detailed technical documentation to support onboarding, knowledge sharing, and long-term project maintainability.</p> </li> <li> <p>Implemented architecture improvements, best practices, and coding standards, resulting in measurable gains in code quality, system performance, and operational resilience.</p> </li> </ul>"},{"location":"about/accomplishments/#project-delivery-highlights","title":"Project Delivery Highlights","text":"<ul> <li> <p>Successfully developed and delivered high-quality software solutions, meeting business requirements and exceeding performance expectations.</p> </li> <li> <p>Consistently met or exceeded project deadlines, ensuring timely delivery of critical features and releases.</p> </li> <li> <p>Implemented new features and enhancements that significantly improved application performance and user experience.</p> </li> <li> <p>Resolved critical bugs and production issues, contributing to stable, reliable, and secure software operations.</p> </li> </ul>"},{"location":"about/accomplishments/#automation-deployment","title":"Automation &amp; Deployment","text":"<ul> <li> <p>Championed a culture of automation by identifying and eliminating repetitive manual tasks, reducing operational overhead and resource costs.</p> </li> <li> <p>Automated deployment pipelines using scalable and secure CI/CD practices, improving delivery speed, quality, and reliability.</p> </li> <li> <p>Collaborated with cross-functional teams to design and implement enterprise-grade features, including environment monitoring, self-healing mechanisms, and autonomous delivery capabilities.</p> </li> </ul>"},{"location":"about/accomplishments/#code-quality-mentoring-collaboration","title":"Code Quality, Mentoring &amp; Collaboration","text":"<ul> <li> <p>Participated in code reviews, providing constructive feedback and promoting code quality, best practices, and team knowledge sharing.</p> </li> <li> <p>Mentored junior engineers, offering technical guidance and supporting their career development and onboarding.</p> </li> <li> <p>Contributed to the design and implementation of scalable architectural patterns, enabling higher team productivity and feature velocity.</p> </li> </ul>"},{"location":"about/accomplishments/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li> <p>Led the implementation of static code analysis tools, proactively identifying vulnerabilities before production deployment.</p> </li> <li> <p>Defined and enforced secure coding standards and security policies, aligning development practices with industry and regulatory requirements.</p> </li> <li> <p>Provided guidance on Secure Software Development Lifecycle (SSDLC) processes and best practices to ensure ongoing application integrity.</p> </li> <li> <p>Identified and remediated security vulnerabilities, ensuring all deliverables met the highest security and compliance standards.</p> </li> </ul>"},{"location":"about/accomplishments/#testing-reliability","title":"Testing &amp; Reliability","text":"<ul> <li>Implemented and improved testing strategies, including unit, integration, and automated testing, to enhance software reliability and reduce regressions.</li> </ul>"},{"location":"about/profile/","title":"Profile Summary","text":"<p>Accomplished Cloud Architect with over 20 years of experience in software architecture, full-stack development, and Microsoft Azure Cloud solutions. Skilled at designing and delivering scalable, secure, and cloud-native applications, with expertise in .NET, Azure, Kubernetes, Kafka, Terraform, Helm, and Azure DevOps. Proven track record in leading enterprise transformation initiatives and scaling systems from startup to enterprise level.</p> <ul> <li> <p>Current Role: Currently employed at AssetMark as a Cloud Architect, leading enterprise cloud transformation initiatives. Actively contributing to the eWM 3.0 project \u2013 an architectural foundation for the company's next generation of digital products. This modern cloud-native platform is designed to become the organizational standard for all future projects, driving consistency, scalability, and innovation across teams.</p> </li> <li> <p>Previous Experience: Spent 12 years with Tata Consultancy Services (TCS), delivering large-scale software solutions across diverse domains and geographies, including the US, UK, and India (multiple states).</p> </li> <li> <p>Certifications: Microsoft Azure, .NET, and  The Open Group Architecture Framework TOGAF certified.</p> </li> <li> <p>Publications: Author of two technical books:</p> <ul> <li> <p>Building Microservices with Containers: A Practical Guide</p> </li> <li> <p>Building Scalable Kubernetes Infrastructure for Microservices</p> </li> </ul> </li> <li> <p>Entrepreneurship: Founder of https://anjikeesari.com, where I share insights and tutorials on a wide range of technologies, including Azure, Kubernetes, PostgreSQL, MongoDB, Terraform, Helm, Azure DevOps, .NET, React, and more.</p> </li> <li> <p>Content Creation: Regular contributor to Medium.com, where I publish articles on emerging cloud-native technologies and best practices.</p> </li> <li> <p>Startup Mindset: Passionate about startup culture, with a proven ability to scale organizations from early-stage to enterprise level. Leverage deep technical expertise and architectural vision to create scalable, resilient, and future-ready solutions that support rapid growth and innovation.</p> </li> <li> <p>AI Exploration: Currently exploring the AI ecosystem to identify strategic opportunities for incorporating intelligent systems into scalable architectures, ensuring alignment with business value and emerging technologies.</p> </li> <li> <p>Azure Expertise: Extensive experience provisioning and managing Azure PaaS services (App Services, API Apps, Logic Apps, API Management, Application Gateway, Azure SQL Server, Redis Cache, Azure Traffic Manager, Virtual Networks, and NSGs) using Terraform for fully automated infrastructure deployment. Implemented CI/CD pipelines to ensure continuous integration and delivery of cloud services, optimizing deployment processes.</p> </li> <li> <p>CI/CD Expertise: Proficient in designing and implementing CI/CD pipelines using Azure DevOps, supporting deployment to both on-premises and Azure Cloud for websites, APIs, and database projects.</p> </li> <li> <p>Security Focus: Led the integration of CrowdStrike with Microsoft Azure Cloud to improve threat detection and response. Worked with Microsoft Defender for Cloud to enhance security, manage vulnerabilities, and ensure compliance across hybrid environments.</p> </li> <li> <p>Ability to adapt to new technologies, learn quickly, and understand relevant cloud trends.</p> </li> </ul>"},{"location":"about/profile/#technical-expertise-current","title":"Technical Expertise (current)","text":"<p>End-to-end expertise in building and managing microservices-based architectures on the cloud. Experience spans the complete application lifecycle\u2014from infrastructure provisioning to deployment, monitoring, security, and data management for Cloud-Native Applications.</p> <ul> <li> <p>Infrastructure Provisioning: Proficient in provisioning Azure resources using Terraform, enabling automated, repeatable, and scalable infrastructure deployment.</p> </li> <li> <p>Containerization: Skilled in building microservices using Docker containers, promoting modular, portable, and isolated service development.</p> </li> <li> <p>Kubernetes Configuration: Experienced in creating Kubernetes manifest objects using Helm charts, allowing efficient templating and management of complex application deployments.</p> </li> <li> <p>GitOps Deployment: Deploys microservices to Kubernetes clusters using GitOps methodologies with tools like ArgoCD, ensuring declarative, version-controlled, and automated deployment pipelines.</p> </li> <li> <p>CI/CD with Azure DevOps: Expertise in designing and implementing robust CI/CD pipelines using Azure DevOps, supporting end-to-end automation for code integration, testing, and deployment across both on-premises and Azure environments. Proficient in managing build and release pipelines, infrastructure as code, and multi-stage deployments.</p> </li> <li> <p>Monitoring &amp; Observability: Implements observability solutions using OpenTelemetry (OTel) Collector, providing comprehensive tracing, metrics, and logging for distributed applications.</p> </li> <li> <p>Event-Driven Architecture: Designs and implements event-driven messaging patterns using Apache Kafka and Azure Event Hub, enabling scalable and decoupled communication between microservices.</p> </li> <li> <p>Database Management: Manages databases such as PostgreSQL and MongoDB, ensuring data consistency, high availability, and performance across both relational and NoSQL systems.</p> </li> <li> <p>Cloud Security: Strong focus on cloud-native security best practices, including role-based access control (RBAC), secrets management, secure API gateways, network policies, and integration with tools like Microsoft Defender for Cloud and CrowdStrike for threat detection and compliance.</p> </li> <li> <p>Architectural Leadership: Demonstrated leadership in shaping the architectural landscape of organizations by driving cloud adoption, defining best practices, mentoring engineering teams, and leading enterprise-scale modernization and digital transformation initiatives. Passionate about building future-ready platforms that are scalable, resilient, and aligned with business goals.</p> </li> </ul>"},{"location":"about/profile/#technical-expertise-earlier","title":"Technical Expertise (earlier)","text":"<ul> <li> <p>Extensive experience in designing and developing enterprise cloud-based applications using .NET Core Web APIs with Microservice clean architecture, .NET Framework 4.8, C#, React JS, jQuery, WCF, Entity Framework Core, and n-Tier and Client Server Architecture.</p> </li> <li> <p>Proficient in creating Azure DevOps build and release pipelines to host different applications like Websites, APIs, and Database projects on-premises and in Azure cloud.</p> </li> <li> <p>Skilled in provisioning Azure Cloud PaaS and IaaS services, including Azure App Services Web App, API App, Logic App, App Service Environment (ILB ASEv2), App Service Plan, API Management, Application Gateway, Azure SQL Server, Redis Cache, Azure Traffic Manager, Virtual Network, and Network Security Group (NSG).</p> </li> <li> <p>Experience in implementing ASP.NET Identity Management for user Authentication and Authorization in web-based applications.</p> </li> <li> <p>Successful migration of Microsoft .NET-based applications from On-Premises to Azure Cloud PaaS model.</p> </li> <li> <p>Established effective DevOps development processes and proficient in Continuous Integration (CI), Continuous Deliver (CD), and build/release using Microsoft release pipelines for ARM templates, PowerShell scripts, Terraform, and .NET applications.</p> </li> <li> <p>Skilled in Terraform configuration, IaC for provisioning Azure IaaS and PaaS services and automating deployment using Azure DevOps CI/CD Pipeline.</p> </li> <li> <p>Proficient in Azure Active Directory (AAD) and application configuration for Authentication, managed Identity, and service principles.</p> </li> <li> <p>Set up Azure B2C for OAuth-based Single Sign-On (SSO) with Microsoft MFA users, experience with Okta</p> </li> <li> <p>Experience in purchasing and configuring DNS custom domains and SSL certifications.</p> </li> <li> <p>Proficient in Azure identities and governance, including creating users and groups, managing role-based access control (RBAC), and providing access to Azure resources by assigning roles at different scopes.</p> </li> <li> <p>Configuration of Azure policies, resource locks, application of tags on resources, cost management, and invoice generation and analysis.</p> </li> <li> <p>Expertise in configuring VMs, managing VM sizes, adding data disks, configuring networking, configuring VM scale sets, and configuring availability sets.</p> </li> <li> <p>Automated deployment of virtual machines (VMs, Windows/Linux) using ARM templates and Terraform.</p> </li> <li> <p>Created ARM templates, PowerShell scripts, and Terraform scripts for Azure resource provisioning.</p> </li> <li> <p>Experience in creating and configuring Azure App Service, App Service plans, and App Service Environments (ASE), configuring scaling settings, securing App Services, configuring custom domain names, and deploying to deployment slots.</p> </li> <li> <p>Configured Application Insights for diagnostic logs and real-time application monitoring.</p> </li> <li> <p>Proficient in creating and configuring virtual networks, configuring private and public IP addresses, implementing subnets, configuring endpoints, and configuring private endpoints.</p> </li> <li> <p>Ensured secure access to virtual networks by creating security rules, associating network security groups (NSG) to subnets or network interfaces, and implementing Azure Firewall and Azure Bastion.</p> </li> <li> <p>Configured Azure Application Gateway, backend pools, installed certificates, and automated Application Gateway using ARM templates through CI/CD pipeline.</p> </li> <li> <p>Experience in monitoring Azure resources using Azure Monitor, configuring Azure Monitor logs, querying and analyzing logs, setting up alerts and actions, and configuring Application Insights.</p> </li> <li> <p>Proficient in creating and configuring storage accounts, configuring network access, managing access keys, and using Azure Storage Explorer.</p> </li> <li> <p>Expertise in writing cross-cutting re-usable components such as Caching, Exception Handling, Logging, and Validation.</p> </li> <li> <p>Experience in database design using SQL Server, Oracle 11g, and hands-on experience with SSIS and SSRS.</p> </li> <li> <p>Proficient in Microservices Architecture, Service Fabric, and Docker containers.</p> </li> <li> <p>Experience in setting up SSO with SAML or OAuth and collaborating closely with vendors during integration.</p> </li> </ul>"},{"location":"about/profile/#education","title":"Education","text":"<ul> <li>Master of Science (M.Sc Computer Science) \u2013 University of Mumbai, Mumbai, India</li> <li>Bachelor of Science (B.Sc Maths, Physics, Chemestry) \u2013 Osmania University, Hyderabad, India.</li> </ul>"},{"location":"articles/1-article-collection/","title":"Articles","text":""},{"location":"articles/1-article-collection/#article-collection","title":"Article Collection","text":"<p>70 published articles on Medium covering DevOps, Cloud Infrastructure, Kubernetes, Docker, Terraform, and Software Development.</p> # Article Title Published Date Reading Time Medium 70 AKS Pod Connecting to PostgreSQL Database using Workload Identity - Passwordless Authentication Nov 13, 2025 12 min Read 69 Getting Started with Kubernetes CronJob Mar 25, 2024 5 min Read 68 Connecting to Azure Cache for Redis with redis-cli and stunnel Mar 24, 2024 3 min Read 67 Install Minio in Azure Kubernetes Services (AKS) Mar 24, 2024 7 min Read 66 Install Redis Helmchart in Azure Kubernetes Services (AKS) Mar 24, 2024 7 min Read 65 Install Grafana Loki-Stack Helmchart in Azure Kubernetes Services (AKS) Mar 16, 2024 6 min Read 64 Install Grafana Helmchart in Azure Kubernetes Services (AKS) Mar 10, 2024 5 min Read 63 Setup Azure Monitor Alerts for CPU &amp; Memory exceeds for PostgreSQL using Terraform Mar 2, 2024 5 min Read 62 Running Azure DevOps Private Agents in AKS Feb 23, 2024 11 min Read 61 Install pgadmin4 in Azure Kubernetes Services (AKS) with Helmchart using Terraform Feb 17, 2024 11 min Read 60 Setting up PostgreSQL database in a Docker Container Feb 10, 2024 11 min Read 59 Setting up SQL Server database in a Docker Container Feb 10, 2024 8 min Read 58 Dockerfile cheat sheet Feb 10, 2024 3 min Read 57 Setting up Drupal in a Docker Container Feb 4, 2024 7 min Read 56 Setup Keycloak in a Docker Container Feb 3, 2024 9 min Read 55 Containerize Your First Microservice with React.js Feb 3, 2024 8 min Read 54 Containerize Your First Website using .NET Core MVC Feb 3, 2024 11 min Read 53 Containerize Your First Microservice with Node.js Feb 3, 2024 8 min Read 52 Containerize Your First Microservice with .NET Core Web API Feb 3, 2024 7 min Read 51 Create a Website Using Material for MkDocs: A Step-by-Step Guide Jan 28, 2024 12 min Read 50 Setting Up Windows Terminal with Oh-My-Posh Jan 28, 2024 6 min Read 49 Local Development Setup with Dev Containers Jan 20, 2024 7 min Read 48 Getting Started with Docker Jan 18, 2024 5 min Read 47 Send Alerts When Website is Down using Azure Application Insights Availability Test Jan 10, 2024 7 min Read 46 What are Development Containers? Jan 8, 2024 5 min Read 45 Exploring Docker Fundamentals Jan 5, 2024 6 min Read 44 Getting Started with ArgoCD Jan 2, 2024 9 min Read 43 Create Azure Event Hubs for Apache Kafka using Terraform Part-2 Jan 1, 2024 12 min Read 42 Azure Event Hubs for Apache Kafka Introduction Part-1 Jan 1, 2024 16 min Read 41 Getting Started with Drupal: A Beginner''s Guide Dec 30, 2023 7 min Read 40 Getting Started with Keycloak Dec 28, 2023 9 min Read 39 Setting Up Mac Terminal with Oh-My-Zsh Dec 25, 2023 6 min Read 38 Setup Azure Logs Alerts &amp; Notifications for Application Exceptions Dec 15, 2023 8 min Read 37 Docker Commands Cheat Sheet Dec 10, 2023 5 min Read 36 Create Azure Storage Account using Terraform Dec 31, 2023 11 min Read 36 Create Azure Cache for Redis using Terraform Dec 31, 2023 11 min Read 37 Create Azure Key Vault using Terraform Dec 30, 2023 13 min Read 38 Create Azure PostgreSQL Flexible Server using terraform Dec 30, 2023 12 min Read 39 Create Azure Application Gateway using terraform Dec 30, 2023 9 min Read 40 Single Sign-On OAuth 2.0 flows Dec 30, 2023 7 min Read 41 Single Sign-On Introduction Dec 30, 2023 10 min Read 42 dig commands Cheat Sheet Dec 27, 2023 4 min Read 43 A Kubernetes Namespace Stuck in the Terminating State Dec 22, 2023 2 min Read 44 Interacting with Azure App Service using a publishing profile Dec 22, 2023 2 min Read 45 Hiding the full file path in VSCode Terminal Dec 9, 2023 2 min Read 46 Reset a branch to a specific tag in Git Nov 22, 2023 1 min Read 47 Create a new user node pool in AKS using terraform Sep 10, 2023 8 min Read 48 Create new service connections &amp; variable groups in Azure DevOps Sep 6, 2023 8 min Read 49 Azure DevOps CI/CD Strategy for Microservices on Kubernetes Sep 5, 2023 7 min Read 50 Create Azure DevOps pipeline for .NET Core Web API Sep 5, 2023 8 min Read 51 Install &amp; Interact with ArgoCD CLI Sep 5, 2023 6 min Read 52 Install ArgoCD in AKS with helm chart using terraform Sep 5, 2023 11 min Read 53 Setup Cert-Manager in AKS using Terraform Sep 4, 2023 13 min Read 54 Setup NGINX ingress controller in AKS using Terraform Sep 4, 2023 12 min Read 55 Deploying an application to Azure Kubernetes Service (AKS) Sep 4, 2023 7 min Read 56 Prepare an application for Azure Kubernetes Service (AKS) Sep 4, 2023 10 min Read 57 Kubernetes Pod troubleshooting Sep 4, 2023 4 min Read 58 Create Azure Kubernetes Service (AKS) using terraform Sep 3, 2023 18 min Read 59 Create Azure Container Registry (ACR) using terraform Sep 3, 2023 12 min Read 60 Create Azure Virtual Network using terraform Sep 3, 2023 11 min Read 61 Create Azure Log Analytics Workspace using terraform Sep 3, 2023 7 min Read 62 Setup Terraform Foundation Part-2 Sep 3, 2023 12 min Read 63 Setup Terraform Foundation Part-1 Sep 3, 2023 15 min Read 6 Download &amp; install Software in Mac OS Sep 3, 2023 4 min Read 7 Download &amp; install Software in Windows OS Sep 3, 2023 5 min Read 66 Terraform Commands Cheat Sheet Sep 3, 2023 4 min Read 67 Helm Commands Cheat Sheet Sep 3, 2023 8 min Read 68 ArgoCD Commands Cheat Sheet Sep 3, 2023 8 min Read 69 Kubectl Commands Cheat Sheet Sep 3, 2023 12 min Read 70 Git Commands Cheat Sheet Sep 3, 2023 7 min Read"},{"location":"articles/20240218.1-build-agent/","title":"Running Azure DevOps Private Agents in AKS","text":""},{"location":"articles/20240218.1-build-agent/#setting-up-azure-devops-private-agents-in-azure-kubernetes-service-aks","title":"Setting up Azure DevOps Private Agents in Azure Kubernetes Service (AKS)","text":""},{"location":"articles/20240218.1-build-agent/#introduction","title":"Introduction","text":"<p>The conventional method for configuring Azure DevOps private agents involves deploying them on virtual machines (VMs). However, an interesting alternative is to utilize Azure Kubernetes Service (AKS) for this purpose.</p> <p>This article serves as a guide for setting up Azure DevOps private agents within an AKS cluster using Helm charts, providing step-by-step instructions for the process.</p> <p>Opting to deploy Azure DevOps private agents in AKS instead of provisioning new VMs can significantly impact cost-efficiency, scalability, and manageability of CI/CD pipelines. While VMs are easy to set up, they incur higher costs and may suffer from resource underutilization due to each VM requiring a full operating system and dedicated resources regardless of workload. This setup introduces complexity in scaling and maintenance, alongside increased compute and storage expenses.</p> <p>In contrast, hosting Azure DevOps private agents in AKS offers a more dynamic and cost-effective solution. AKS abstracts much of the complexity in managing containerized applications and leverages Kubernetes' orchestration capabilities. By deploying DevOps agents as containers within AKS, organizations benefit from automatic scaling based on workload, faster startup times compared to VMs, and simplified management of agent fleets. This article aims to explore the advantages of using AKS over VM-based approaches and provide a comprehensive guide to efficiently implementing Azure DevOps private agents within AKS, enhancing DevOps workflows for improved performance, reliability, and cost-effectiveness.</p>"},{"location":"articles/20240218.1-build-agent/#what-is-an-azure-devops-private-agent","title":"What is an Azure DevOps Private Agent?","text":"<p>At its core, an Azure DevOps private agent is a software agent that runs on a user-provided infrastructure, whether that's on-premises servers, virtual machines, or within containerized environments like Azure Kubernetes Service (AKS). These agents are responsible for executing the tasks defined in Azure DevOps pipelines, such as compiling code, running tests, and deploying applications. Each private agent is registered to a specific Azure DevOps organization and can be further associated with one or more projects within that organization.</p> <p>Here are some advantages of Azure DevOps private Agents:</p> <ul> <li> <p>Self-Hosted: Unlike the hosted agents provided by Azure DevOps, which are managed by Microsoft and run in a shared environment, private agents are self-hosted by you or your organization. You install, configure, and manage these agents on your own infrastructure, giving you more control over the execution environment.</p> </li> <li> <p>Customization: Private agents can be customized to match the specific requirements of your build and release tasks. You can install additional software, configure environment variables, and modify the agent to your project's needs.</p> </li> <li> <p>Security and Isolation: Private agents are often used when you need to execute CI/CD jobs in an environment that is not accessible from the public internet or when you want to maintain greater control over security. Private agents can operate within your private network, enhancing security and isolation.</p> </li> <li> <p>Access to Private Resources: If your CI/CD processes need access to private resources, databases, or services that are not publicly accessible, private agents can be configured to reach these resources within your network.</p> </li> </ul>"},{"location":"articles/20240218.1-build-agent/#technical-scenario","title":"Technical Scenario","text":"<p>When you're operating Azure services within a private network that is not accessible via the public internet, and you need connectivity from an Azure DevOps pipeline located in a public network, the solution is to deploy a private build machine within the same virtual network as your private services. This private build machine acts as a bridge, facilitating seamless connections to resources within the private network from Azure DevOps pipelines.</p> <p>For instance, if you've established a private AKS cluster to run your applications, you'll need a dedicated private self-hosted agent within the same virtual network to facilitate the deployment process. Similarly, consider a scenario where you've configured a PostgreSQL server with a private DNS zone, and your Azure DevOps pipeline resides in a public network. To establish a connection to this database, it is necessary to deploy your own private agent within the virtual network housing the PostgreSQL server. This solution enables secure and efficient communication with your private resources.</p>"},{"location":"articles/20240218.1-build-agent/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Installing Private Agent Helmchart in Kubernetes</li> <li>Step-2: Verify private agent resources in AKS</li> <li>Step-3: Create new agent pool in Azure DevOps</li> <li>Step-4: Create new Personal Access Token (PAT)</li> <li>Step-5: Register the Self-Hosted Agent</li> <li>Step-6: Update Build pipeline with Private Agent.</li> <li>Step-7: Test the new Private Agent</li> </ul>"},{"location":"articles/20240218.1-build-agent/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure you have the following prerequisites in place:</p> <ol> <li> <p>Azure Subscription: You must have an active Azure subscription to create and manage an AKS cluster.</p> </li> <li> <p>Azure Kubernetes Service (AKS) Cluster: An operational AKS cluster where you will deploy the Azure Pipelines agent. If you do not have one, you must set it up beforehand.</p> </li> <li> <p>Azure DevOps Organization: Access to an Azure DevOps organization is required for creating agent pools and pipelines. </p> </li> <li> <p>Azure CLI: The Azure Command Line Interface (CLI) installed on your workstation for managing Azure resources, including AKS.</p> </li> <li> <p>kubectl: The Kubernetes command-line tool, <code>kubectl</code>, must be installed and configured to communicate with your AKS cluster. This tool is essential for deploying and managing applications on Kubernetes.</p> </li> <li> <p>Helm: Helm, the package manager for Kubernetes, is required for installing and managing the private agent Helm chart. Ensure Helm is installed and ready to use on your system.</p> </li> <li> <p>Permissions in Azure DevOps: Adequate permissions within your Azure DevOps organization to create and manage agent pools, personal access tokens (PATs), and pipelines.</p> </li> </ol>"},{"location":"articles/20240218.1-build-agent/#implementation-details","title":"Implementation Details","text":"<p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>To interact with your Azure Kubernetes Service (AKS) cluster, you need to establish a connection. Depending on your role, you can use either the User or Admin credentials:</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# verify the aks connection by running following commands\nkubectl get no\nkubectl get namespace -A\n</code></pre> <p>Deployment Methods</p> <p>Deploying Azure DevOps private agents within an Azure Kubernetes Service (AKS) cluster offers a modern approach to managing build and deployment infrastructure, providing scalability and cost efficiency. There are two primary methods to achieve this deployment, each with its own set of procedures and advantages.</p> <p>Method 1: Creating and Deploying Your Own Docker Container to AKS</p> <p>This method involves packaging the Azure DevOps agent into a custom Docker container and then deploying that container to your AKS cluster. It allows for maximum customization and control over the agent environment.</p> <ol> <li>Prepare the Azure DevOps Agent Dockerfile</li> <li>Build the Docker Image</li> <li>Push the Image to a Container Registry</li> <li>Deploy the Container to AKS</li> <li>Configure and Run the Azure DevOps Agent</li> </ol> <p>Advantages:</p> <ul> <li>Customizability: This method offers the most flexibility, allowing you to include any tools, scripts, or configurations needed for your builds.</li> <li>Control: You have complete control over the agent's environment, updates, and lifecycle management.</li> </ul> <p>Option 2: Deploying Private Agent Using Helm Charts to AKS</p> <p>Helm charts offer a simplified and declarative way to deploy and manage applications in Kubernetes, including Azure DevOps agents.</p> <p>Advantages:</p> <ul> <li>Simplicity: Helm charts abstract away much of the complexity associated with Kubernetes deployments, making it easier to deploy and manage Azure DevOps agents.</li> <li>Scalability: Easily scale the number of agents up or down by adjusting the Helm release.</li> <li>Reusability: Helm charts can be shared and reused across different environments or projects, promoting consistency and saving time.</li> </ul> <p>Both methods provide a way to implement in AKS for Azure DevOps agent deployment, with the choice depending on your specific needs, expertise, and preferences for customization and management.</p> <p>In this article, we will be utilizing Method 2 to install the private agent, which involves the installation of Private agent Helm charts in AKS, However in case if you want to test docker container locally before using helmchart, here are commands:</p> <pre><code>docker run -d -e AZP_AGENT_NAME=\"&lt;agent name&gt;\" -e AZP_URL=\"https://dev.azure.com/&lt;your org.&gt;\" -e AZP_POOL=\"&lt;agent pool&gt;\" -e AZP_TOKEN=\"&lt;PAT&gt;\" emberstack/azure-pipelines-agent\n</code></pre> <p>verify docker image and container</p> <pre><code>docker ps\ndocker image ls\ndocker container ls\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-1-installing-private-agent-helmchart-in-kubernetes","title":"Step-1: Installing Private Agent Helmchart in Kubernetes","text":"<p>Find the <code>azure-pipelines-agent</code> Helmchart from <code>emberstack</code> on the ArtifactHUB website:</p> <p>Click on the \"Install\" button to retrieve the necessary details for this Helmchart installation</p> <p>For this installation, I am going to use the following Helmchart. You can find detailed information about this Helm chart on the following website:</p> <p>azure-pipelines-agent Helm chart</p> <p>Add helm repo</p> <pre><code>$ helm repo add emberstack https://emberstack.github.io/helm-charts\n</code></pre> <p>Update helm repo</p> <pre><code>$ helm repo update\n</code></pre> <p>Now that you have a Helm chart for your Azure Pipelines agent, it's time to deploy it to your AKS cluster.</p> <p>Install helmchart</p> <pre><code>helm install azure-pipelines-agent emberstack/azure-pipelines-agent --namespace \"build-agent\"\n</code></pre> <p>output</p> <pre><code>NAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:01:21 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre> <p>List helm chart</p> <pre><code>helm list --namespace build-agent\n</code></pre> <p>output</p> <pre><code>NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\nazure-pipelines-agent   build-agent     2               2024-02-05 09:02:09.946719 -0800 PST    deployed        azure-pipelines-agent-2.2.26    2.2.26\n</code></pre> <p>Lsit helm chart history </p> <pre><code>helm history azure-pipelines-agent -n build-agent\n</code></pre> <p>output</p> <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Mon Feb  5 09:01:21 2024        superseded      azure-pipelines-agent-2.2.26    2.2.26          Install complete\n2               Mon Feb  5 09:02:09 2024        deployed        azure-pipelines-agent-2.2.26    2.2.26          Upgrade complete\n</code></pre> <p>Show helm chart status</p> <pre><code>helm status azure-pipelines-agent --namespace build-agent\n</code></pre> <p>output</p> <pre><code>NAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:02:09 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-2-verify-private-agent-resources-in-aks","title":"Step 2. Verify private agent resources in AKS","text":"<p>Validate to make sure all the deployments / services created and running as expected. </p> <p>Run the following <code>kubectl</code> commands to verify the installation in the AKS cluster.</p> <pre><code>kubectl get all -n build-agent\n# or\nkubectl get all,configmaps,secrets -n build-agent\n</code></pre> <p>or</p> <pre><code>kubectl get namespace build-agent\nkubectl get deployments -n build-agent\nkubectl get pods -n build-agent\nkubectl get services -n build-agent\n</code></pre> <p>expected output</p> <pre><code>\n</code></pre> <pre><code>kubectl get configmaps -n build-agent\n</code></pre> <pre><code>NAME               DATA   AGE\nkube-root-ca.crt   1      6h48m\n</code></pre> <pre><code>kubectl get secrets -n build-agent\n</code></pre> <pre><code>\n</code></pre> <pre><code>kubectl get ingress -n build-agent\n</code></pre> <pre><code>No resources found in argocd namespace.\n</code></pre> <p>Ensure that the agent deployment in AKS is successful and verify its connectivity to Azure DevOps. You can also configure auto-scaling rules for the agent pool as needed.</p> <p>Verify the pod deployment by checking pod logs.</p> <pre><code>kubectl logs pods/azure-pipelines-agent-0 -n 'build-agent'\n</code></pre> <p>output</p> <p> </p>"},{"location":"articles/20240218.1-build-agent/#step-3-create-new-agent-pool-in-azure-devops","title":"Step-3: Create new agent pool in Azure DevOps","text":"<p>To group and manage your private agents effectively, create a new agent pool in Azure DevOps. This information will be used during agent registration.</p> <p>Here are the steps to create a new agent pool in Azure DevOps :</p> <ol> <li>Sign in to Azure DevOps.</li> <li>Access your organization or Project depnding on requirement.</li> <li>Navigate to Project Settings.</li> <li>Click on \"Agent pools.\"</li> <li>Create a new agent pool by clicking \"+ New agent.\"</li> <li>Configure the agent pool with a name, description, visibility, and security settings.</li> <li>Click \"Create\" to complete the process.</li> <li>The new agent pool is now ready for use.</li> </ol> <p></p>"},{"location":"articles/20240218.1-build-agent/#step-4-create-new-personal-access-token-pat","title":"Step-4: Create new Personal Access Token (PAT)","text":"<p>Generate a new Personal Access Token (PAT) with required permissions to authenticate the agent with Azure DevOps. This token will be used during agent registration.</p> <p>here are the steps to create a new Personal Access Token (PAT) in Azure DevOps:</p> <ol> <li>Sign in to Azure DevOps.</li> <li>Access your organization.</li> <li>Go to User Settings &gt; Security.</li> <li>Click \"New token\" under Personal access tokens.</li> <li>Configure the PAT: name, expiration, access scope.</li> <li>Click \"Create\" to generate the token.</li> <li>Copy and securely store the PAT.</li> <li>Confirmation message will appear.</li> <li>Use the PAT for authentication in Azure DevOps services.</li> </ol> <p> </p>"},{"location":"articles/20240218.1-build-agent/#step-5-register-the-self-hosted-agent","title":"Step-5: Register the Self-Hosted Agent","text":"<p>After deploying the agent to your AKS cluster using the Helm chart, the next crucial step is to register the agent with Azure DevOps. This process involves configuring the agent with essential details such as your Personal Access Token (PAT), the agent pool for assignment, and any specific capabilities the agent should possess. Successfully completing this step links your self-hosted agent to Azure DevOps, enabling it to execute pipelines and tasks.</p> <p>Updating Parameters in the Helm Chart</p> <p>To register the agent, you'll need to update the Helm chart with specific parameters related to your Azure DevOps setup. These include:</p> <ul> <li><code>pipelines.url</code>: The base URL for your Azure DevOps organization.</li> <li><code>pipelines.pat.value</code>: The Personal Access Token (PAT) that the agent will use to authenticate with Azure DevOps.</li> <li><code>pipelines.pool</code>: The name of the agent pool to which the agent should be registered.</li> </ul> <p>Ensure that you replace placeholders like <code>your-organization</code>, <code>your-personal-access-token</code>, <code>your-agent-pool-name</code>, and any other specific values with the actual information relevant to your setup.</p> <p>Command to Update Helm Chart Values:</p> <p>Execute the following command to update the Helm chart values and register the agent with Azure DevOps:</p> <pre><code>helm upgrade azure-pipelines-agent emberstack/azure-pipelines-agent --set pipelines.url=https://dev.azure.com/orgname,pipelines.pat.value=lsir5gjt2djieulvmlmgv66jdrbmcaeww4oydtsxf25ap52ztpyq,pipelines.pool=azure-pipelines-agent --namespace \"build-agent\"\n</code></pre> <p>Remember to replace <code>your-organization</code>, <code>YOUR_PERSONAL_ACCESS_TOKEN</code>, and <code>your-agent-pool-name</code> with your specific details.</p> <p>Expected Output:</p> <pre><code>Release \"azure-pipelines-agent\" has been upgraded. Happy Helming!\nNAME: azure-pipelines-agent\nLAST DEPLOYED: Mon Feb  5 09:51:23 2024\nNAMESPACE: build-agent\nSTATUS: deployed\nREVISION: 9\nTEST SUITE: None\nNOTES:\nCheck your Azure DevOps portal to manage the Azure Pipelines Agent.\n</code></pre> <p>This output confirms that the Helm chart has been successfully updated and the agent is registered with Azure DevOps. You can now navigate to the Azure DevOps portal to manage and utilize your self-hosted agent within your CI/CD pipelines.</p>"},{"location":"articles/20240218.1-build-agent/#step-6-update-build-pipeline-with-private-agent","title":"Step-6: Update Build pipeline with Private Agent","text":"<p>Once your self-hosted private agent is successfully registered and operational within Azure DevOps, the subsequent step involves integrating this agent into your build pipelines. By specifying the agent pool containing your private agent in the pipeline configuration, you can direct your builds to run on this self-managed infrastructure, leveraging its benefits for your CI/CD processes.</p> <p>Modifying Azure Pipelines Configuration</p> <p>To utilize your self-hosted private agent, you need to modify the configuration of your Azure Pipelines. This adjustment involves specifying the agent pool that your private agent belongs to. By doing so, you ensure that the pipeline runs on your private infrastructure.</p> <p>How to Update Your Build Pipeline with the New Agent Pool:</p> <p>You can update the YAML file defining your build pipeline by including the <code>pool</code> property with the name of your agent pool. Below is an example of how to specify the agent pool within a job in your Azure Pipelines YAML configuration:</p> <pre><code>jobs:\n  - job: test_private_agent_job\n    pool:\n      name: azure-pipelines-agent\n    timeoutInMinutes: 5\n</code></pre>"},{"location":"articles/20240218.1-build-agent/#step-7-test-the-new-private-agent","title":"Step-7: Test the new Private Agent","text":"<p>To verify that your self-hosted agent is working as expected, you can queue a build or release pipeline that targets the agent pool where your AKS agent is registered. Azure Pipelines will automatically route the job to your self-hosted agent, and you can monitor the job's progress in the Azure DevOps portal.</p>"},{"location":"articles/20240218.1-build-agent/#conclusion","title":"Conclusion","text":"<p>Setting up a self-hosted Azure Pipelines agent in an AKS cluster using Helm charts offers control, scalability, and resource efficiency for your CI/CD workflows. By following the steps outlined in this article, you can identify a Helm chart, deploy it to your AKS cluster, and register the agent with Azure DevOps seamlessly. </p>"},{"location":"articles/20240218.1-build-agent/#reference","title":"Reference","text":"<ul> <li>Run a self-hosted agent in Docker</li> <li>azure-pipelines-agent - Helm Chart</li> </ul>"},{"location":"articles/20240302.1-alerts-cpu-memory/","title":"Setup Azure Monitor Alerts for CPU &amp; Memory exceeds for PostgreSQL - using Terraform","text":""},{"location":"articles/20240302.1-alerts-cpu-memory/#introduction","title":"Introduction","text":"<p>Monitoring and managing your Azure resources are always crucial for any kind of project, and setting up alerts for CPU and memory spikes, especially in database systems like PostgreSQL or SQL Server, is very important and necessary.</p> <p>Whether you're using PostgreSQL or SQL Server databases on Azure, monitoring their performance metrics such as CPU and memory utilization is essential for ensuring optimal performance and availability. Setting up alerts for CPU and memory exceeds allows you to proactively address potential issues before they impact your applications and users.</p> <p>In this article, I will guide you through the steps to set up email, SMS, or voice notifications for your team members when CPU &amp; Memory exceeds more than 80% for PostgreSQL.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#prerequisites","title":"Prerequisites","text":"<p>Before setting up Azure Monitor alerts for CPU and memory spikes in your PostgreSQL or SQL Server databases, ensure the following prerequisites are met:</p> <ol> <li>An active Azure subscription.</li> <li>Access to Azure portal with appropriate permissions.</li> <li>PostgreSQL or SQL Server database deployed and running on Azure.</li> <li>Azure Monitor configured to monitor your database metrics.</li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#create-an-alert-when-cpu-exceeds-an-average-of-80-percent","title":"Create an alert when CPU exceeds an average of 80 percent","text":"<ol> <li> <p>Navigate to Azure Monitor: Go to the Azure portal (https://portal.azure.com) and navigate to your Azure Monitor instance.</p> </li> <li> <p>Create New Alert Rule: Select the appropriate resource (PostgreSQL or SQL Server database) from the list of monitored resources.  </p> </li> <li> <p>Set Conditions: Under the \"Alerts\" section, click on \"New alert rule\" and select the condition type as \"Metric alert.\"</p> </li> <li> <p>Define Condition: Choose the appropriate metric (e.g., \"CPU percentage\") and set the condition to \"Greater than\" and the threshold to 80 percent.  </p> </li> <li>Configure Alert Details: Provide a meaningful name and description for the alert. You can also customize the severity level and alert frequency as per your requirements.  </li> <li> <p>Set Action Group: Specify the action group to be notified when the alert is triggered. This could include sending emails, SMS notifications, or triggering automated remediation tasks.  </p> </li> <li> <p>Review and Create: Review the alert rule configuration and click on \"Create\" to finalize the setup.</p> </li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#create-an-alert-when-memory-exceeds-an-average-of-80-percent","title":"Create an alert when Memory exceeds an average of 80 percent","text":"<ol> <li>Follow the same steps as above, but this time select the memory metric instead of CPU.  </li> <li> <p>Set the condition to \"Greater than\" and the threshold to 80 percent.</p> </li> <li> <p>Configure the alert details and action group as per your preferences.</p> </li> <li> <p>Review and create the alert rule.</p> </li> </ol>"},{"location":"articles/20240302.1-alerts-cpu-memory/#test-alert-rule","title":"Test Alert Rule","text":"<p>To ensure that the alert rules are working as expected, you can simulate a CPU or memory spike in your PostgreSQL or SQL Server database. This can be done by running resource-intensive queries or scripts against the database.</p> <p>Once the CPU or memory exceeds the defined threshold, Azure Monitor will trigger the corresponding alert, and notifications will be sent to the specified action group.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#setup-metric-alert-using-terraform","title":"Setup Metric Alert Using Terraform","text":"<p>Below are the steps to set up Azure Monitor alerts for CPU and memory exceeds using Terraform for PostgreSQL or SQL Server databases:</p> <p>Step 1: Install Terraform: If you haven't already installed Terraform, you can download it from the official website: https://www.terraform.io/downloads.html. Follow the installation instructions for your operating system.</p> <p>Step 2: Configure Azure Provider: Create a new directory for your Terraform configuration and create a file named <code>main.tf</code>. In this file, configure the Azure provider with your Azure credentials.</p> <pre><code>provider \"azurerm\" {\n  features {}\n}\n</code></pre> <p>Step 3: Define Alert Rule Resources: Add resource definitions for creating alert rules for CPU and memory exceeds. Specify the appropriate metric and condition for each alert rule.</p> <p>Memory Utilization:</p> <pre><code>// This alert will be triggered when Memory Utilization exceeds 80% for longer than 5 minutes\nresource \"azurerm_monitor_metric_alert\" \"psql_memory_usage\" {\n  name                = \"${azurerm_postgresql_flexible_server.psql.name}-memory_usage\"\n  resource_group_name = azurerm_resource_group.rg.name\n  scopes              = [azurerm_postgresql_flexible_server.psql.id]\n  description         = \"Action will be triggered when Memory utilization exceeds 80% for longer than 5 minutes\"\n  frequency           = \"PT5M\"\n  window_size         = \"PT5M\"\n  auto_mitigate = true\n  enabled = true\n  severity            = 1\n  criteria {\n    metric_namespace = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n    metric_name      = \"memory_percent\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n  }\n\n  action {\n    action_group_id = \"&lt;your-action-group-id&gt;\"\n  }\n  tags = \"default_tags goes here\"\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_postgresql_flexible_server.psql,    \n  ]\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>CPU Utilization:</p> <pre><code>// This alert will be triggered when CPU Utilization exceeds 80% for longer than 5 minutes\nresource \"azurerm_monitor_metric_alert\" \"psql_cpu_usage\" {\n  name                = \"${azurerm_postgresql_flexible_server.psql.name}-cpu_usage\"\n  resource_group_name = azurerm_resource_group.rg.name\n  scopes              = [azurerm_postgresql_flexible_server.psql.id]\n  description         = \"Action will be triggered when CPU utilization exceeds 80% for longer than 5 minutes\"\n  frequency           = \"PT5M\"\n  window_size         = \"PT5M\"\n  severity            = var.alert_severity\n  criteria {\n    metric_namespace = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n    metric_name      = \"cpu_percent\"\n    aggregation      = \"Average\"\n    operator         = \"GreaterThan\"\n    threshold        = 80\n  }\n\n  action {\n    action_group_id = \"&lt;your-action-group-id&gt;\"\n  }\n  tags = \"default_tags goes here\"\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_postgresql_flexible_server.psql,    \n  ]\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>Replace <code>&lt;your-resource-group-name&gt;</code>, <code>&lt;your-database-resource-id&gt;</code>, and <code>&lt;your-action-group-id&gt;</code> with your actual Azure resource group name, database resource ID, and action group ID respectively.</p> <p>Step 4: Initialize and Apply Terraform Configuration: Initialize Terraform in your working directory and apply the configuration.</p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <p>By following these steps, you can set up Azure Monitor alerts for CPU and memory exceeds using Terraform for PostgreSQL or SQL Server databases.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#conclusion","title":"Conclusion","text":"<p>Setting up Azure Monitor alerts for CPU and memory exceeds is crucial for maintaining the performance and availability of your PostgreSQL or SQL Server databases on Azure. By proactively monitoring these metrics and configuring alerts, you can quickly identify and address potential issues before they escalate, ensuring a seamless experience for your applications and users.</p>"},{"location":"articles/20240302.1-alerts-cpu-memory/#references","title":"References","text":"<ul> <li>Microsoft Azure Documentation:</li> <li>Azure Monitor Overview:</li> <li>Azure Monitor Alerts Documentation:</li> <li>Manages a Metric Alert within Azure Monitor:</li> </ul>"},{"location":"articles/20240323.1-redis-connection/","title":"Connecting to Azure Cache for Redis with redis-cli and stunnel","text":""},{"location":"articles/20240323.1-redis-connection/#introduction","title":"Introduction","text":"<p>Azure Cache for Redis is a fully managed, open-source, in-memory data store that is used for improving the performance of applications in Microservices architecture. In Azure, security is something always important, and as such, the Azure redis cache non-SSL port (6379) is often disabled with SSL enabled. also, when utilizing private endpoints, direct connections to Azure Cache for Redis become restricted. In this scenario, connecting to Azure Cache for Redis requires additional steps.</p> <p>In this article, we will walk through the process of connecting to Azure Cache for Redis using <code>redis-cli</code> along with <code>stunnel</code> tools. <code>stunnel</code> acts as a secure tunneling tool, enabling SSL encryption for the connection, while <code>redis-cli</code> serves as the command-line interface to interact with Redis.</p>"},{"location":"articles/20240323.1-redis-connection/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have the following prerequisites installed:</p> <ul> <li><code>stunnel</code>: Download and install <code>stunnel</code> from the official website here.</li> <li><code>redis-cli</code>: Download and install <code>redis-cli</code> from the GitHub repository here or here.</li> <li>Virtual Machine (Jumpbox) in the same network as the private endpoint of redis</li> </ul>"},{"location":"articles/20240323.1-redis-connection/#step-1-login-virtual-machine-jumpbox","title":"Step 1: Login Virtual Machine (jumpbox)","text":"<p>It's essential to have a virtual machine deployed within the same virtual network as the Azure Cache for Redis instance. This ensures that the virtual machine can establish a secure connection to the Redis cache without relying on public endpoints or traversing the internet. you need to first login into this Virtual Machine (VM) in the same network as the private endpoint of redis.</p> <p>Test Redis Connection from jumpbox before making any further steps to make sure connection is successful.</p> <p>Test-NetConnection</p> <pre><code>Test-NetConnection -Port 6380 -ComputerName redis-redis1-poc.redis.cache.windows.net\n</code></pre> <pre><code>ComputerName     : redis-redis1-poc.redis.cache.windows.net\nRemoteAddress    : 10.64.3.7\nRemotePort       : 6380\nInterfaceAlias   : Ethernet 3\nSourceAddress    : 10.24.50.78\nTcpTestSucceeded : True\n</code></pre> <p>nslookup</p> <pre><code>nslookup redis-redis1-poc.privatelink.redis.cache.windows.net\n</code></pre> <pre><code>Server:  UnKnown\nAddress:  168.63.129.14\n\nNon-authoritative answer:\nName:    redis-redis1-poc.privatelink.redis.cache.windows.net\nAddress:  10.64.3.6\n</code></pre>"},{"location":"articles/20240323.1-redis-connection/#step-2-download-and-install-stunnel","title":"Step 2: Download and install stunnel","text":"<p>Download and install <code>stunnel</code> from the provided link. Follow the installation instructions provided by the installer.</p>"},{"location":"articles/20240323.1-redis-connection/#step-3-download-and-install-redis-cli","title":"Step 3: Download and install redis-cli","text":"<p>Download and install <code>redis-cli</code> from the provided GitHub repository links. Follow the installation instructions for your respective operating system.</p>"},{"location":"articles/20240323.1-redis-connection/#step-4-configure-stunnel-to-connect-to-azure-cache","title":"Step 4: Configure stunnel to connect to Azure Cache","text":"<ol> <li> <p>Open <code>stunnel</code> and click on \"Edit Configuration\".     </p> </li> <li> <p>Update the Azure Redis Cache endpoint details in the configuration file.</p> <p> </p> </li> <li> <p>Reload the configuration in <code>stunnel</code>. You should see a \"Connection successful\" message.</p> <p> </p> </li> </ol>"},{"location":"articles/20240323.1-redis-connection/#step-5-start-stunnel-and-connect","title":"Step 5: Start stunnel and connect","text":"<ol> <li> <p>Start <code>stunnel</code>.</p> </li> <li> <p>Open a command prompt or PowerShell window and enter the <code>redis-cli</code> command along with the appropriate authentication key obtained from the Azure Cache for Redis authentication settings.</p> <p> </p> </li> <li> <p>Test the Redis connection by entering commands such as <code>info</code>.</p> <p> </p> </li> </ol> <p>You have now successfully connected to Azure Cache for Redis using the <code>stunnel</code> tool.</p>"},{"location":"articles/20240323.1-redis-connection/#conclusion","title":"Conclusion","text":"<p>Connecting to Azure Cache for Redis, especially when SSL is enforced and private endpoints are in use, requires additional steps beyond a direct connection. By utilizing <code>stunnel</code> to create a secure tunnel and <code>redis-cli</code> to interact with Redis, you can securely access your Azure Cache for Redis instance.</p>"},{"location":"articles/20240323.1-redis-connection/#references","title":"References","text":"<ul> <li>stunnel Downloads</li> <li>Microsoft Archive Redis Releases</li> <li>Redis Hashes Releases</li> </ul>"},{"location":"articles/20240325.1-cronjob/","title":"Getting Started with Kubernetes CronJob","text":""},{"location":"articles/20240325.1-cronjob/#introduction","title":"Introduction","text":"<p>In the container orchestration &amp; Kubernetes technology, managing recurring tasks efficiently is crucial for maintaining a healthy and automated system. One powerful tool in the Kubernetes for handling scheduled tasks is the CronJob. </p> <p>In this article, I will explain what the CronJobs are, their utility in Kubernetes clusters, explore some common use cases, and walk through the process of creating couple of CronJob examples.</p>"},{"location":"articles/20240325.1-cronjob/#what-is-cronjob","title":"What is CronJob?","text":"<p>In Kubernetes <code>CronJob</code> is a resource type used in Kubernetes to automate the execution of tasks on a recurring schedule. It is similar to the traditional cron utility used in Unix-like operating systems, but it operates within the Kubernetes ecosystem. </p> <p>CronJobs allow users to define jobs, which are tasks or pods that run to completion, and specify a schedule in Cron format (minute, hour, day of month, month, day of week) for when these jobs should be executed. Kubernetes CronJobs ensure that these jobs are run at the specified intervals, providing a convenient way to automate repetitive tasks within Kubernetes clusters.</p> <p>Kubernetes CronJobs simplify the management of scheduled tasks within Kubernetes clusters, enabling users to automate operations, backups, data processing, perform routine maintenance, and execute batch processes efficiently.</p>"},{"location":"articles/20240325.1-cronjob/#use-cases","title":"Use Cases","text":"<p>1. Running Scheduled PostgreSQL Queries</p> <p>Imagine you have a PostgreSQL database running for your system, and you need to run specific queries at regular intervals to generate reports or perform data cleanup. CronJobs can be configured to execute psql queries against the database periodically, automating this process.</p> <p>2. Microservices Scenarios</p> <p>In a microservices architecture, various components may require periodic tasks such as log rotation, database backups, or cache refreshing. CronJobs can be employed to schedule these tasks across different microservices, ensuring smooth operation of the entire system.</p>"},{"location":"articles/20240325.1-cronjob/#creating-your-first-cronjob","title":"Creating your First CronJob","text":"<p>Let's walk through the process of creating a simple CronJob using Kubernetes YAML configuration.</p> <p>Step 1: Define the CronJob</p> <p>Create a YAML file (e.g., <code>cronjob.yaml</code>) with the following content:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\n  namespace: sample # update your namespace here$$\nspec:\n  schedule: \"*/1 * * * *\"  # Runs every minute\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: my-container\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo \"Hello, Kubernetes!\"\n          restartPolicy: OnFailure\n</code></pre> <p>Step 2: Apply the Configuration</p> <p>Apply the YAML configuration using </p> <pre><code># kubectl apply -f cronjob-1.yaml\n</code></pre> <p>Step 3: Verify CronJob</p> <p>Check the status of the CronJob using <code>kubectl get cronjobs</code> and <code>kubectl get jobs</code>.</p> <pre><code>kubectl get cronjobs -n sample\n</code></pre> <p>output</p> <pre><code>NAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nmy-cronjob   */1 * * * *   False     0        &lt;none&gt;          38s\n</code></pre> <pre><code>kubectl get jobs -n sample\n</code></pre> <p><pre><code>NAME                  COMPLETIONS   DURATION   AGE\nmy-cronjob-28523782   1/1           4s         11s\n</code></pre> Check the logs</p> <p><pre><code>kubectl logs jobs/my-cronjob-28523783 -n sample\n</code></pre> output</p> <p><pre><code>Tue Mar 26 04:23:01 UTC 2024\nHello, Kubernetes!\n</code></pre> Explanation of key fields in the CronJob YAML</p> <ul> <li> <p>schedule: Specifies the schedule in Cron format (minute, hour, day of month, month, day of week) when the job should run.</p> </li> <li> <p>jobTemplate: Defines the template for the Job created by the CronJob, including pod specifications like containers, volumes, and restart policies.</p> </li> </ul>"},{"location":"articles/20240325.1-cronjob/#creating-your-second-cronjob","title":"Creating your Second CronJob","text":"<p>Let's create a CronJob that demonstrates a real-world use case: performing daily backups of a PostgreSQL database running in a Kubernetes cluster.</p> <p>Here's the YAML configuration for the CronJob:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: postgres-backup\nspec:\n  schedule: \"0 0 * * *\"  # Run at midnight every day\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: postgres-backup\n            image: postgres:latest  # You can use a custom image with backup tools installed\n            command: [\"sh\", \"-c\"]\n            args:\n            - pg_dump -U &lt;username&gt; -h &lt;host&gt; &lt;database_name&gt; &gt; /backup/$(date +\"%Y%m%d\").sql\n            volumeMounts:\n            - name: backup-volume\n              mountPath: /backup\n          restartPolicy: OnFailure\n          volumes:\n          - name: backup-volume\n            persistentVolumeClaim:\n              claimName: postgres-pvc  # Name of the PersistentVolumeClaim for PostgreSQL data\n</code></pre> <p>Make sure to replace <code>&lt;username&gt;</code>, <code>&lt;host&gt;</code>, and <code>&lt;database_name&gt;</code> with appropriate values for your PostgreSQL database. also, ensure that you have a PersistentVolumeClaim named <code>postgres-pvc</code> associated with your PostgreSQL deployment.</p> <p>With this CronJob configuration, Kubernetes will automatically execute the backup command at midnight every day, ensuring that your PostgreSQL database is backed up regularly.</p> <p>Explanation of key fields in the CronJob YAML</p> <ul> <li> <p>concurrencyPolicy: Determines how to handle multiple executions of the job concurrently. Options include <code>Allow</code> (default), <code>Forbid</code>, and <code>Replace</code>. Here's a breakdown of the possible values for the concurrencyPolicy field:</p> <ul> <li> <p>Allow: Allows concurrent executions of the job. This means that if a new job is scheduled to run while a previous instance of the job is still running, both jobs will run concurrently.</p> </li> <li> <p>Forbid: Disallows concurrent executions of the job. If a new job is scheduled to run while a previous instance of the job is still running, the new job will not start until the previous one completes.</p> </li> <li> <p>Replace: Replaces the existing job with the new one if a new job is scheduled to run while the previous instance of the job is still running. This effectively terminates the running job and starts the new one.</p> </li> </ul> </li> <li> <p>successfulJobsHistoryLimit:  The successfulJobsHistoryLimit field specifies the number of successfully completed jobs that should be retained in the history of the CronJob. In this case, successfulJobsHistoryLimit: 1 indicates that only the latest successful job will be kept in the history.</p> </li> </ul>"},{"location":"articles/20240325.1-cronjob/#creating-your-third-cronjob","title":"Creating your Third CronJob","text":"<p>Below is an example of a CronJob YAML configuration that schedules the execution of a <code>kubectl</code> command:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: kubectl-command\nspec:\n  schedule: \"*/5 * * * *\"  # Run every 5 minutes\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: kubectl\n            image: bitnami/kubectl:latest\n            command:\n            - kubectl\n            args:\n            - &lt;kubectl_command&gt;  # Replace &lt;kubectl_command&gt; with your desired kubectl command and arguments\n          restartPolicy: OnFailure\n</code></pre> <p>Example:</p> <p>Also, keep in mind that you will need to set up proper Role-Based Access Control (RBAC) permissions, as you may encounter errors such as 'Error from server (Forbidden): services is forbidden' if your service account lacks the necessary permissions.</p> <pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  namespace: jp-test\n  name: jp-runner\nrules:\n- apiGroups:\n  - extensions\n  - apps\n  resources:\n  - pods\n  verbs:\n  - 'get'\n\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: jp-runner\n  namespace: jp-test\nsubjects:\n- kind: ServiceAccount\n  name: sa-jp-runner\n  namespace: jp-test\nroleRef:\n  kind: Role\n  name: jp-runner\n  apiGroup: \"\"\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa-jp-runner\n  namespace: jp-test\n\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: sa-jp-runner\n          containers:\n          - name: hello\n            image: bitnami/kubectl:latest\n            command:\n            - /bin/sh\n            - -c\n            - kubectl patch deployment runners -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"jp-runner\",\"env\":[{\"name\":\"START_TIME\",\"value\":\"'$(date +%s)'\"}]}]}}}}' -n jp-test\n          restartPolicy: OnFailure\n\n# kubectl apply -f cronjob-3.yaml\n</code></pre>"},{"location":"articles/20240325.1-cronjob/#conclusion","title":"Conclusion","text":"<p>In this guide, we've explored the fundamentals of Kubernetes CronJobs, their significance in scheduling recurring tasks within Kubernetes clusters, and provided practical insights into creating and managing CronJobs. By leveraging CronJobs effectively, you can automate routine tasks, streamline operations, and enhance the efficiency of your Kubernetes environment. </p>"},{"location":"articles/20240325.1-cronjob/#references","title":"References","text":"<ul> <li>Running Automated Tasks with a CronJob</li> </ul>"},{"location":"articles/20251113.1-aks-workload-identity/","title":"AKS Pod Connecting to PostgreSQL Database using Workload Identity - Passwordless Authentication","text":""},{"location":"articles/20251113.1-aks-workload-identity/#introduction","title":"Introduction","text":"<p>Securely connecting Kubernetes pods to Azure services like PostgreSQL is essential for modern cloud applications. Traditional approaches using connection strings with usernames and passwords pose security risks and management overhead. Azure Kubernetes Service (AKS) workload identity provides a secure, passwordless authentication mechanism that enables pods to authenticate to Azure services using Azure Active Directory (Azure AD) identities.</p> <p>This article demonstrates how to configure AKS workload identity to enable pods to connect to Azure Database for PostgreSQL Flexible Server using managed identities, eliminating the need for storing database credentials in the application code or Kubernetes secrets.</p>"},{"location":"articles/20251113.1-aks-workload-identity/#key-concepts","title":"Key Concepts","text":"<p>Before diving into the implementation details, it's essential to understand the fundamental concepts that enable passwordless authentication between AKS pods and PostgreSQL. This approach eliminates the security risks of storing database credentials while leveraging Azure AD's identity platform. Familiarizing yourself with these building blocks will make the actual implementation much easier to follow and troubleshoot.</p> <p>What is an OIDC Issuer?</p> <p>An OpenID Connect (OIDC) issuer is a service that issues and validates identity tokens. In the context of AKS, the OIDC issuer provides a way for Kubernetes service accounts to be authenticated by Azure AD through federated identity credentials. For Example: AKS cluster's OIDC issuer enables pods to prove their identity to Azure AD without storing database passwords in the application code.</p> <p>What are Federated Credentials?</p> <p>Federated credentials establish a trust relationship between an Azure AD application or managed identity and an external identity provider. They define which external identities can impersonate the Azure AD identity. For Example: When the pod needs to connect to PostgreSQL, federated credentials allow the Kubernetes service account to \"become\" the managed identity, which can then authenticate to the PostgreSQL server using Azure AD authentication.</p> <p>What is a Managed Identity?</p> <p>A managed identity is an Azure AD identity that is automatically managed by Azure. It eliminates the need for developers to manage credentials by providing an identity for applications to use when connecting to resources that support Azure AD authentication. For Example: The managed identity serves as the application's database user in Azure AD. Instead of creating a traditional PostgreSQL user with a password, you assign the managed identity appropriate database permissions.</p> <p>What is AKS Workload Identity?</p> <p>AKS workload identity is a feature that enables Kubernetes pods to authenticate to Azure services using Azure AD identities. It uses OIDC federation to establish trust between the Kubernetes cluster and Azure AD, allowing pods to obtain Azure AD tokens using their Kubernetes service account. For Example: When a pod with workload identity starts up, it automatically receives an Azure AD token that can be used to authenticate to PostgreSQL as the configured managed identity user.</p> <p>What is Workload Identity Federation?</p> <p>Workload identity federation allows external identity providers (like Kubernetes) to exchange their tokens for Azure AD tokens without storing long-lived credentials. This enables secure access to Azure resources from workloads running outside of Azure. For Example: This eliminates the need to store PostgreSQL passwords in Kubernetes secrets. the pod's service account token is exchanged for an Azure AD token that PostgreSQL recognizes for authentication.</p> <p>What is Kubernetes Service Accounts?</p> <p>A Kubernetes service account provides an identity for processes that run in a pod. When pods need to communicate with the Kubernetes API server or external services, they use service accounts for authentication. In the context of workload identity, service accounts are linked to Azure AD identities through annotations, enabling pods to authenticate to Azure services without storing credentials. For Example: The service account acts as the pod's database identity card. By annotating it with workload identity configuration, you're telling AKS \"when pods use this service account, let them authenticate to PostgreSQL as this specific managed identity user.\"</p>"},{"location":"articles/20251113.1-aks-workload-identity/#prerequisites","title":"Prerequisites","text":"<ul> <li>A well-configured AKS cluster with Kubernetes version 1.22 or later</li> <li>Azure CLI installed and configured</li> <li>kubectl command-line tool installed</li> <li>A well-configured Azure Database for PostgreSQL Flexible Server with databases</li> <li>Azure AD permissions to create and manage managed identities</li> <li>Contributor or Owner role on the Azure subscription</li> <li>Basic understanding of Kubernetes concepts (pods, service accounts, deployments)</li> <li>Docker image of the application ready to deploy</li> </ul>"},{"location":"articles/20251113.1-aks-workload-identity/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram illustrates the high-level architecture and authentication flow for AKS workload identity with PostgreSQL:</p> <p> </p> <p>Authentication Flow Details</p> <ol> <li> <p>Token Request: When the application pod starts, it uses the configured service account to request an identity token from the AKS OIDC issuer.</p> </li> <li> <p>Token Exchange: The workload identity system automatically exchanges the Kubernetes service account token for an Azure AD access token using the federated credential configuration established between the AKS cluster and Azure AD.</p> </li> <li> <p>Token Validation: Azure AD validates the token by:</p> </li> <li>Verifying it came from the trusted OIDC issuer (AKS cluster)</li> <li>Confirming the service account subject matches the federated credential</li> <li> <p>Issuing an Azure AD token for the managed identity</p> </li> <li> <p>Database Authentication: The application uses the Azure AD token for authentication when connecting to PostgreSQL, which validates it against Azure AD and allows the connection.</p> </li> </ol> <p>Key Components</p> <ul> <li>OIDC Issuer: Provides the trust anchor between AKS and Azure AD</li> <li>Service Account: Links the pod to the managed identity through annotations  </li> <li>Managed Identity: Acts as the database user in Azure AD (no password needed)</li> <li>Federated Credential: Establishes the trust relationship between the service account and managed identity</li> <li>PostgreSQL AD Admin: Configures the database to accept Azure AD authentication</li> </ul> <p>This architecture eliminates credential management by using cryptographic tokens and trust relationships instead of storing passwords or connection strings in the application or Kubernetes secrets.</p>"},{"location":"articles/20251113.1-aks-workload-identity/#authentication-sequence-diagram","title":"Authentication Sequence Diagram","text":"<p>The following sequence diagram illustrates the detailed step-by-step authentication flow when a pod connects to PostgreSQL using workload identity:</p> <p> </p> <p>Sequence Flow Explanation:</p> <ol> <li>Pod Initialization: The application pod starts with the configured service account that has workload identity annotations (<code>azure.workload.identity/use: \"true\"</code> label triggers the workload identity webhook to inject environment variables and projected service account token volumes into the pod)</li> <li>Token Request: Pod requests a service account token from the AKS OIDC issuer</li> <li>Service Account Token: OIDC issuer returns a JWT token proving the pod's identity</li> <li>Token Exchange: The workload identity system initiates token exchange with Azure AD using the federated credential and projected token</li> <li>Federated Validation: Azure AD validates the federated credential configuration</li> <li>Trust Confirmation: Managed identity confirms the trust relationship exists</li> <li>Azure AD Token: Azure AD returns an access token for the managed identity</li> <li>Database Connection: Pod connects to PostgreSQL using the Azure AD token for authentication</li> <li>Token Validation: PostgreSQL validates the token with Azure AD</li> <li>Authentication Success: Azure AD confirms token validity</li> <li>Connection Established: Database connection is established with managed identity permissions</li> <li>Query Execution: Pod can now execute SQL queries as the managed identity user</li> <li>Results: PostgreSQL returns query results to the authenticated pod</li> </ol> <p>Key Point: The <code>azure.workload.identity/use: \"true\"</code> label is essential - it signals the workload identity mutating webhook to automatically inject the necessary environment variables (<code>AZURE_CLIENT_ID</code>, <code>AZURE_TENANT_ID</code>, <code>AZURE_FEDERATED_TOKEN_FILE</code>) and mount the projected service account token, enabling automatic token exchange through federated credentials without manual configuration.</p> <p>This sequence eliminates traditional password-based authentication and provides secure, token-based access to PostgreSQL.</p>"},{"location":"articles/20251113.1-aks-workload-identity/#step-1-enable-workload-identity-on-aks","title":"Step 1: Enable Workload Identity on AKS","text":"<p>The first step is configuring the AKS cluster to support workload identity authentication. By default, AKS clusters don't have this feature enabled, which means pods cannot authenticate to Azure services using managed identities.</p> <p>Enabling workload identity requires both the workload identity addon and an OIDC issuer. The OIDC issuer establishes the trust relationship between Kubernetes and Azure AD, acting as the identity provider that Azure AD can verify and trust. Think of this as installing a \"passport system\" for the cluster - just like you need a passport office to issue passports for international travel, the cluster needs an OIDC issuer to provide identity tokens that Azure AD recognizes for PostgreSQL authentication.</p> <p>Using Azure CLI</p> <pre><code># Enable workload identity on existing cluster\naz aks update \\\n  --resource-group myResourceGroup \\\n  --name myAKSCluster \\\n  --enable-workload-identity \\\n  --enable-oidc-issuer\n</code></pre> <p>Using Terraform</p> <pre><code>resource \"azurerm_kubernetes_cluster\" \"aks\" {\n  name                = \"myAKSCluster\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  workload_identity_enabled = true\n  oidc_issuer_enabled      = true\n\n  # ... other configuration\n}\n</code></pre> <p>Verification Commands</p> <pre><code># Verify workload identity is enabled\naz aks show -g myResourceGroup -n myAKSCluster --query \"securityProfile.workloadIdentity.enabled\" -o tsv\n# Expected output: true\n\n# Verify OIDC issuer is enabled and get the URL\naz aks show -g myResourceGroup -n myAKSCluster --query \"oidcIssuerProfile.issuerUrl\" -o tsv\n# Expected output: https://oidc.prod-aks.azure.com/[tenant-id]/[unique-id]/\n\n# Check cluster addon status\naz aks show -g myResourceGroup -n myAKSCluster --query \"addonProfiles.azureKeyvaultSecretsProvider.enabled\" -o tsv\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-2-create-user-assigned-managed-identity","title":"Step 2: Create User-Assigned Managed Identity","text":"<p>The next step involves creating the Azure AD identity that enables passwordless authentication to PostgreSQL, completely eliminating traditional username/password credentials. PostgreSQL with Azure AD authentication requires an Azure AD principal to connect, and the managed identity fulfills this requirement.</p> <p>This is essentially creating a \"service account\" in Azure AD specifically for the application. Just like you might create a dedicated database user for an application, you're creating a dedicated Azure AD identity. The key difference is this identity doesn't have a password - it uses cryptographic tokens for passwordless authentication, eliminating the need to store or manage any credentials in the application or Kubernetes secrets.</p> <pre><code># Create a user-assigned managed identity\naz identity create \\\n  --name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --location myLocation\n\n# Get the identity client ID\nexport USER_ASSIGNED_CLIENT_ID=\"$(az identity show --name myAppIdentity --resource-group myResourceGroup --query 'clientId' -o tsv)\"\n</code></pre> <p>Verification Commands</p> <pre><code># Verify the managed identity was created successfully\naz identity show --name myAppIdentity --resource-group myResourceGroup --query '{name:name, clientId:clientId, principalId:principalId, resourceGroup:resourceGroup}' -o table\n# Expected output: Table showing identity details\n\n# List all managed identities in the resource group\naz identity list --resource-group myResourceGroup --query '[].{name:name, clientId:clientId}' -o table\n\n# Verify environment variable is set\necho \"CLIENT_ID: $USER_ASSIGNED_CLIENT_ID\"\n# Expected output: CLIENT_ID: [guid-format-client-id]\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-3-create-kubernetes-service-account","title":"Step 3: Create Kubernetes Service Account","text":"<p>The Kubernetes service account acts as the bridge between the pods and the Azure managed identity. Pods need a way to signal that they want to use workload identity for authentication, and the service account provides this mechanism through specific annotations that tell the workload identity system which Azure identity to use.</p> <p>Think of this as giving the pod a passport that identifies it as belonging to a specific country (managed identity) and having the right to travel to certain destinations (database access). When the pod starts, Kubernetes sees this passport and automatically provides the necessary authentication tokens.</p> <p>Create the service account with workload identity annotations:</p> <pre><code># postgresql-service-account.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: postgresql-workload-identity-sa\n  namespace: default\n  annotations:\n    azure.workload.identity/client-id: &lt;USER_ASSIGNED_CLIENT_ID&gt;\n  labels:\n    azure.workload.identity/use: \"true\"\n</code></pre> <p>Apply the service account: <pre><code># Create the service account\nkubectl apply -f postgresql-service-account.yaml\n</code></pre></p> <p>Verification Commands</p> <pre><code># Verify the service account was created with correct annotations\nkubectl get serviceaccount postgresql-workload-identity-sa -n default -o yaml\n# Expected output: ServiceAccount with azure.workload.identity annotations\n\n# Check service account annotations specifically\nkubectl get serviceaccount postgresql-workload-identity-sa -n default -o jsonpath='{.metadata.annotations}' | jq\n# Expected output: JSON showing azure.workload.identity/client-id and azure.workload.identity/use annotations\n\n# List all service accounts in the namespace\nkubectl get serviceaccounts -n default\n# Expected output: List including postgresql-workload-identity-sa\n\n# Verify the client ID annotation matches your managed identity\nkubectl get serviceaccount postgresql-workload-identity-sa -n default -o jsonpath='{.metadata.annotations.azure\\.workload\\.identity/client-id}'\necho \"\"\necho \"Expected: $USER_ASSIGNED_CLIENT_ID\"\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-4-create-federated-identity-credential","title":"Step 4: Create Federated Identity Credential","text":"<p>Federated credentials establish the trust relationship between the Kubernetes service account and the Azure managed identity - this is the core of workload identity federation. Azure AD needs explicit permission to trust tokens from the AKS cluster, and the federated credential provides this by saying \"if a token comes from this specific OIDC issuer (cluster) and claims to be this specific service account, then treat it as this managed identity.\"</p> <p>This is like registering the country's passport system with international border control. You're telling Azure AD \"when someone presents a passport from our Kubernetes cluster claiming to be the 'postgresql-workload-identity-sa' service account from the default namespace, trust them as if they're our managed identity citizen.\"</p> <p>The OIDC issuer URL is a unique identifier that Azure AD uses to verify identity tokens are coming from the specific AKS cluster. Each AKS cluster has a unique issuer URL that serves as its \"digital fingerprint\" in the identity federation process. Think of this URL as the official address of the cluster's passport office - when the pod presents an identity token to PostgreSQL, Azure AD checks this URL to confirm the token came from the trusted cluster before allowing database access.</p> <p>Using Azure CLI</p> <pre><code># Get the OIDC issuer URL from the AKS cluster\nexport AKS_OIDC_ISSUER=\"$(az aks show -n myAKSCluster -g myResourceGroup --query \"oidcIssuerProfile.issuerUrl\" -o tsv)\"\necho $AKS_OIDC_ISSUER\n\n# Create federated identity credential\naz identity federated-credential create \\\n  --name myAppFederatedCredential \\\n  --identity-name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --issuer $AKS_OIDC_ISSUER \\\n  --subject system:serviceaccount:default:postgresql-workload-identity-sa\n</code></pre> <p>Using Terraform</p> <pre><code># Get the OIDC issuer URL from the AKS cluster\ndata \"azurerm_kubernetes_cluster\" \"aks\" {\n  name                = \"myAKSCluster\"\n  resource_group_name = \"myResourceGroup\"\n}\n\n# Reference the existing managed identity\ndata \"azurerm_user_assigned_identity\" \"app_identity\" {\n  name                = \"myAppIdentity\"\n  resource_group_name = \"myResourceGroup\"\n}\n\n# Create federated identity credential\nresource \"azurerm_federated_identity_credential\" \"app_federated_credential\" {\n  name                = \"myAppFederatedCredential\"\n  resource_group_name = data.azurerm_user_assigned_identity.app_identity.resource_group_name\n  audience            = [\"api://AzureADTokenExchange\"]\n  issuer              = data.azurerm_kubernetes_cluster.aks.oidc_issuer_url\n  parent_id           = data.azurerm_user_assigned_identity.app_identity.id\n  subject             = \"system:serviceaccount:default:postgresql-workload-identity-sa\"\n}\n</code></pre> <p>Note: The <code>audience</code> is set to <code>[\"api://AzureADTokenExchange\"]</code> which is the standard audience for Azure AD workload identity federation. The <code>subject</code> follows the Kubernetes service account format: <code>system:serviceaccount:&lt;namespace&gt;:&lt;service-account-name&gt;</code>.</p> <p>Verification Commands</p> <pre><code># Verify the federated credential was created successfully\naz identity federated-credential show \\\n  --name myAppFederatedCredential \\\n  --identity-name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --query '{name:name, issuer:issuer, subject:subject, audience:audiences}' -o table\n\n# List all federated credentials for the managed identity\naz identity federated-credential list \\\n  --identity-name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --query '[].{name:name, subject:subject, issuer:issuer}' -o table\n\n# Verify the OIDC issuer URL matches\necho \"OIDC Issuer from cluster: $AKS_OIDC_ISSUER\"\naz identity federated-credential show \\\n  --name myAppFederatedCredential \\\n  --identity-name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --query 'issuer' -o tsv\n# Expected output: Both URLs should match\n\n# Verify the subject format is correct\naz identity federated-credential show \\\n  --name myAppFederatedCredential \\\n  --identity-name myAppIdentity \\\n  --resource-group myResourceGroup \\\n  --query 'subject' -o tsv\n# Expected output: system:serviceaccount:default:postgresql-workload-identity-sa\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-5-configure-postgresql-access","title":"Step 5: Configure PostgreSQL Access","text":"<p>Now you need to grant the managed identity access to the PostgreSQL server. Even though you have a managed identity, PostgreSQL doesn't automatically know about it. You must register the managed identity as an Azure AD administrator on the PostgreSQL server, which enables it to create database connections using Azure AD authentication instead of traditional passwords.</p> <p>This is essentially adding the service account to the database's user directory. You're telling PostgreSQL \"this Azure AD identity is authorized to connect, and when it does, treat it as a legitimate database user with appropriate permissions.\"</p> <pre><code># Grant the managed identity access to PostgreSQL\naz postgres flexible-server ad-admin set \\\n  --resource-group myResourceGroup \\\n  --server-name myPostgreSQLServer \\\n  --object-id $(az identity show --name myAppIdentity --resource-group myResourceGroup --query 'principalId' -o tsv) \\\n  --display-name myAppIdentity \\\n  --type User\n</code></pre> <p>Note: After configuring the managed identity as AD admin, you may need to connect to PostgreSQL and create specific database users and grant appropriate permissions:</p> <pre><code>-- Connect as AD admin and create user for managed identity\nCREATE USER \"myAppIdentity\" WITH LOGIN;\nGRANT CONNECT ON DATABASE mydatabase TO \"myAppIdentity\";\nGRANT USAGE ON SCHEMA public TO \"myAppIdentity\";\n-- Grant specific permissions based on application needs\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO \"myAppIdentity\";\n</code></pre> <p>Verification Commands</p> <pre><code># Verify the managed identity is set as AD admin\naz postgres flexible-server ad-admin show \\\n  --resource-group myResourceGroup \\\n  --server-name myPostgreSQLServer \\\n  --query '{login:login, sid:sid, tenantId:tenantId}' -o table\n\n# Check if Azure AD authentication is enabled\naz postgres flexible-server show \\\n  --resource-group myResourceGroup \\\n  --name myPostgreSQLServer \\\n  --query 'authConfig.activeDirectoryAuth' -o tsv\n# Expected output: Enabled\n\n# Verify the managed identity object ID matches\nMANAGED_IDENTITY_PRINCIPAL_ID=$(az identity show --name myAppIdentity --resource-group myResourceGroup --query 'principalId' -o tsv)\necho \"Managed Identity Principal ID: $MANAGED_IDENTITY_PRINCIPAL_ID\"\n\naz postgres flexible-server ad-admin show \\\n  --resource-group myResourceGroup \\\n  --server-name myPostgreSQLServer \\\n  --query 'sid' -o tsv\n# Expected output: Should match the Principal ID above\n\n# Test PostgreSQL connectivity (requires psql client)\n# Replace &lt;your-tenant-id&gt; with actual tenant ID\n# az postgres flexible-server execute \\\n#   --name myPostgreSQLServer \\\n#   --resource-group myResourceGroup \\\n#   --admin-user myAppIdentity \\\n#   --database-name mydatabase \\\n#   --querytext \"SELECT current_user;\"\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-6-deploy-the-application","title":"Step 6: Deploy the Application","text":"<p>This is where everything comes together. You need to configure the application deployment to use the workload identity service account and provide the necessary environment variables for Azure AD authentication to PostgreSQL. The pods must be explicitly configured to use the service account you created, and the application code needs to know the Azure client ID and tenant ID to properly authenticate with Azure AD before connecting to PostgreSQL.</p> <p>Notice that you're providing the managed identity name as the database user, but no password - the workload identity system with federated credentials handles authentication automatically. You're essentially telling Kubernetes \"when you run this pod, give it the identity (service account) we set up, and the federated credential system will handle token exchange to prove its identity to PostgreSQL.\"</p> <p>Key Workload Identity Configuration Points:</p> <pre><code># api-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dotnet-api\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dotnet-api\n  template:\n    metadata:\n      labels:\n        app: dotnet-api\n        # \ud83d\udd11 REQUIRED: This label tells the workload identity webhook to inject environment variables\n        azure.workload.identity/use: \"true\"\n    spec:\n      # \ud83d\udd11 REQUIRED: Reference the workload identity service account\n      serviceAccountName: postgresql-workload-identity-sa\n      containers:\n      - name: api-app\n        image: your-registry.azurecr.io/myapp-api:latest\n        ports:\n        - containerPort: 8080\n        env:\n        # \ud83d\udd11 APPLICATION-SPECIFIC: Connection string uses managed identity name as username, NO PASSWORD\n        - name: DATABASE_URL\n          value: \"Host=myPostgreSQLServer.postgres.database.azure.com;Database=mydatabase;Username=myAppIdentity;SSL Mode=Require;\"\n        # Note: AZURE_CLIENT_ID, AZURE_TENANT_ID, and AZURE_FEDERATED_TOKEN_FILE are automatically \n        # injected by the workload identity webhook when azure.workload.identity/use: \"true\" label is present\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n</code></pre> <p>Critical Configuration Changes:</p> <ol> <li><code>serviceAccountName: postgresql-workload-identity-sa</code> - Links the pod to the workload identity service account</li> <li>Automatic Environment Variable Injection - The workload identity webhook automatically injects <code>AZURE_CLIENT_ID</code>, <code>AZURE_TENANT_ID</code>, and <code>AZURE_FEDERATED_TOKEN_FILE</code> when the pod has the <code>azure.workload.identity/use: \"true\"</code> label</li> <li>Connection String - Uses the managed identity name (<code>myAppIdentity</code>) as the username with no password specified</li> </ol> <p>Important: The <code>azure.workload.identity/use: \"true\"</code> label is required in two places: 1. Service Account: Identifies the service account as workload identity enabled 2. Pod Template (in Deployment): Tells the workload identity mutating webhook which pods to inject configuration into</p> <p>The webhook looks for this label on the pod template to: - Inject environment variables (<code>AZURE_CLIENT_ID</code>, <code>AZURE_TENANT_ID</code>, <code>AZURE_FEDERATED_TOKEN_FILE</code>) - Mount the projected service account token at <code>/var/run/secrets/azure/tokens/azure-identity-token</code> - Configure the pod for automatic token exchange</p> <p>The workload identity system with federated credentials automatically handles token exchange and injects the necessary configuration into the pod, eliminating the need for password-based authentication.</p> <p>Verification Commands</p> <pre><code># Apply the deployment\nkubectl apply -f api-deployment.yaml\n\n# Verify the deployment was created successfully\nkubectl get deployment dotnet-api -n default -o wide\n# Expected output: Deployment with READY status\n\n# Check if pods are running with the correct service account\nkubectl get pods -n default -l app=dotnet-api -o jsonpath='{.items[*].spec.serviceAccountName}'\n# Expected output: postgresql-workload-identity-sa\n\n# Check pod events for any issues\nkubectl describe pods -n default -l app=dotnet-api\n\n# Verify workload identity webhook injected the necessary volumes and environment variables\nkubectl get pods -n default -l app=dotnet-api -o yaml | grep -A 10 -B 10 \"azure-workload-identity\"\n\n# Check if the pod has the projected service account token\nkubectl exec -it deployment/dotnet-api -n default -- ls -la /var/run/secrets/azure/tokens/\n\n# Verify injected environment variables (automatically added by webhook)\nkubectl exec -it deployment/dotnet-api -n default -- env | grep AZURE\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#step-7-verification","title":"Step 7: Verification","text":"<p>Finally, you need to validate that the entire workload identity chain is functioning correctly and the pods can successfully authenticate to PostgreSQL without credentials. Since workload identity involves multiple components working together (OIDC issuer, service accounts, federated credentials, managed identity, PostgreSQL AD admin), verification helps identify which component might be misconfigured if authentication fails.</p> <p>Think of this as testing the passport system. You're checking that: 1) the passport is valid and recognized (token acquisition), 2) border control accepts the passport (Azure AD trusts the token), and 3) you can actually enter the country and access what you need (PostgreSQL connection).</p> <pre><code># Check if workload identity is working\nkubectl logs deployment/dotnet-api -n default\n\n# Test connection from within the pod\nkubectl exec -it deployment/dotnet-api -n default -- /bin/bash\n\n# Inside the pod, test Azure AD token acquisition\ncurl -H \"Metadata: true\" \"http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&amp;resource=https://ossrdbms-aad.database.windows.net/\"\n</code></pre> <p>Additional Verification Commands</p> <pre><code># Check overall workload identity integration\nkubectl get pods -n default -l app=dotnet-api -o jsonpath='{.items[0].metadata.labels.azure\\.workload\\.identity/use}'\n# Expected output: true\n\n# Verify the webhook mutations were applied\nkubectl get mutatingwebhookconfiguration azure-wi-webhook-mutating-webhook-configuration -o yaml\n\n# Check application logs for authentication success/failure\nkubectl logs deployment/dotnet-api -n default --tail=50\n\n# Verify end-to-end connectivity (if your app has a health endpoint)\nkubectl port-forward deployment/dotnet-api 8080:8080 -n default &amp;\nsleep 5\ncurl -f http://localhost:8080/health || echo \"Health check failed\"\npkill -f \"kubectl port-forward\"\n</code></pre>"},{"location":"articles/20251113.1-aks-workload-identity/#conclusion","title":"Conclusion","text":"<p>AKS workload identity with federated credentials provides a secure, scalable solution for passwordless authentication between Kubernetes pods and Azure Database for PostgreSQL. By leveraging OIDC federation, managed identities, and federated identity credentials, you can completely eliminate password-based authentication while maintaining fine-grained access control.</p> <p>This passwordless authentication approach significantly improves security by removing the need to store database passwords in application code or Kubernetes secrets, while providing seamless authentication through Azure AD integration and automatic token exchange via federated credentials. Organizations can now deploy applications with confidence, knowing that database credentials are never exposed or stored anywhere in the system.</p>"},{"location":"articles/20251113.1-aks-workload-identity/#references","title":"References","text":"<ul> <li>PostgreSQL Commands Cheat Sheet - Essential PostgreSQL commands and operations</li> <li>Workload Identity Overview</li> <li>Workload Identity Federation</li> <li>Azure AD Workload Identity</li> <li>Azure AD Workload Identity</li> <li>OIDC on AKS</li> <li>Workload Identity on AKS</li> <li>Azure Samples - Spring Boot Application</li> <li>Azure Samples - Spring Boot Application</li> <li>Spring configuration for passwordless authentication</li> <li>Configure AD authentication with PostgreSQL</li> <li>PostgreSQL Authentication with Managed Identity</li> <li>A great article about workload identity federation</li> <li>Connect your Kubernetes application to your database without any credentials (and securely)</li> </ul>"},{"location":"articles/app-availability-alert/","title":"Send Alerts When Website is Down - Azure Application Insights Availability Test","text":""},{"location":"articles/app-availability-alert/#introduction","title":"Introduction","text":"<p>Website downtime can be a critical issue for companies in industries such as Insurance, Healthcare, Finance, and Banking, especially for applications that are mission critical. It can lead to inconvenience for users and potentially result in significant financial losses. To proactively address this challenge, Azure Application Insights offers a powerful feature called availability tests. These tests enable you to monitor your website's availability and receive timely alerts if the site goes down.</p> <p>In this article, I will guide you through the steps to set up email, SMS, or voice notifications for your team members using Azure Application Insights.</p>"},{"location":"articles/app-availability-alert/#prerequisites","title":"Prerequisites","text":"<p>Before we look into the steps, make sure you have the following prerequisites are in place:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure</li> <li>A Application Insights associated to the web application.</li> </ul> <p>Now, let's get started!</p>"},{"location":"articles/app-availability-alert/#step-1-create-action-groups","title":"Step 1: Create Action Groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p> <p>look into my other article for more details on Create Action Groups</p> <p>Action Group &gt; Overview</p> <p></p>"},{"location":"articles/app-availability-alert/#step-2-add-standard-test-in-application-insights","title":"Step 2: Add Standard Test in Application Insights","text":"<p>here are the steps to \"Add Standard Test in Application Insights\" under the \"Availability\" left navigation:</p> <ol> <li> <p>Access Azure Portal</p> </li> <li> <p>Select your application insights</p> </li> <li>Navigate to availability from left nav</li> <li>Click on Add Standard Test</li> <li> <p>Configure Application URL</p> <ul> <li>Enter the complete URL of your website, including the protocol (e.g., https://www.example.com).</li> </ul> </li> <li> <p>Select test locations</p> <ul> <li> <p>In the \"Locations\" section, you can choose the geographic locations from where you want to run the availability test. Azure offers a variety of locations worldwide to ensure comprehensive coverage.</p> </li> <li> <p>Click on \"Add location\" to select additional testing locations if needed.</p> </li> </ul> </li> <li> <p>Configure HTTP response codes:</p> <ul> <li> <p>Under the \"Availability\" tab, you can specify the expected HTTP response codes that indicate your website is running correctly. These codes are used to determine whether the test passes or fails. - in this scenario select 200 status code</p> </li> <li> <p>You can also set up more advanced scenarios by configuring content checks in the \"Content\" tab.</p> </li> </ul> </li> <li> <p>Save and create test:</p> <ul> <li>After configuring the test settings, click the \"Create\" or \"Save\" button to create the availability test. Azure Application Insights will now regularly run this test to check the availability of your website from the selected locations.</li> </ul> </li> <li> <p>Verification:</p> <ul> <li>To verify that the test is working as expected, you can check the test results in the \"Availability\" section. It will display the status of the test and any detected availability issues.</li> </ul> </li> </ol> <p>Application Insights &gt; Availability</p> <p></p> <p>Availability &gt; Add Standard Test</p> <p></p> <p>Add Standard Test &gt; Results</p> <p></p>"},{"location":"articles/app-availability-alert/#step-3-create-alert-rule","title":"Step 3: Create Alert Rule","text":"<p>Now that your availability test is set up, it's time to create an alert rule to trigger notifications when issues arise. by default it already creates a Rule for this.</p> <p>Click on \"Open Rules Alerts page\"</p> <p>Update Alert Rule details like description, severity, automatically resolve alerts checkbox etc.. </p> <p>Configure Actions group</p> <ol> <li>Click on \"Select Action group\" section of the alert rule and choose \"Action groups.</li> <li>Associate the alert rule with the Action Group you created in Step 1.</li> </ol> <p>Alert Rule</p> <p></p> <p>Alert Rule &gt; Details</p> <p></p> <p>Alert Rule &gt; Scope, Action, Condition</p> <p></p>"},{"location":"articles/app-availability-alert/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can set up proactive monitoring and alerting for your website using Azure Application Insights availability tests. This ensures that your team members added in action groups are immediately notified when your website is downtime, allowing for quicker response times and minimizing user disruption.</p>"},{"location":"articles/app-availability-alert/#references","title":"References","text":"<p>Here are some helpful references for further information:</p> <ul> <li>Azure Application Insights Documentation</li> <li>Azure Action Groups Documentation</li> <li>Azure Monitor Alerts Documentation</li> </ul>"},{"location":"articles/app-service-publishing-profile/","title":"Interacting with Azure App Service using a publishing profile","text":""},{"location":"articles/app-service-publishing-profile/#introduction","title":"Introduction","text":"<p>Interacting with Azure App Service using a publishing profile is something you might need in certain scenarios, developers may need to automate tasks such as file management within their App Service.</p> <p>This article provides a step-by-step explanation of a PowerShell script designed to delete files from an Azure App Service using the Azure CLI and REST API.</p>"},{"location":"articles/app-service-publishing-profile/#the-scenario","title":"The Scenario","text":"<p>Consider a situation where you have files within a specific directory on your Azure App Service, and you want to automate the process of deleting these files. This could be part of a larger automation or maintenance workflow.</p>"},{"location":"articles/app-service-publishing-profile/#the-powershell-script","title":"The PowerShell Script","text":"<p>Let's break down the PowerShell script that accomplishes this task:</p> <pre><code># Get publishing profile for web application ( get credentials)\n\n$publishingProfile = az webapp deployment list-publishing-credentials -n $WebAppServiceName -g $ResourceGroupName `\n                        --query '{name:name, publishingUserName:publishingUserName, publishingPassword:publishingPassword}' | Out-String\n\n$publishingProfileObject = ConvertFrom-JSON -InputObject $publishingProfile\n\n# Create Base64 authorization header \n$username = $publishingProfileObject.publishingUserName\n$password = $publishingProfileObject.publishingPassword \n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes((\"{0}:{1}\" -f $username,$password)))\n\n# Define Request Body\n$bodyToPOST = @{   \n    command = \"find . -mindepth 1 -delete\"\n    dir = \"/home/site/wwwroot/App_Data/jobs/triggered/job1\"   \n}   \n\n# Splat all parameters together in $param   \n$param = @{   \n    Uri = \"https://$WebAppServiceName.scm.azurewebsites.net/api/command\"   \n    Headers = @{Authorization=(\"Basic {0}\" -f $base64AuthInfo)}   \n    Method = \"POST\"   \n    Body = (ConvertTo-Json $bodyToPOST)   \n    ContentType = \"application/json\"   \n}   \n\n# Invoke REST call   \nInvoke-RestMethod @param\n</code></pre> <p>Script Explanation</p> <ul> <li>Get Publishing Profile:</li> <li> <p>The script uses the <code>az</code> command-line interface to obtain publishing credentials for the specified Azure App Service. These credentials include a username and password required for authentication.</p> </li> <li> <p>Create Base64 Authorization Header:</p> </li> <li> <p>The script extracts the publishing username and password from the obtained publishing profile and creates a Base64-encoded authorization header. This header is used for authentication when making the REST API call.</p> </li> <li> <p>Define Request Body:</p> </li> <li> <p>The script defines a request body in the form of a hash table (<code>$bodyToPOST</code>). In this example, the command is set to \"find . -mindepth 1 -delete,\" which is a command to delete files in a specified directory (<code>dir</code>).</p> </li> <li> <p>Invoke REST Call:</p> </li> <li>Using splatting, the script combines parameters into the <code>$param</code> hash table for the <code>Invoke-RestMethod</code> cmdlet. Parameters include the URI (REST API endpoint), headers (including the authorization header), HTTP method (POST), request body (converted to JSON), and content type.</li> <li>The <code>Invoke-RestMethod</code> cmdlet is then used to make the REST API call to the Azure App Service's SCM (Site Control Manager) endpoint to execute the specified command.</li> </ul>"},{"location":"articles/app-service-publishing-profile/#conclusion","title":"Conclusion","text":"<p>This PowerShell script provides a practical example of automating file deletion within an Azure App Service. Developers can customize the script for their specific scenarios, incorporating it into larger automation workflows or maintenance tasks. By leveraging Azure CLI and REST APIs, this script showcases the flexibility and extensibility of Azure App Service for managing web applications in the cloud.</p>"},{"location":"articles/application-availability-alerts/","title":"How to Configure Alerts in Azure Application Insights?","text":"<p>Proactive monitoring and alerting are critical for maintaining the reliability and performance of web applications hosted on Azure. Azure Application Insights provides monitoring and alerting capabilities, allowing you to gain insights into your application's behavior and health. </p> <p>In this article, I will provide step by step instructions for configuring monitoring alerts in Azure Application Insights specifically for exceptions that occur in any web application.</p>"},{"location":"articles/application-availability-alerts/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following prerequisites:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure </li> <li>A Application Insights associated to the web application.</li> <li>A web application configured to send logs data to Azure Application Insights.</li> </ul>"},{"location":"articles/application-availability-alerts/#step-1-create-action-groups","title":"Step 1: Create action groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p> <p>Here are the steps to create an Action Group:</p> <ol> <li>Login into azure portal, select Monitoring or \"Application Insights\" -&gt; Alerts - &gt; Action Group - This will take you to the Action Groups page.</li> <li>Select Action groups &gt; Create.</li> <li>Select values for Subscription, Resource group, and Region.</li> <li>Enter a name for Action group name and Display name.</li> <li>In the \"Notifications\" tab, you can set up one or more notification methods for the action group. These can include email, SMS, voice, or other options. To configure a notification, do the following:</li> <li>In the \"Actions\" tab, you can define automated actions that should be taken when the action group is triggered. These actions can include running a Logic App, invoking a webhook, or sending an email.</li> <li>Add Tags (Optional)</li> <li>Review and Create</li> <li>Confirmation</li> </ol> <p>Azure will begin creating the action group. Once the creation process is complete, you will receive a confirmation message.</p> <p>This action group can be associated with alert rules from various Azure services to trigger notifications and automated responses based on specific conditions.</p>"},{"location":"articles/application-availability-alerts/#step-2-create-a-new-alert-rule","title":"Step 2: Create a new alert rule","text":"<ol> <li> <p>Application Insights &gt; Alerts</p> </li> <li> <p>Open the + Create menu, and select Alert rule.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-3-configure-alert-rule-details","title":"Step 3: Configure Alert Rule Details","text":"<ol> <li> <p>In the \"Basics\" tab of the alert rule creation wizard, provide a Name and Description for your alert rule.</p> </li> <li> <p>Under Resource, select the web application associated with the Application Insights instance you're monitoring.</p> </li> <li> <p>For the Condition, click on the \"Add condition\" button.</p> </li> <li> <p>In the condition configuration:</p> </li> <li> <p>Choose \"Custom log search\" as the signal type.</p> </li> <li> <p>Configure the query to filter exceptions. For example, you can use the following query to detect exceptions:      <pre><code>exceptions\n| where type == \"Microsoft.ApplicationInsights.Web.Exceptions.HandledException\"\n</code></pre></p> </li> <li> <p>Set the Aggregation type to \"Count.\"</p> </li> <li> <p>Define the Threshold value that will trigger the alert. For example, if you want to be alerted when there are more than 5 exceptions in a 5-minute window, set the threshold to 5.</p> </li> <li> <p>Set the Operator to \"Greater than.\"</p> </li> <li> <p>Configure the Severity and Alert logic according to your requirements.</p> </li> <li> <p>Click \"Done\" to save the condition.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-5-define-alert-details","title":"Step 5: Define Alert Details","text":"<ol> <li> <p>In the \"Alert details\" section, specify a Name for your alert instance.</p> </li> <li> <p>Set the Severity based on the criticality of the alert.</p> </li> <li> <p>Configure the Action group by selecting the action group you created earlier.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-6-review-and-create","title":"Step 6: Review and Create","text":"<ol> <li> <p>Review all the configurations to ensure they are accurate.</p> </li> <li> <p>Click \"Create alert rule\" to create the alert rule.</p> </li> </ol>"},{"location":"articles/application-availability-alerts/#step-7-testing-and-validation","title":"Step 7: Testing and Validation","text":"<p>Test your alert rule ensure it triggers as expected. To do this, you can deliberately introduce exceptions into your web application and monitor the Azure Application Insights interface for alerts.</p>"},{"location":"articles/application-availability-alerts/#conclusion","title":"Conclusion","text":"<p>Configuring monitoring alerts in Azure Application Insights for exceptions in web applications is a critical to ensure the reliability and performance of your applications. </p>"},{"location":"articles/application-availability-alerts/#references","title":"References","text":"<ul> <li>Create or edit a metric alert rule</li> </ul>"},{"location":"articles/azure-log-alerts/","title":"Setup Azure Logs Alerts &amp; Notifications for Application Exceptions","text":""},{"location":"articles/azure-log-alerts/#introduction","title":"Introduction","text":"<p>Monitoring and detecting application exceptions is crucial for maintaining the reliability and performance of your applications hosted on Azure. Azure Log Analytics and Azure Monitor provide powerful tools to help you achieve this. </p> <p>In this article, I will provide step by step instructions for setting up Azure Logs Alerts and Notifications specifically for application exceptions.</p>"},{"location":"articles/azure-log-alerts/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An Azure account with an active subscription.</li> <li>A web application hosted on Azure </li> <li>A Application Insights associated to the web application.</li> <li>A web application configured to send logs data to Azure Application Insights.</li> <li>An Azure Log Analytics workspace set up to collect logs from your application.</li> <li>Diagnostics Settings configured for your application's logs to send exceptions to the Log Analytics workspace.</li> </ul>"},{"location":"articles/azure-log-alerts/#step-1-create-action-groups","title":"Step 1: Create Action Groups","text":"<p>Action Group allows you to define a set of notification and automated actions that can be triggered by alerts from various Azure services.</p>"},{"location":"articles/azure-log-alerts/#step-11-action-groups-structure","title":"Step 1.1: Action Groups Structure","text":"<p>Before creating action groups, let me provide some valuable tips here.</p> <p>The number and structure of action groups you need to create can vary based on your application team size and the nature of your project. It's important to modify your approach to meet your specific requirements and needs.</p> <p>Here are a few suggestions:</p> <ol> <li> <p>Single Action Group for All Environments: You can opt for a single action group that covers all your environments. This approach simplifies management and is suitable for smaller teams.</p> </li> <li> <p>Separate Action Groups for Each Environment: You can set up separate action groups for individual environments. This provides more granularity and allows you to modify notifications and responses to each environment's unique characteristics.</p> </li> <li> <p>One for Non-Production and One for Production: For smaller teams, it's often practical to create two action groups\u2014one dedicated to non-production environments and the other focused on the production environment. This way, you can prioritize alerts and responses accordingly.</p> </li> </ol> <p>For larger applications with complex microservices architectures, consider creating separate action groups for each domain or team group. This approach ensures that alerts are directed to the right teams, facilitating efficient issue resolution.</p> <p>The structure and number of action groups you create should align with your team's size, project complexity, and operational requirements. </p>"},{"location":"articles/azure-log-alerts/#step-11-create-action-groups","title":"Step 1.1: Create Action Groups","text":"<p>Here is the list of steps to create action group:</p> <ol> <li> <p>Sign in to the Azure Portal.</p> </li> <li> <p>In the left navigation pane, click on \"Monitor.\"</p> </li> <li> <p>Under \"Alerts,\" click on \"Action groups.\"</p> </li> <li> <p>Click on the \"+ New action group\" button.</p> </li> <li> <p>Under Resource group, select the appropriate resource group.</p> </li> <li> <p>In the \"Basics\" tab, provide a unique Name and Short Name for the action group.</p> </li> <li> <p>In the \"Notifications\" section, configure the action to notify relevant stakeholders for non-production environments. You can set up email notifications, SMS, or other preferred communication channels.</p> </li> <li> <p>Click \"OK\" to create the action group.</p> </li> </ol> <p>Action Group</p> <p></p> <p>Action Group &gt; Basic</p> <p></p> <p>Action Group &gt; Notification</p> <p></p> <p>Action Group &gt; Tags</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-12-test-action-group","title":"Step 1.2: Test Action Group","text":"<p>Before proceeding to the alert rule setup, test the action group by simulating an alert. This will ensure that notifications are properly configured and reaching the intended recipients.</p> <p>Action Group &gt; Test</p> <p>Test Alerts</p> <p></p> <p>Email from mail box</p> <p></p> <p>Email from mail box</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-2-create-a-new-alert-rule","title":"Step 2: Create a new alert rule","text":"<ol> <li> <p>In the Azure Portal, under \"Monitor,\" click on \"Alert rules.\"</p> </li> <li> <p>Click the \"+ New alert rule\" button.</p> </li> </ol>"},{"location":"articles/azure-log-alerts/#step-21-setup-conditions","title":"Step 2.1: Setup Conditions","text":"<ol> <li> <p>In the \"Basics\" tab, choose the appropriate Resource type and Resource for your application.</p> </li> <li> <p>Under the \"Conditions\" section, click on \"+ Add condition.\"</p> </li> <li> <p>In the condition configuration: Choose \"Custom log search\" as the signal type.</p> </li> <li> <p>Click \"Done\" to save the condition.</p> </li> </ol> <p>Monitoring &gt; Alert Rule </p> <p></p> <p>or </p> <p>Web App or Api App &gt; Alert Rule </p> <p></p> <p>Alert Rule &gt; Condition</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-22-write-log-query","title":"Step 2.2: Write Log Query","text":"<p>The log query written should accurately filter and select the exceptions you want to monitor. Make sure it targets the relevant log data and time frame.</p> <p>Write a log query that selects application exceptions. For example:</p> <pre><code>exceptions\n| where timestamp &gt; ago(1h)\n</code></pre> <pre><code>AppServiceAppLogs\n| where TimeGenerated &gt; ago(30m)\n| order by TimeGenerated desc\n| take 100\n</code></pre> <p>Adjust the time range and query as per your requirements.</p> <p>Alert Rule &gt; Log Query</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-23-associate-action-group","title":"Step 2.3: Associate Action Group","text":"<ol> <li> <p>In the \"Actions\" section, click on \"+ Add action group.\"</p> </li> <li> <p>Choose the action group created for the specific environment (non-production or production) where the alert will be triggered.</p> </li> <li> <p>Configure the alert action settings, including the severity and threshold.</p> </li> <li> <p>Click \"OK\" to associate the action group with the alert rule.</p> </li> </ol> <p>Alert Rule &gt; Action Group</p> <p></p> <p>Alert Rule &gt; Details</p> <p></p> <p>Alert Rule &gt; Overvie</p> <p></p>"},{"location":"articles/azure-log-alerts/#step-24-test-alert-rule","title":"Step 2.4: Test Alert Rule","text":"<p>Before finalizing the alert rule, it's essential to test it by creating a simulated exception or by adjusting the log query to match actual exceptions. This ensures that alerts trigger as expected.</p> <p></p> <p></p>"},{"location":"articles/azure-log-alerts/#conclusion","title":"Conclusion","text":"<p>Setting up Azure Logs Alerts &amp; Notifications for Application Exceptions is a proactive approach to identifying and addressing issues in your applications. By following the steps outlined in this guide, you can ensure that relevant teams are promptly notified when exceptions occur, allowing for timely troubleshooting and maintenance.</p>"},{"location":"articles/azure-log-alerts/#references","title":"References","text":"<ul> <li>Azure Monitor Documentation</li> <li>Azure Log Analytics Documentation</li> <li>Azure Action Groups Documentation</li> <li>Create or edit a log alert rule</li> </ul>"},{"location":"articles/dev-containers-introduction/","title":"What are Development Containers?","text":""},{"location":"articles/dev-containers-introduction/#overview","title":"Overview","text":"<p>Modern software development often involves complex setups, dependencies, and configurations. Ensuring that every team member's development environment matches and keeping it consistent can be challenging. That's where Development Containers come into play.</p> <p>In this article, we will explore the fundamentals of <code>Dev Containers</code>. We'll try to understand what they are, why the <code>.devcontainer</code> folder is crucial, how Dev Containers work, and ultimately, we'll learn how to develop applications inside a Dev Container.</p>"},{"location":"articles/dev-containers-introduction/#prerequisites","title":"Prerequisites","text":"<p>Before look into Development Containers, it's helpful to have a basic understanding of application development, containerization, and version control systems like Git. also, you should have Docker and VS code installed on your local machine, as Development Containers often rely on Docker to create isolated development environments.</p>"},{"location":"articles/dev-containers-introduction/#what-is-dev-containers","title":"What is Dev Containers?","text":"<p><code>Development containers</code>, or <code>dev containers</code> also known as <code>Remote Container</code>, are a standardized approach to defining and managing development environments within containers. They encapsulate all the necessary tools, libraries, and configurations required for a specific development project. Dev Containers enable developers to work in a consistent environment, regardless of their local setup, operating system, or development machine.</p> <p>Using Dev Containers can significantly enhance the development experience by eliminating the setup overhead, ensuring consistency, and simplifying collaboration across teams. </p>"},{"location":"articles/dev-containers-introduction/#what-is-devcontainer-folder","title":"What is <code>.devcontainer</code> folder ?","text":"<p>The <code>.devcontainer</code> folder is a special directory in a project that is often used with the Visual Studio Code (VS Code) Dev Containers extension. This folder contains configuration files that define how the development container should be set up when a developer opens the project in VS Code. The configuration details include settings for the container image, runtime, extensions, environment variables, and more.</p> <p>The purpose of the <code>.devcontainer</code> folder is to encapsulate the development environment configuration as code. This enables developers to define and version their development environment settings alongside the project code. When someone opens the project in VS Code, the Dev Containers extension reads the configuration in the .devcontainer folder and automatically configures the development container accordingly.</p> <p><code>.devcontainer</code> folder typically include:</p> <ol> <li> <p>devcontainer.json:  The primary configuration file is <code>devcontainer.json.</code> This JSON file outlines the settings for the development container, specifying the Docker image, runtime, environment variables, user settings, and VS Code extensions to be installed.</p> </li> <li> <p>Dockerfile: Optionally, you may include a Dockerfile in the .devcontainer folder if you need to customize the base Docker image further. This file is used to build the image when the container is created.</p> </li> <li> <p>docker-compose.yml (Optional): If your project requires additional services or multiple containers, you can include a docker-compose.yml file to define the multi-container configuration.</p> </li> </ol>"},{"location":"articles/dev-containers-introduction/#how-dev-containers-works","title":"How Dev Containers works?","text":"<p>Dev Containers, often associated with Visual Studio Code's <code>Remote - Containers extension</code>, works by enabling developers to create and use containerized development environments. These environments are defined within a container and provide a consistent, reproducible setup for coding, building, and running applications. </p> <p>Here's a step-by-step explanation of how Dev Containers work:</p> <ul> <li> <p>Project Configuration: Developers create a special folder in their project named as <code>.devcontainer</code>. Inside this folder, configuration files are added to define the development environment.</p> </li> <li> <p>Configuration Files:  The primary configuration file is <code>devcontainer.json</code>. This JSON file specifies details about the development container, such as the Docker image to use, runtime settings, environment variables, and Visual Studio Code settings.</p> </li> <li> <p>Optional Dockerfile:  Optionally, a <code>Dockerfile</code> can be included in the <code>.devcontainer</code> folder. This file allows developers to customize the base Docker image further. It is used to build the container image when the development environment is created.</p> </li> <li> <p>Opening the Project in Visual Studio Code: Developers open the project in Visual Studio Code, and the presence of the <code>.devcontainer</code> folder is detected by the \"Remote - Containers\" extension.</p> </li> <li> <p>Extension Activation:  The \"Remote - Containers\" extension automatically recognizes the project as a Dev Container project and suggests reopening it in a containerized environment.</p> </li> <li> <p>Container Creation: When developers choose to reopen the project in a container, Visual Studio Code uses the information from the <code>devcontainer.json</code> file to create a Docker container that encapsulates the development environment.</p> </li> <li> <p>Mounting Project Files: Project files are mounted from the local file system into the container, allowing developers to work with their source code seamlessly.</p> </li> <li> <p>Extensions Installation: Visual Studio Code extensions specified in <code>devcontainer.json</code> are installed and run inside the container. This ensures that developers have the necessary tools and extensions for their development tasks.</p> </li> <li> <p>Running and Debugging:  Developers can run and debug their applications within the container. This allows them to test and iterate in an environment that mirrors production closely.</p> </li> </ul>"},{"location":"articles/dev-containers-introduction/#benefits-of-developing-applications-inside-a-dev-container","title":"Benefits of developing applications inside a Dev Container","text":"<p>Setup local development environment that leverages containerization through Microsoft's DevContainer mechanism allows developers to create and run their development environments within containers, providing a consistent and reproducible setup for coding, testing, and debugging. </p> <p>Here are some key benefits of a local development setup using Microsoft's DevContainer:</p> <ul> <li> <p>Container-Based Development: Developers use containers to encapsulate their development environments, ensuring consistency and reproducibility across different machines.</p> </li> <li> <p>Run Services and Databases together: All the necessary services, databases, and supporting components for the application are containerized. This includes running databases like PostgreSQL, MySQL, or services like Redis or RabbitMQ in separate containers.</p> </li> <li> <p>Consistent Development Environments: Developers benefit from a consistent development environment, minimizing the \"it works on my machine\" problem. Everyone working on the project uses the same containerized setup.</p> </li> <li> <p>Isolation and Portability:  Containerization provides isolation for services and dependencies, preventing conflicts between different projects. It also ensures portability, allowing developers to easily share their container configurations.</p> </li> <li> <p>Ease of Onboarding: New developers can quickly get started by cloning the repository and using the predefined Dev Container configuration. This streamlines the onboarding process, as developers don't need to spend time setting up dependencies manually.</p> </li> <li> <p>Integrated Development Environment (IDE) Support: Integrated Development Environments (IDEs) like Visual Studio Code support the \"Remote - Containers\" extension, allowing developers to seamlessly work with containerized environments.</p> </li> <li> <p>Version Control for Development Environments::  The <code>devcontainer.json</code> file, along with other configuration files like Dockerfiles, can be version controlled. This allows teams to track changes to the development environment settings and ensures a versioned and documented setup.</p> </li> <li> <p>Local Testing and Debugging: Developers can locally test and debug their applications within the containerized environment. This includes running and debugging services, APIs, and other components.</p> </li> <li> <p>Facilitates Microservices Development: Container-based development aligns well with microservices architecture. Each microservice can have its own containerized development environment, simplifying the overall development process for microservices-based applications.</p> </li> <li> <p>Docker Compose Integration: Docker Compose may be utilized to define and manage multi-container environments locally. It simplifies the orchestration of multiple containers needed for the complete development setup.</p> </li> </ul>"},{"location":"articles/dev-containers-introduction/#conclusion","title":"Conclusion","text":"<p>In conclusion, Development Containers are adds huge value in modern software development. They bring consistency, portability, isolation, and version control to development environments, making collaboration and project management more efficient. By adopting Dev Containers, development teams can streamline their workflows, reduce setup time, and ensure that everyone is on the same page when it comes to building and testing applications locally.</p>"},{"location":"articles/dev-containers-introduction/#references","title":"References","text":"<ul> <li>Development Containers</li> <li>Use a Docker container</li> <li>Beginner's Series to: Dev Containers</li> <li>Visual Studio Code Remote - Containers</li> </ul>"},{"location":"articles/dev-containers-local-setup/","title":"Local Development Setup with Dev Containers","text":""},{"location":"articles/dev-containers-local-setup/#overview","title":"Overview","text":"<p>In this lab, I will walk you through the process of setting up a local development environment using Dev Containers. <code>Dev Containers</code> provide a consistent and isolated environment for your development projects, ensuring a seamless and uniform experience across team members.</p> <p>If you're new to Dev Containers and want to learn more about the concept, please visit our website to explore - What are Development Containers </p>"},{"location":"articles/dev-containers-local-setup/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ol> <li>Step 1: Install Docker</li> <li>Step 2: Install the remote - containers extension</li> <li>Step 3: Create a new project</li> <li>Step 4: Add Dev Container configuration to a project</li> <li>Step 5: Running a project in a dev container</li> <li>Step 6: Verify the Setup</li> </ol>"},{"location":"articles/dev-containers-local-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure that you have the following prerequisites installed on your machine:</p> <ul> <li>Visual Studio Code</li> <li>(Optional) A Project in Azure DevOps</li> <li>(Optional) Git client tool</li> <li>Docker (If not installed, refer to \"Step 1: Install Docker\" below)</li> </ul>"},{"location":"articles/dev-containers-local-setup/#step-1-install-docker","title":"Step 1: Install Docker","text":"<p>If Docker is not already installed on your machine, follow these steps:</p> <ol> <li>Visit the Docker official website  to download and install Docker for your operating system.</li> <li>Complete the installation process by following the on-screen instructions.</li> <li>Verify the installation by running <code>docker --version</code> in your terminal or command prompt.</li> </ol> <p>Verify the docker installation by running following commands:</p> <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre>"},{"location":"articles/dev-containers-local-setup/#step-2-install-the-remote-containers-extension","title":"Step 2: Install the remote containers extension","text":"<p>To work effectively with Dev Containers in Visual Studio Code, you'll need the \"Remote - Containers\" extension:</p> <p>Open Visual Studio Code and navigate to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window (or press <code>Ctrl+Shift+X</code>). Search for \"Remote - Containers\" and install the extension provided by Microsoft.</p> <p></p>"},{"location":"articles/dev-containers-local-setup/#step-3-create-a-new-project","title":"Step 3: Create a new project","text":"<p>Now, you can start working on your project. You have the option to create a new project or open an existing one in Visual Studio Code.</p> <p>In this step, we will set up a new Node.js API project using a basic Express application as our example. To expedite the process, we'll utilize Express's scaffolding tool to generate the necessary directory structure and essential files.</p> <p>Open your terminal and execute the following commands:</p> <pre><code>$ npx express-generator --no-view src\n$ cd src\n$ npm install\n</code></pre> <p>npx express-generator:</p> <p>The npx express-generator command initializes the project, creating a structure that includes directories like 'bin' and 'routes'.</p> <p></p> <p>npm install:</p> <p>Ensure you run npm install to set up and configure all required Node.js modules.</p> <p>This step ensures that your project is equipped with the necessary dependencies, allowing seamless integration with Docker and efficient containerization of your Node.js application.</p> <p></p> <p>folder structure</p> <p>you've established the foundation for your Node.js API project, complete with a standardized directory structure and essential files.</p> <p></p> <p>This should have created a number of files in your directory, including bin and routes directories. Make sure to run npm install so that npm can get all of your Node.js modules set up and ready to use.</p>"},{"location":"articles/dev-containers-local-setup/#step-4-add-dev-container-configuration-to-a-project","title":"Step 4: Add Dev Container configuration to a project","text":"<p>In the root of your project, create a folder named <code>.devcontainer</code> if it doesn't already exist. Inside this folder, create a file named <code>devcontainer.json</code>. This file will contain the configuration for your Dev Container.</p> <p>Here is a basic example for a Node.js project:</p> <pre><code>// .devcontainer/devcontainer.json\n{\n  \"name\": \"Node.js Dev Container\",\n  \"image\": \"node:14\",\n  \"extensions\": [\"dbaeumer.vscode-eslint\"],\n  \"forwardPorts\": [3000],\n  \"settings\": {\n    \"terminal.integrated.shell.linux\": \"/bin/bash\"\n  }\n}\n</code></pre> <p>Adjust the configuration according to your project's requirements and dependencies.</p> <p>Sample <code>docker-compose.yml</code> for Dev Containers</p> <p>Here is an example of a Docker Compose file defining a multi-container application with three services. The application consists of ASP.NET Core API services that depend on a SQL Server database. Docker Compose is utilized to orchestrate the deployment of these three containers.</p> <pre><code>version: '3'\n\nservices:\n  aspnet-api:\n    build:\n      context: ../aspnet-api\n      dockerfile: ../aspnet-api/Dockerfile\n      args:\n        - ARG1=value1\n        - ARG2=value2\n    container_name: aspnet-api-container\n    ports:\n      - \"80:80\"\n    networks:\n      - default\n    environment:\n      ASPNETCORE_ENVIRONMENT: Production\n      API_VERSION: v1\n    depends_on:\n      - sqlserver-db\n\n  aspnet-app:\n    build:\n      context: ../aspnet-app\n      dockerfile: ../aspnet-app/Dockerfile\n      args:\n        - ARG1=value3\n        - ARG2=value4\n    container_name: aspnet-app-container\n    ports:\n      - \"5000:5000\"\n    networks:\n      - default\n    environment:\n      ASPNETCORE_ENVIRONMENT: Development\n      APP_NAME: MyApp\n    depends_on:\n      - sqlserver-db\n\n  sqlserver-db:\n    build:\n      context: ../sqlserver-db\n      dockerfile: ../sqlserver-db/Dockerfile\n    container_name: sqlserver-db-container\n    environment:\n      SA_PASSWORD: YourStrongPassword\n    ports:\n      - \"1433:1433\"\n    networks:\n      - default\n    command: sh -c \"sleep 20 &amp;&amp; /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P YourStrongPassword -Q 'CREATE DATABASE YourDatabase'\"\n</code></pre> <p>Explanation:</p> <ul> <li>The <code>docker-compose.yml</code> file describes a multi-container application with three services: <code>aspnet-api</code>, <code>aspnet-app</code>, and <code>sqlserver-db</code>.</li> <li><code>aspnet-api</code> and <code>aspnet-app</code> are ASP.NET Core applications, each with its own Dockerfile and build arguments.</li> <li><code>sqlserver-db</code> is a SQL Server container with a specific command to initialize a database after starting.</li> <li>Services are connected to the default network for communication.</li> <li>Dependencies are specified using the <code>depends_on</code> key, ensuring that services wait for others to start before launching.</li> </ul>"},{"location":"articles/dev-containers-local-setup/#step-5-running-a-project-in-a-dev-container","title":"Step 5: Running a project in a dev container","text":"<p>Reopen a project in a container</p> <p>When you choose to \"Reopen in Container\" in Visual Studio Code, it triggers the Remote - Containers extension to rebuild and reopen your project within a containerized environment. </p> <p>Open the Command Palette (<code>Ctrl+Shift+P</code>), type \"Reopen in Container,\" and select the option to rebuild the project inside the Dev Container.</p>"},{"location":"articles/dev-containers-local-setup/#step-6-verify-the-setup","title":"Step 6: Verify the Setup","text":"<p>Once the container is built and the project is reopened, verify that your development environment is running smoothly inside the Dev Container.</p> <p>Now that your local development environment is containerized, you can start coding with the confidence that everyone on your team will have a consistent setup.</p>"},{"location":"articles/dev-containers-local-setup/#docker-compose-commands","title":"Docker compose commands","text":"<p>For more comprehensive details on Docker commands and some commonly used Docker Compose commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"articles/dev-containers-local-setup/#conclusion","title":"Conclusion","text":"<p>Developing applications inside a Dev Container offers a consistent, isolated, and reproducible development environment, streamlining your development workflow. With Docker and VS Code's Remote - Containers extension, you can quickly set up and manage Dev Containers, ensuring that your team works in agreement with the same development environment. This approach simplifies onboarding, enhances collaboration, and minimizes environment-related issues, allowing you to focus on building application development.</p>"},{"location":"articles/dev-containers-local-setup/#references","title":"References","text":"<ul> <li>Development Containers</li> <li>Use a Docker container</li> <li>Beginner's Series to: Dev Containers</li> </ul>"},{"location":"articles/docker-fundamentals/","title":"Exploring Docker Fundamentals","text":""},{"location":"articles/docker-fundamentals/#overview","title":"Overview","text":"<p>In this article, we'll explore the basics of Docker, which are like building blocks for understanding how containers work. Whether you're an experienced coder or just starting out, grasping these basics is essential for easily deploying applications in containers. These core concepts will come in handy as you continue your learning journey with docker.</p>"},{"location":"articles/docker-fundamentals/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a powerful platform that simplifies the process of developing, shipping, and running applications. Docker uses a technology known as containerization to encapsulate an application and its dependencies into a self-contained unit called a <code>container</code>. These containers are lightweight, portable, and consistent across different environments.</p>"},{"location":"articles/docker-fundamentals/#why-use-docker","title":"Why use Docker?","text":"<p>Docker simplifies the development, deployment, and management of applications, offering an adaptable solution for modern software development practices. Its popularity comes from from its ability to address challenges related to consistency, scalability, and efficiency in the software development lifecycle.</p> <p>Docker has become increasingly popular in the software development and IT industry due to its numerous advantages. Here are some key benefits of using Docker:</p> <ol> <li> <p>Portability:    Docker containers encapsulate applications and their dependencies, ensuring consistency across different environments. This portability eliminates the common problem of \"it works on my machine\" and facilitates seamless deployment across various systems.</p> </li> <li> <p>Isolation:    Containers provide a lightweight and isolated environment for applications. Each container runs independently, preventing conflicts between dependencies and ensuring that changes made in one container do not affect others.</p> </li> <li> <p>Efficiency:    Docker's containerization technology enables efficient resource utilization. Containers share the host OS kernel, making them lightweight compared to traditional virtual machines. This results in faster startup times and improved performance.</p> </li> <li> <p>Scalability:    Docker makes it easy to scale applications horizontally by running multiple instances of containers. This scalability allows developers to change the workloads and ensures optimal resource utilization.</p> </li> <li> <p>Microservices architecture:    Docker is integral to the microservices architecture, where applications are composed of small, independently deployable services. Containers facilitate the development, deployment, and scaling of microservices, enabling agility and ease of management.</p> </li> <li> <p>DevOps integration:    Docker aligns well with DevOps practices by promoting collaboration between development and operations teams. Containers can be easily integrated into continuous integration and continuous deployment (CI/CD) pipelines, streamlining the software delivery process.</p> </li> <li> <p>Community support:    Docker's community offers lot of pre-made tools and solutions, helping developers work faster and learn from others.</p> </li> <li> <p>Security:     Docker provides built-in security features, such as isolation and resource constraints, to enhance application security. </p> </li> <li> <p>Cross-platform compatibility:     Docker containers can run on various operating systems, including Linux, Windows, and macOS. This cross-platform compatibility is beneficial for teams working in heterogeneous environments.</p> </li> </ol>"},{"location":"articles/docker-fundamentals/#docker-concepts","title":"Docker concepts","text":"<p>Understanding these basic concepts is essential for effectively working with Docker and leveraging its advantages in terms of portability, scalability, and consistency across different environments.  Here are basic concepts of Docker:</p> <ul> <li> <p>Containerization Containerization is a technology that allows you to package an application and its dependencies, including libraries and configuration files, into a single container image.</p> </li> <li> <p>Images An image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Docker images are used to create containers. They are built from a set of instructions called a Dockerfile.</p> </li> <li> <p>Dockerfile A Dockerfile is a text file that contains a set of instructions for building a Docker image. It specifies the base image, adds dependencies, copies files, and defines other settings necessary for the application to run.</p> </li> <li> <p>Containers Containers are instances of Docker images. They run in isolated environments, ensuring that the application behaves consistently across different environments. Containers share the host OS kernel but have their own file system, process space, and network interfaces.</p> </li> <li> <p>Registries Docker images can be stored and shared through registries. The default registry is Docker Hub, but private registries can also be used. Registries allow versioning, distribution, and collaboration on Docker images.</p> </li> <li> <p>Docker compose Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to define a multi-container application in a single file, specifying services, networks, and volumes.</p> </li> <li> <p>Docker engine Docker Engine is the core component that manages Docker containers. It includes a server, REST API, and a command-line interface (CLI). The Docker daemon runs on the host machine, and the Docker CLI communicates with it to build, run, and manage containers.</p> </li> <li> <p>Volumes Volumes provide a way for containers to persist data outside their lifecycle. They can be used to share data between containers or to persist data even if a container is stopped or removed.</p> </li> <li> <p>Networking Docker provides networking capabilities that allow containers to communicate with each other or with the external world. Containers can be connected to different networks, and ports can be mapped between the host and the containers.</p> </li> </ul>"},{"location":"articles/docker-fundamentals/#container-orchestration","title":"Container orchestration","text":"<p>Whether managing a small cluster or a large-scale production environment, adopting container orchestration is crucial for containerized applications. Here are some container orchestrations:</p> <ul> <li> <p>Kubernetes: Kubernetes is the most widely adopted container orchestration platform. It automates the deployment, scaling, and management of containerized applications, providing a robust and extensible framework.</p> </li> <li> <p>Docker Swarm: Docker Swarm is a native clustering and orchestration solution provided by Docker. While it may not be as feature-rich as Kubernetes, it offers simplicity and seamless integration with Docker.</p> </li> <li> <p>Amazon ECS: Amazon Elastic Container Service (ECS) is a fully managed container orchestration service provided by AWS. It integrates with other AWS services and is suitable for users already utilizing the AWS ecosystem.</p> </li> <li> <p>Azure Kubernetes Service (AKS): AKS is a managed Kubernetes service offered by Microsoft Azure. It simplifies the deployment and management of Kubernetes clusters in the Azure cloud.</p> </li> </ul>"},{"location":"articles/docker-fundamentals/#docker-desktop","title":"Docker Desktop","text":"<p>Docker Desktop is a powerful tool that provides a user-friendly interface and environment for developing, building, and testing applications using Docker containers on local machine. </p> <p>Docker Desktop provides a convenient environment for developers to work with containers on their personal machines.</p>"},{"location":"articles/docker-fundamentals/#install-docker","title":"Install Docker","text":"<p>Here are the steps to install Docker on a different operating systems:</p> <p>Windows:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Windows page.</li> <li>Click on the \"Download for Windows\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Follow the installation wizard, accepting the default options.</li> <li>The installer may require you to restart your computer.</li> </ul> <p>Enable Hyper-V (Windows 10 Pro/Enterprise):</p> <ul> <li>If you're running Windows 10 Pro or Enterprise, Docker Desktop will use Hyper-V for virtualization. Ensure that Hyper-V is enabled in the Windows Features.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, start Docker Desktop from the Start Menu.</li> <li>The Docker icon will appear in the system tray when Docker Desktop is running.</li> </ul> <p>macOS:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Mac page.</li> <li>Click on the \"Download for Mac\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Drag the Docker icon to the Applications folder.</li> <li>Launch Docker from Applications.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, Docker Desktop should start automatically.</li> <li>The Docker icon will appear in the menu bar when Docker Desktop is running.</li> </ul> <p>Verify Docker install:</p> <p>To verify that Docker is installed correctly, open a terminal and run the following command:</p> <pre><code>docker --version\n\n# or\ndocker version\n</code></pre> <p>If you notice this, it indicates that your Docker is not in a running status.</p> <pre><code>error during connect: this error may indicate that the docker daemon is not running: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version\": open //./pipe/docker_engine: The system cannot find the file specified.\nClient:\n Cloud integration: v1.0.35\n Version:           24.0.2\n API version:       1.43\n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n</code></pre> <p>After Docker desktop is started and if everything is set up correctly, you should see following message indicating that your Docker installation is working.</p> <pre><code>Client:\n Cloud integration: v1.0.35 \n Version:           24.0.2  \n API version:       1.43    \n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n\nServer: Docker Desktop 4.21.1 (114176)\n Engine:\n  Version:          24.0.2\n  API version:      1.43 (minimum version 1.12)\n  Go version:       go1.20.4\n  Git commit:       659604f\n  Built:            Thu May 25 21:52:17 2023\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.21\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\n runc:\n  Version:          1.1.7\n  GitCommit:        v1.1.7-0-g860f061\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>Docker is now installed on your machine, and you can start using it to containerize your applications.</p>"},{"location":"articles/docker-fundamentals/#docker-commands","title":"Docker Commands","text":"<p>For more comprehensive details on Docker commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"articles/docker-fundamentals/#conclusion","title":"Conclusion","text":"<p>Docker and containerization have changed the way we build and use application development. Now that you understand the basics of Docker, you're ready to dive deeper. Docker is straightforward and flexible, making it a great tool for developers. It ensures that your application works the same way in different situations, keeps things separate, and easily grows with your needs. So, go ahead and start your journey with containers.</p>"},{"location":"articles/docker-fundamentals/#references","title":"References","text":"<ul> <li>Getting started guide</li> <li>Docker images</li> <li>Docker Documentation</li> <li>Docker Hub</li> </ul>"},{"location":"articles/drupal/","title":"Getting Started with Drupal: A Beginner's Guide","text":"<p>Drupal is a free open-source powerful and flexible content management system (CMS) that allows you to create and manage websites. Whether you're a beginner or an experienced web developer, Drupal can be an excellent choice for building websites, from personal blogs to large enterprise-level applications. </p> <p>In this article, we'll walk you through the basics of getting started with Drupal. exploring what it is, its key features, and why it's essential to consider using it for your website development.</p>"},{"location":"articles/drupal/#what-is-drupal","title":"What is Drupal?","text":"<p>Drupal is a free open-source content management system (CMS) written in PHP and supported by global community of developers. Drupal offers a robust platform for creating dynamic and feature-rich websites. Drupal is suitable for a wide range of web projects, from personal blogs to enterprise-level websites and applications. Drupal allows you to create, organize, and manage content, making it a flexible tool for a wide range of web projects.</p>"},{"location":"articles/drupal/#why-choose-drupal","title":"Why Choose Drupal?","text":"<p>Here are some reasons why Drupal is a good choice for web development:</p> <ul> <li> <p>Flexibility: Drupal's modular architecture allows you to create custom content types, design layouts, and extend functionality through a vast library of contributed modules.</p> </li> <li> <p>Community: Drupal has community of developers, designers, and users who actively contribute to its growth. You'll find extensive documentation, forums, and support resources.</p> </li> <li> <p>Scalability: It's easy to scale your Drupal website as your needs grow. Whether you're running a small blog or a high-traffic e-commerce site, Drupal can handle it.</p> </li> <li> <p>Security: Drupal prioritizes security, offering robust features and frequent security updates to keep your website safe from vulnerabilities.</p> </li> <li> <p>Multilingual Support: Drupal provides multilingual capabilities out of the box, making it accessible for global audiences.</p> </li> </ul>"},{"location":"articles/drupal/#uses-cases-of-drupal","title":"Uses cases of Drupal","text":"<p>Drupal is a flexible content management system (CMS) with a wide range of applications across various industries. Its flexibility and scalability make it suitable for different use cases. </p> <p>Here are some common uses for Drupal:</p> <ul> <li> <p>Corporate Websites: Many businesses choose Drupal to build their corporate websites. Drupal's robust user management, content organization, and scalability make it an ideal choice for showcasing products, services, and company information.</p> </li> <li> <p>E-commerce: Drupal can power e-commerce websites with the help of modules like Drupal Commerce. It enables businesses to set up online stores, manage products, process payments, and provide a seamless shopping experience to customers.</p> </li> <li> <p>Blogs and News Portals: Drupal offers excellent support for blogging and news websites. Content authors can create, categorize, and publish articles easily. Features like tagging, commenting, and social media integration enhance reader engagement.</p> </li> <li> <p>Government Websites: Many government agencies and organizations rely on Drupal for their websites. Drupal's security features, accessibility compliance, and multilingual support meet government requirements for online services.</p> </li> <li> <p>Educational Institutions: Schools, colleges, and universities use Drupal to create websites for academic institutions. Drupal allows for the management of course content, student information, events, and alumni networks.</p> </li> <li> <p>Media and Entertainment: Drupal serves as a foundation for media and entertainment websites, including those for music, movies, and publications. It can handle multimedia content and provide content recommendations.</p> </li> <li> <p>Healthcare: The healthcare industry uses Drupal to create websites for medical practices, hospitals, and health information portals. Drupal's privacy features ensure compliance with healthcare data regulations.</p> </li> </ul>"},{"location":"articles/drupal/#key-features","title":"Key Features","text":"<p>Here are some of the key features of Drupal:</p> <ul> <li> <p>Content Management: Drupal allows users to easily create, manage, and organize content. You can define custom content types, such as articles, blog posts, events, and more, to suit your specific needs.</p> </li> <li> <p>Modular Architecture: Drupal's modular architecture allows you to extend its functionality by adding modules. There are thousands of contributed modules available in the Drupal community, enabling you to add features like e-commerce, SEO optimization, social media integration, and more.</p> </li> <li> <p>Themes and Design: Drupal offers a theming system that allows you to change the look and feel of your website. You can use pre-built themes or create custom themes to match your brand's identity.</p> </li> <li> <p>Multilingual Support: Drupal provides built-in multilingual support, making it easy to create websites in multiple languages. You can translate content, configure language-specific settings, and deliver a global user experience.</p> </li> <li> <p>User Management: Drupal allows you to define user roles and set permissions, ensuring that only authorized users can access and edit specific content or perform certain actions on the site.</p> </li> <li> <p>Scalability: Whether you're running a small blog or a large enterprise website, Drupal can scale to meet your needs. It can handle high-traffic websites and complex web applications.</p> </li> <li> <p>Security: Drupal prioritizes security and offers frequent security updates to protect your website from vulnerabilities. It has a dedicated security team and follows best practices for secure web development.</p> </li> <li> <p>SEO-Friendly: Drupal is SEO-friendly out of the box. It provides clean URLs, customizable meta tags, XML sitemaps, and other SEO features to help your website rank well in search engines.</p> </li> <li> <p>Community Support: Drupal has huge community of developers, designers, and users who contribute to its growth. You can find extensive documentation, forums, and support resources to assist you with any Drupal-related questions or issues.</p> </li> <li> <p>Customization: Drupal's flexibility allows you to create custom content types, modules, and themes to customize your website to your unique requirements. You can build highly customized solutions with Drupal.</p> </li> </ul>"},{"location":"articles/drupal/#drupals-versioning","title":"Drupal's Versioning","text":"<p>Understanding Drupal's versioning is importent for users and developers to know which version of Drupal they are using and what to expect in terms of features, support, and compatibility.</p> <p>Here's an overview of Drupal's versioning system:</p> <p>Major Versions</p> <ol> <li> <p>Drupal 7: Drupal 7 was a significant release known for its stability and contributed module ecosystem. It was widely used for many years. Drupal 7 reached its end of life on November 28, 2022, which means it no longer receives official support and updates from the Drupal community.</p> </li> <li> <p>Drupal 8: Drupal 8 introduced a modern architecture and significant changes, including the use of Symfony components. It focused on improving developer experience and site building capabilities. Drupal 8 reached its end of life on November 2, 2021.</p> </li> <li> <p>Drupal 9: Drupal 9 is an evolution of Drupal 8 rather than a completely new codebase. It removed deprecated code and introduced updated dependencies. Migrating from Drupal 8 to Drupal 9 is typically smoother than a major version transition in the past.</p> </li> <li> <p>Drupal 10: Drupal 10 improves content modeling, block management, menu and taxonomy organization, and permission administration.</p> </li> </ol> <p>Release Cycle</p> <p>Drupal follows a regular release cycle, with minor and patch releases to address bugs, security issues, and add minor improvements. The release cycle for major versions is as follows:</p> <ul> <li> <p>Major Version: Major versions introduce significant changes and new features. They are released approximately every five years.</p> </li> <li> <p>Minor Version: Minor versions are released every six months and include new features and improvements. These updates are backward-compatible within the same major version.</p> </li> <li> <p>Patch Version: Patch versions are released more frequently, often monthly, to address security vulnerabilities and critical bugs. They maintain backward compatibility within the minor version.</p> </li> </ul> <p>Versioning Example</p> <p>Let's take an example to understand Drupal's versioning:</p> <ul> <li>Drupal 8.9.3: In this version, \"8\" represents the major version (Drupal 8), \"9\" represents the minor version (8.9), and \"3\" represents the patch version (8.9.3). This release is part of the Drupal 8 series and includes bug fixes and security updates.</li> </ul> <p>Upgrade Paths</p> <p>To keep your Drupal website secure and up-to-date, it's important to plan for version upgrades. When major versions reach their end of life, it's advisable to upgrade to the next major version, as that's where active development and support are concentrated.</p> <p>For example, Drupal 7 users were encouraged to migrate to Drupal 9 once Drupal 7 reached end of life. Drupal provides tools and documentation to facilitate the migration process.</p>"},{"location":"articles/drupal/#drupals-high-level-components","title":"Drupal's high-level components","text":"<p>Drupal's high-level architecture consists of several key components that work together to deliver its functionality. Here are the high-level architecture components of Drupal:</p> <ul> <li> <p>Core: Drupal's core is the foundational software that includes essential features and functionalities. It provides the basic structure for building websites and web applications. Core components include the content management system, user management, access control, and API support.</p> </li> <li> <p>Modules: Modules are extensions that add specific features and functionality to Drupal. They can be contributed modules developed by the Drupal community or custom modules created to meet specific project requirements. Modules can extend Drupal's capabilities for content types, user roles, e-commerce, SEO, and more.</p> </li> <li> <p>Themes: Themes determine the visual appearance and layout of a Drupal website. Drupal supports theming to create custom designs and styles for your site. Themes can be customized or selected from pre-built themes available in the Drupal theme repository.</p> </li> <li> <p>Database: Drupal uses a relational database to store and manage content, configuration, and user data. It supports various database management systems like MySQL, PostgreSQL, and SQLite. The database stores content types, taxonomy, user profiles, and more.</p> </li> <li> <p>Entities: Entities are fundamental data objects in Drupal, representing items like nodes, users, and taxonomy terms. They have properties and fields, making them flexible and extensible. Entities allow you to define and manage different types of content.</p> </li> <li> <p>Fields: Fields are reusable data elements that can be attached to entities. They enable the creation of structured content with specific data types, such as text, numbers, images, and dates. Fields are used to define content types and capture data within them.</p> </li> <li> <p>Nodes: Nodes are a specific type of entity that represents content items. In Drupal, content is often organized as nodes, and each node belongs to a content type. Nodes can be articles, pages, blog posts, or any other structured content.</p> </li> <li> <p>Taxonomy: Taxonomy is a system for categorizing and organizing content. It allows you to create and manage vocabularies and terms to classify content. Taxonomy terms are used to tag and categorize nodes, making content organization and navigation easier.</p> </li> <li> <p>User Management: Drupal includes a robust user management system that allows you to define user roles, set permissions, and control user access to content and functionality. User profiles, authentication, and registration features are also included.</p> </li> <li> <p>APIs: Drupal provides APIs that enable developers to interact with and extend the platform's functionality. These APIs include the Entity API, Form API, Database API, RESTful Web Services, and more.</p> </li> <li> <p>Cache and Performance Optimization: Drupal includes caching mechanisms to improve website performance. Caching stores frequently accessed data, reducing server load and improving page load times. Performance optimization is a critical consideration in Drupal architecture.</p> </li> <li> <p>Security: Drupal prioritizes security and includes security features such as input validation, output sanitization, and protection against common web vulnerabilities. It also has a dedicated security team that releases timely security updates.</p> </li> <li> <p>Search: Drupal includes search functionality to enable users to search for content within the website. It supports search indexing and can be extended with modules like Apache Solr for more advanced search capabilities.</p> </li> <li> <p>Multilingual Support: Drupal provides built-in multilingual support, allowing you to create websites in multiple languages. It offers translation tools and configuration options for managing multilingual content.</p> </li> <li> <p>Content Workflow: Drupal offers content workflow management features, allowing users to create, review, and publish content through defined workflows. Content moderation, revision tracking, and approval processes can be configured.</p> </li> </ul>"},{"location":"articles/drupal/#php-role-in-drupal","title":"PHP Role in Drupal","text":"<p>Understanding PHP role in Drupal is importent for anyone looking to work with Drupal. Let's explore the PHP relationship with Drupal.</p> <p>Drupal is built using PHP. Here's how PHP is used in Drupal:</p> <ul> <li> <p>Themes: PHP is used to create and customize Drupal themes. Theme files often contain PHP code that generates HTML markup and dynamically renders content.</p> </li> <li> <p>Modules: Drupal modules are extensions that enhance the CMS's functionality. Modules are written in PHP and can be used to add features, create custom content types, and integrate with third-party services.</p> </li> <li> <p>Templates: PHP templates in Drupal are used to control the presentation layer of a website. They determine how content is displayed and can be customized to match the desired design.</p> </li> <li> <p>Custom Code: Developers can write custom PHP code to extend Drupal's capabilities. This includes creating custom modules, hooks, and plugins to help Drupal to specific project requirements.</p> </li> <li> <p>Database Interaction: PHP in Drupal interacts with the underlying database to store and retrieve content, configuration settings, and user data. Drupal uses the PHP Data Objects (PDO) API for secure database operations.</p> </li> <li> <p>User Authentication: PHP handles user authentication and access control in Drupal. It verifies user credentials and manages user sessions.</p> </li> <li> <p>Form Handling: Drupal relies on PHP for form generation and processing. PHP code is used to build and validate forms, handle user submissions, and process data.</p> </li> <li> <p>Dynamic Content: PHP is helpful in rendering dynamic content on Drupal websites. It allows for the generation of content based on user interactions and data from various sources. ich websites and web applications using this flexible CMS.</p> </li> </ul>"},{"location":"articles/drupal/#understanding-databases-and-drupal","title":"Understanding Databases and Drupal","text":"<p>Databases play a fundamental role in how Drupal manages, stores, and retrieves content, configuration, and user data. Understanding the relationship between databases and Drupal is key for  site builders, developers, and administrators to effectively working with the CMS.</p> <p>Whether you're setting up a small blog or a large enterprise website, a solid understanding Drupal's database structure is essential for building and maintaining successful Drupal projects.</p> <p>Here are some most commonly used DBMS systems with Drupal:</p> <ul> <li> <p>MySQL: MySQL is a popular open-source relational database system known for its speed and reliability. </p> </li> <li> <p>PostgreSQL: PostgreSQL is another robust open-source relational database system known for its advanced features and data integrity. Drupal has strong support for PostgreSQL.</p> </li> <li> <p>SQLite: SQLite is a self-contained, serverless, and lightweight relational database engine. It is often used for smaller Drupal installations and development environments.</p> </li> </ul> <p>Key components of Drupal's database architecture include:</p> <ul> <li> <p>Nodes: Content in Drupal, such as articles, pages, and custom content types, is stored as nodes in the database.</p> </li> <li> <p>Fields: Fields define the types of data that can be associated with nodes. They can include text, images, dates, and more.</p> </li> <li> <p>Taxonomy: Taxonomy vocabularies and terms are used to categorize and tag content. They are stored in the database and help organize content.</p> </li> <li> <p>Users and Permissions: User accounts, roles, and permissions are stored in the database, allowing Drupal to manage access control.</p> </li> <li> <p>Configuration Settings: Drupal's configuration settings are stored in the database, enabling site administrators to customize the CMS without code changes.</p> </li> </ul>"},{"location":"articles/drupal/#understanding-drush","title":"Understanding Drush","text":"<p>Drush (short for \"Drupal Shell\") is a command-line tool that enhances Drupal development, administration, and site management. It's a powerful utility that simplifies many tasks, making Drupal development more efficient and convenient. </p> <p>Understanding how to leverage Drush effectively can significantly boost your productivity when working with Drupal. Whether you're a developer streamlining your workflow or a site administrator managing multiple Drupal installations, Drush is a valuable tool for simplifying common tasks and enhancing your Drupal experience.</p> <p>Here's an overview of how Drush integrates with Drupal:</p> <p>What is Drush?</p> <p>Drush is a command-line interface (CLI) tool that provides a wide range of commands for managing Drupal websites. It allows you to perform various tasks, such as site installation, module management, database updates, and more, directly from the command line. Drush is especially popular among Drupal developers and administrators for its speed and flexibility.</p> <p>Installing Drush</p> <p>To use Drush, you need to install it on your local development environment or server. Drush can be installed globally or locally within a Drupal project. The installation method may vary depending on your operating system and preferences. Once installed, you can run Drush commands from your terminal or command prompt.</p> <p>Common Drush Commands</p> <p>Drush offers a vast array of commands, but here are some common tasks you can perform with Drush:</p> <ul> <li> <p>Site Installation: Drush can automate the process of installing Drupal. You can specify installation parameters, such as database credentials, site name, and admin user details, in a command.</p> </li> <li> <p>Module Management: You can use Drush to enable, disable, and update modules on your Drupal site. Drush can also download and install modules directly from Drupal.org or other sources.</p> </li> <li> <p>Theme Management: Drush simplifies theme management by allowing you to enable and set themes as the default from the command line.</p> </li> <li> <p>Database Updates: When Drupal core or contributed modules release updates that require database schema changes, Drush can run these updates quickly and efficiently.</p> </li> <li> <p>Configuration Export and Import: Drush facilitates the export and import of Drupal configuration settings, making it easier to manage configuration changes across different environments.</p> </li> <li> <p>Clearing Caches: Drush provides commands to clear various caches within Drupal, improving site performance and ensuring that changes take effect immediately.</p> </li> <li> <p>User and Role Management: You can create and manage users and roles using Drush commands, streamlining administrative tasks.</p> </li> <li> <p>Batch Processing: Drush can execute commands in batch mode, allowing you to perform tasks on large datasets efficiently.</p> </li> </ul> <p>Drush and Drupal Console</p> <p>Drupal Console is another CLI tool for Drupal, and while it shares some similarities with Drush, it focuses more on code generation and scaffolding for Drupal 8 and later versions. Depending on your project's requirements, you may choose to use either Drush or Drupal Console, or both, to enhance your Drupal development workflow.</p> <p>Scripting and Automation</p> <p>Drush's scripting capabilities allow developers to automate repetitive tasks and build custom scripts for managing Drupal sites. This is particularly useful for deployment processes and site maintenance.</p> <p>Community Support</p> <p>Drush has a robust and active community that regularly contributes to its development. You can find extensive documentation, tutorials, and community support to help you make the most of Drush's capabilities.</p>"},{"location":"articles/drupal/#understanding-multi-tenant-architecture-with-drupal","title":"Understanding Multi-tenant Architecture with Drupal","text":"<p>Multi-tenant architecture is a design approach that allows multiple independent clients, often referred to as \"tenants,\" to share a common software application while maintaining their isolation and customization. This architectural concept is particularly valuable for software platforms like Drupal when serving multiple clients, organizations, or websites from a single codebase and infrastructure. </p> <p>Understanding multi-tenant architecture with Drupal is importent for organizations that serve multiple clients or maintain various websites on a shared platform. Whether you're a SaaS provider, a managed hosting company, or a large organization, multi-tenancy can streamline management, reduce costs, and provide modified solutions to your tenants while maintaining the advantages of Drupal as a robust content management system.</p> <p>Here's an overview of multi-tenant architecture in the context of Drupal:</p> <p>What is Multi-tenant Architecture?</p> <p>Multi-tenant architecture, sometimes called \"Software as a Service\" (SaaS) architecture, is a model where a single instance of an application serves multiple tenants. In the case of Drupal, tenants can represent separate websites, organizations, or clients, each with distinct requirements, data, and customization needs. Despite sharing the same codebase and infrastructure, each tenant remains isolated and can have its configuration, content, and appearance.</p> <p>Key Components</p> <p>To understand multi-tenant architecture with Drupal, let's explore its key components:</p> <ul> <li> <p>Shared Codebase: All tenants share the same Drupal core code, contributed modules, and themes. This centralizes maintenance and ensures consistency across the platform.</p> </li> <li> <p>Tenant-Specific Configuration: Each tenant can have its configuration settings, including database tables, settings.php files, and even custom modules or themes change as per its needs.</p> </li> <li> <p>Shared Database or Separate Databases: Multi-tenancy can employ a shared database approach where all tenants share a single database or separate databases for each tenant, ensuring data isolation.</p> </li> <li> <p>Domain or Subdomain Mapping: Multi-tenant setups often involve domain or subdomain mapping, allowing each tenant to have its unique domain name while running on the same Drupal installation.</p> </li> <li> <p>Tenant-Specific Content and Users: While the codebase and shared resources are common, each tenant can have its content, users, and content structure.</p> </li> </ul> <p>Use Cases</p> <p>Multi-tenant architecture is applicable in various scenarios:</p> <ul> <li> <p>SaaS Applications: Organizations offering software solutions to multiple clients can use multi-tenancy to provide individual instances of their application with customized branding and configurations.</p> </li> <li> <p>Managed Hosting Providers: Hosting providers can offer multi-tenant Drupal hosting services, allowing customers to create and manage their websites while sharing underlying infrastructure.</p> </li> </ul> <p>Advantages</p> <p>Multi-tenant architecture with Drupal offers several advantages:</p> <ul> <li> <p>Cost-Efficiency: Sharing a common codebase and infrastructure reduces maintenance and hosting costs compared to managing separate instances.</p> </li> <li> <p>Centralized Updates: Drupal core updates, module updates, and security patches can be applied once to the shared codebase, ensuring consistency and security across all tenants.</p> </li> <li> <p>Scalability: Adding new tenants is straightforward, making multi-tenant Drupal platforms scalable as the client base grows.</p> </li> <li> <p>Customization: Each tenant can customize its appearance, content, and functionality within the defined constraints, providing flexibility while maintaining consistency.</p> </li> </ul> <p>Considerations</p> <p>When implementing multi-tenant architecture in Drupal, consider these important factors:</p> <ul> <li> <p>Security: Robust access control mechanisms and data isolation are crucial to maintaining security and preventing data leaks between tenants.</p> </li> <li> <p>Resource Management: Monitor resource usage to prevent one tenant's activities from affecting the performance of others.</p> </li> <li> <p>Backup and Recovery: Implement comprehensive backup and recovery strategies to safeguard data across all tenants.</p> </li> <li> <p>Governance: Define governance policies and procedures to manage tenant onboarding, customization, and support efficiently.</p> </li> </ul>"},{"location":"articles/drupal/#drupal-vs-sitecore","title":"Drupal vs Sitecore","text":"<p>Drupal and Sitecore are both content management systems (CMS), but they have significant differences in terms of their target audience, features, licensing, and use cases. </p> <p>Here's a comparison of Drupal and Sitecore:</p> <p>Target Audience:</p> <ul> <li>Drupal: Drupal is an open-source CMS that caters to a wide range of users, from small businesses and individual bloggers to large enterprises and government organizations. It's known for its flexibility and scalability, making it suitable for various types of websites and applications.</li> <li>Sitecore: Sitecore is primarily targeted at larger enterprises and organizations that require a robust and feature-rich CMS. It's often used by businesses with complex digital marketing needs.</li> </ul> <p>Licensing:</p> <ul> <li>Drupal: Drupal is open-source software. This means it's free to download, use, and modify. You only need to pay for hosting and any premium modules or themes you choose to use.</li> <li>Sitecore: Sitecore is a proprietary CMS, which means it requires a paid license to use. The cost of a Sitecore license can be substantial and is typically based on factors like the number of users and the scale of the implementation.</li> </ul> <p>Flexibility and Customization:</p> <ul> <li>Drupal: Drupal is renowned for its flexibility and extensibility. It offers a vast library of contributed modules and themes that allow you to customize your website extensively. Developers can create custom modules and themes to customize Drupal to specific needs.</li> <li>Sitecore: Sitecore is known for its robust digital experience platform (DXP) capabilities. It provides extensive out-of-the-box features for personalization, analytics, and marketing automation. While it's customizable, it's often considered less flexible than Drupal due to its emphasis on specific enterprise-level features.</li> </ul> <p>Scalability:</p> <ul> <li>Drupal: Drupal is highly scalable and can handle websites of all sizes. It's used for everything from small blogs to large, high-traffic sites. Its scalability depends on the hosting infrastructure and optimization.</li> <li>Sitecore: Sitecore is designed for enterprise-level scalability. It can manage large volumes of content, handle high traffic, and provide advanced analytics and personalization features that are essential for big organizations.</li> </ul> <p>Content Management Features:</p> <ul> <li>Drupal: Drupal provides essential content management features like content creation, editing, and versioning. While it can handle content workflows, it may require additional modules for more advanced content management needs.</li> <li>Sitecore: Sitecore offers advanced content management capabilities, including sophisticated content personalization, A/B testing, and marketing automation. It excels in managing complex content strategies.</li> </ul> <p>Learning curve:</p> <ul> <li>Drupal: Drupal can have a steeper learning curve, especially for beginners. Its flexibility and extensive features may require some time to master.</li> <li>Sitecore: Sitecore is known for its user-friendly interface and comprehensive documentation, making it relatively easier to learn for those with the right budget and resources.</li> </ul> <p>In summary, Drupal is a flexible and open-source CMS suitable for a wide range of users and projects, while Sitecore is an enterprise-level CMS with a focus on advanced digital marketing and personalization features. The choice between Drupal and Sitecore depends on your organization's specific requirements, budget, and expertise in managing and customizing these platforms.</p>"},{"location":"articles/drupal/#conclusion","title":"Conclusion","text":"<p>Drupal is a open-source content management system (CMS) that helps you to create websites that stand out in functionality and design. Whether you're a beginner or an experienced developer, Drupal's flexibility and community support make it an excellent choice for building dynamic and engaging websites. </p>"},{"location":"articles/drupal/#references","title":"References","text":"<ul> <li>Drupal Official Website</li> <li>Drupal Documentation</li> <li>Drupal Community Events</li> </ul>"},{"location":"articles/git-reset/","title":"Reset a branch to a specific tag in Git","text":""},{"location":"articles/git-reset/#introduction","title":"Introduction","text":"<p><code>git reset</code> is a powerful command in Git that allows you to undo changes in your working directory and reset the repository to a specific commit. This is useful when you want to discard commits and get back to a particular state in your project's history.</p>"},{"location":"articles/git-reset/#scenario","title":"Scenario","text":"<p>Let's consider a scenario where you have two branches: <code>main</code> and <code>develop</code>. The <code>main</code> branch was initially in a good state, but due to a mistake, some commits were accidentally merged into it instead of the <code>develop</code> branch. Now, you need to reset the <code>main</code> branch to its original state.</p>"},{"location":"articles/git-reset/#steps-to-reset-main-branch","title":"Steps to Reset <code>main</code> Branch","text":"<p>Follow these steps to reset the <code>main</code> branch:</p> <ol> <li> <p>Check the Status:    <pre><code>git status\n</code></pre></p> </li> <li> <p>Switch to <code>main</code> Branch:    <pre><code>git checkout main\n</code></pre></p> </li> <li> <p>View Commit History:    <pre><code>git log --oneline\n</code></pre></p> </li> <li> <p>Perform the Reset:    <pre><code>git reset --hard 592ac92\n</code></pre>    Replace <code>592ac92</code> with the commit hash you want to reset to.</p> </li> <li> <p>Force Push to Remote:    <pre><code>git push -f origin main\n</code></pre>    This forcefully overwrites the existing history in the upstream repository.</p> </li> </ol>"},{"location":"articles/git-reset/#conclusion","title":"Conclusion","text":"<p>Using <code>git reset</code> can be a lifesaver when you need to undo changes and get back to a specific commit. However, use it with caution, especially when force-pushing, as it can rewrite history.</p>"},{"location":"articles/hide-full-path/","title":"Hiding the full file path in VSCode Terminal","text":"<p>In this guide, we'll walk through the steps to hide the full file path and only show the current folder name in your PowerShell prompt.</p> <p>The customization involves modifying the PowerShell profile.</p>"},{"location":"articles/hide-full-path/#step-1-open-the-powershell-terminal-in-vscode","title":"Step 1: Open the PowerShell Terminal in VSCode","text":"<p>Launch Visual Studio Code and open the integrated terminal by selecting  <code>Terminal</code></p>"},{"location":"articles/hide-full-path/#step-2-open-the-powershell-profile","title":"Step 2: Open the PowerShell Profile","text":"<p>Type the following command into the terminal and press Enter:</p> <pre><code>code $PROFILE\n</code></pre> <p>or </p> <p>you can open the profile in <code>Notepad</code> by using the following command:</p> <pre><code>notepad $PROFILE\n</code></pre>"},{"location":"articles/hide-full-path/#step-3-add-custom-prompt-function","title":"Step 3: Add Custom Prompt Function","text":"<pre><code>Function Prompt { \"$( ( get-item $pwd).Name )&gt; \" }\n</code></pre> <p>or</p> <pre><code>function prompt {\n  $p = Split-Path -leaf -path (Get-Location)\n  \"$p&gt; \"\n}\n</code></pre> <p>This modification ensures that only the current folder name is displayed in the PowerShell prompt.</p>"},{"location":"articles/hide-full-path/#step-4-save-the-profile","title":"Step 4: Save the Profile","text":"<p>Save the changes to the profile file and close the editor.</p>"},{"location":"articles/hide-full-path/#step-5-restart-powershell","title":"Step 5: Restart PowerShell","text":"<p>To apply the changes, restart your PowerShell session.</p> <p>Now, when you open a new PowerShell terminal in Visual Studio Code, you will notice that the full file path is hidden, and only the current folder name is displayed in the prompt.</p> <p>before </p> <p></p> <p>after </p> <p></p>"},{"location":"articles/hide-full-path/#conclusion","title":"Conclusion","text":"<p>Customizing the PowerShell prompt in Visual Studio Code allows you to hide the full file path and displaying only the current folder name.</p>"},{"location":"articles/keycloak/","title":"Getting Started with Keycloak: A Beginner\u2019s Guide","text":""},{"location":"articles/keycloak/#introduction","title":"Introduction","text":"<p>In a microservices architecture, ensuring the security of applications and services (APIs) is importent. Unauthorized access to protected data can potentially cost millions of dollars in banking and financial IT companies. User authentication, authorization, and identity management are critical aspects of securing web and mobile applications for these companies. This is where Keycloak comes into play.</p> <p>This in this article I will walk you through the fundamentals of Keycloak, what is keycloak is, Key features, why do we need to use it, where do we use it, By the end of this guide, you'll have a solid understanding of how to leverage Keycloak to enhance the security and usability of your microservices applications.</p>"},{"location":"articles/keycloak/#what-is-keycloak","title":"What is Keycloak?","text":"<p><code>Keycloak</code> is an open-source identity and access management (IAM) solution developed by <code>Red Hat</code>. It offers a robust and flexible framework for securing your applications and services with modern authentication and authorization mechanisms, particularly beneficial in microservices architecture.</p> <p>Keycloak simplifies the process of managing user identities, ensuring only authorized users can access your resources. Whether you're developing a single-page web application, a mobile app, or a complex microservices architecture, Keycloak offers a centralized and highly customizable solution for handling authentication and access control.</p>"},{"location":"articles/keycloak/#key-features","title":"Key Features","text":"<p>Here are the Keycloak's main features in the context of modern web and mobile applications and microservices architectures.</p> <ol> <li> <p>Single Sign-On (SSO): Keycloak enables users to sign in once and gain access to multiple applications without the need to re-enter their credentials. </p> </li> <li> <p>Open source: Keycloak is an open-source project, making it accessible and customizable for various use cases.</p> </li> <li> <p>User federation: Keycloak can integrate with external identity providers, such as LDAP, Active Directory, or social media platforms, allowing you to leverage existing user databases and credentials.</p> </li> <li> <p>Role-Based Access Control: You can define fine-grained access control by assigning roles and permissions to users and applications, ensuring that users only access the resources they are authorized to.</p> </li> <li> <p>Multi-Factor authentication (MFA): Enhance security by implementing MFA, requiring users to provide multiple forms of verification before gaining access.</p> </li> <li> <p>OAuth 2.0 and openID connect: Keycloak supports modern identity and authorization protocols, making it suitable for both traditional and modern web application development.</p> </li> <li> <p>multi-tenant support: which allows you to efficiently manage and secure multiple tenants or organizations within a single Keycloak instance, providing isolation and customized access control for each tenant.</p> </li> <li> <p>Scalability and high availability: Keycloak can be deployed in a scalable and highly available configuration, making it suitable for enterprise-level applications.</p> </li> </ol>"},{"location":"articles/keycloak/#installing-keycloak","title":"Installing Keycloak","text":"<p>Setting up Keycloak can be done in various ways depending on your specific requirements and preferences. Here are some different methods and deployment options for setting up Keycloak:</p> <ol> <li> <p>Running as a container on Docker:    Keycloak is available as an official Docker image on Docker Hub. You can run Keycloak in a Docker container, making it easy to deploy and manage. Docker Compose can also be used for multi-container setups. This approach will also provides full control over the configuration and allows you to customize Keycloak to your needs.</p> </li> <li> <p>Running Keycloak on Kubernetes:    Deploying Keycloak on Kubernetes is another popular choice for container orchestration. Kubernetes provides scalability, high availability, and ease of management. Helm charts are available to simplify deployment on Kubernetes.</p> </li> <li> <p>Installing and running Keycloak locally:    Keycloak provides a standalone server distribution that's easy for developers to set up on their local development machines. This is useful for testing and development purposes.</p> </li> </ol>"},{"location":"articles/keycloak/#when-to-use-and-when-not-to-use-keycloak","title":"When to use and when not to use Keycloak?","text":"<p>Keycloak is a powerful identity and access management (IAM) solution, but it may not be the best fit for every situation. Here's when you should consider using Keycloak and when you might want to explore other options:</p> <p>When to Use Keycloak:</p> <ol> <li> <p>Multi-application environments: Use Keycloak when you have multiple applications or microservices that need centralized authentication and authorization. Keycloak allows you to manage user access across various applications from a single point.</p> </li> <li> <p>Single Sign-On (SSO) requirements: Keycloak is a good choice when you want to implement single sign-on (SSO) across multiple applications, allowing users to log in once and access multiple services without re-entering credentials.</p> </li> <li> <p>Open standards: If you prefer using open standards like OAuth 2.0, OpenID Connect, and SAML for identity and access management, Keycloak supports these protocols, making it suitable for integrating with a wide range of platforms and applications.</p> </li> <li> <p>Customization: When you need a flexible IAM solution that you can customize extensively to meet your organization's specific requirements, Keycloak offers a high degree of customization through its configuration options and extension points.</p> </li> <li> <p>Open source preference: If you prefer open-source solutions with no licensing costs, Keycloak is open-source and can be a cost-effective option.</p> </li> <li> <p>Enterprise needs: Red Hat Single Sign-On (RH-SSO), based on Keycloak, is a suitable choice for enterprises requiring commercial support, certification, and integration with other Red Hat products.</p> </li> <li> <p>Security and compliance: When security and compliance are paramount, Keycloak provides features for securing applications, auditing user activities, and enforcing access controls, helping you meet regulatory requirements.</p> </li> </ol> <p>When Not to Use Keycloak:</p> <ol> <li> <p>Simple authentication: For simple applications or websites that require basic authentication but don't need extensive identity management features, implementing a custom authentication solution or using a lightweight framework might be more straightforward and efficient.</p> </li> <li> <p>Lightweight requirements: If your project has minimal authentication and authorization needs and you don't require features like SSO, fine-grained access control, or user federation, using a full-fledged IAM solution like Keycloak might be overkill. You could opt for simpler authentication methods or third-party authentication providers.</p> </li> <li> <p>Legacy systems: In situations where integrating modern IAM solutions is challenging due to legacy systems or complex infrastructure, you may need to consider alternatives that can work more seamlessly with your existing setup.</p> </li> </ol>"},{"location":"articles/keycloak/#high-level-architecture-of-keycloak","title":"High-level architecture of Keycloak*","text":"<p>Here are the key architectural components of Keycloak:</p> <ul> <li> <p>Keycloak server:    The core component of Keycloak is the Keycloak Server itself. It is responsible for managing user identities, enforcing authentication and authorization policies, and serving as the central hub for all identity-related operations.</p> </li> <li> <p>Realms:    Realms are isolated security domains within Keycloak. Each realm defines its own set of users, roles, policies, and authentication settings. Realms are used to separate and organize different applications or services.</p> </li> <li> <p>Clients:    Clients represent applications or services that interact with Keycloak for authentication and authorization. Each client is associated with a specific realm and defines how the application interacts with Keycloak, including configuration details and security settings.</p> </li> <li> <p>Users and identity providers:    Keycloak allows you to manage user accounts directly within realms. You can also integrate external identity providers (IdPs), such as LDAP, Active Directory, or social media logins, to federate user identities.</p> </li> <li> <p>Authentication flows:    Authentication flows define the steps and mechanisms used to authenticate users. Keycloak supports various authentication methods and allows you to customize the flows to meet your security requirements.</p> </li> <li> <p>Authorization policies:    Authorization policies specify access control rules for protecting resources. Keycloak enables you to define policies based on roles, attributes, or custom logic, ensuring that only authorized users can access specific resources.</p> </li> <li> <p>Tokens:    Keycloak issues tokens, such as access tokens and ID tokens, to represent user authentication and authorization. These tokens are used by clients to access protected resources and services.</p> </li> <li> <p>Single Sign-On (SSO):    Keycloak provides single sign-on capabilities, allowing users to log in once and gain access to multiple applications without the need to re-enter credentials. SSO enhances user experience and security.</p> </li> <li> <p>Adapters and libraries:    Keycloak offers adapters and libraries for various programming languages and frameworks (e.g., Java, Node.js, Spring Boot). These components facilitate integration with Keycloak, enabling your applications to participate in the authentication and authorization process.</p> </li> <li> <p>Administration console:     The Keycloak Administration Console is a web-based interface for administrators to configure and manage realms, clients, users, roles, authentication flows, and other Keycloak settings.</p> </li> </ul>"},{"location":"articles/keycloak/#what-is-keycloak-admin-console","title":"What is Keycloak Admin Console?","text":"<p>The Keycloak Admin Console is a web-based administrative interface provided by Keycloak for managing and configuring the identity and access management (IAM) system. It serves as the primary graphical user interface (GUI) for administrators, allowing them to perform various tasks related to the setup and maintenance of Keycloak realms, clients, users, roles, authentication flows, and other IAM components for securing applications and services.</p> <p>Key features and functions of the Keycloak Admin Console include:</p> <ul> <li> <p>Realm management: Administrators can create, configure, and manage realms. Realms are isolated security domains that define user stores, authentication settings, and authorization policies for different applications or services.</p> </li> <li> <p>Client configuration: Within each realm, administrators can define client applications and configure their settings, including authentication methods, redirect URIs, and fine-grained security policies.</p> </li> <li> <p>User management: Users and their attributes can be managed within realms. Administrators can create, modify, or delete user accounts, as well as perform actions like resetting passwords and enabling or disabling users.</p> </li> <li> <p>Role management: Keycloak allows the creation of roles and role-based access control (RBAC) policies. Administrators can assign roles to users and specify which roles have access to specific resources.</p> </li> <li> <p>Authentication Flows: Administrators can customize and configure authentication flows for realms, defining how users authenticate and which authentication methods are used during login.</p> </li> <li> <p>Token configuration: Settings related to access tokens, refresh tokens, and identity tokens can be adjusted to meet security and performance requirements.</p> </li> <li> <p>Client scopes: Administrators can create and manage client scopes, which define the attributes and permissions associated with clients. Client scopes can be assigned to clients to influence their behavior.</p> </li> <li> <p>Sessions and Single Sign-On (SSO): Administrators can monitor user sessions, including active sessions, and manage single sign-on settings to enhance user experience and security.</p> </li> <li> <p>Logs and auditing: Keycloak provides logs and audit records within the Admin Console for administrators to track system events, user activities, and authentication events.</p> </li> <li> <p>Security configuration: Security settings, such as password policies, brute-force protection, and token validation, can be configured to ensure the IAM system's security.</p> </li> <li> <p>Themes and customization: The Admin Console's look and feel can be customized through themes and branding to match an organization's visual identity.</p> </li> <li> <p>User federation: Administrators can configure user federation with external identity providers (IdPs) within realms, enabling the integration of LDAP, Active Directory, social media logins, and more.</p> </li> <li> <p>Realm export and import: Keycloak allows administrators to export realm configurations and import them into other Keycloak instances, simplifying deployment and migration tasks.</p> </li> </ul>"},{"location":"articles/keycloak/#references","title":"References","text":"<ul> <li>Keycloak Official Documentation</li> <li>GitHub repository</li> <li>Stack Overflow</li> </ul>"},{"location":"articles/mac-terminal-setup/","title":"Setting up Mac Terminal with Oh-My-Zsh","text":"<p>In this tutorial, I'll guide you through the process of customizing your Mac terminal using Oh-My-Zsh.  <code>Oh-My-Zsh</code> is a popular open-source framework for managing Zsh configurations, and it comes with a variety of themes and plugins to enhance your terminal experience.</p>"},{"location":"articles/mac-terminal-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the following prerequisites in place:</p> <ul> <li>A Mac computer running macOS.</li> <li>A terminal emulator. or iTerm2</li> </ul>"},{"location":"articles/mac-terminal-setup/#step-1-install-oh-my-zsh","title":"Step 1: Install Oh-My-Zsh","text":"<p>The first step is to install Oh-My-Zsh. Open your terminal and run the following command:</p> <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>This command will download and install Oh-My-Zsh on your system. Follow the on-screen prompts to complete the installation.</p>"},{"location":"articles/mac-terminal-setup/#step-2-customize-the-theme","title":"Step 2: Customize the Theme","text":"<p>Oh-My-Zsh comes with a variety of themes to choose from. To customize your terminal's appearance, open the Zsh configuration file in a text editor. You can use nano, vim, or any text editor you prefer. Here's an example using nano:</p> <pre><code>nano ~/.zshrc\n# or\nopen ~/.zshrc\n</code></pre> <p>Look for the <code>ZSH_THEME</code> variable in the configuration file and change it to your desired theme. You can find a list of available themes on the Oh-My-Zsh Themes GitHub page.</p> <p>for example:</p> <pre><code>ZSH_THEME=\"aussiegeek\"\n</code></pre> <p>Save your changes and exit the text editor. Your terminal will now display the selected theme.</p>"},{"location":"articles/mac-terminal-setup/#step-3-install-plugins","title":"Step 3: Install Plugins","text":"<p>Plugins enhance the functionality of your terminal. Oh-My-Zsh has a plugin system that allows you to easily add new features. Let's install two popular plugins: autocomplete and auto-highlighting.</p>"},{"location":"articles/mac-terminal-setup/#autocomplete-plugin","title":"Autocomplete Plugin","text":"<p>To install the autocomplete plugin, open your terminal and run:</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions\n</code></pre> <p>Next, add the plugin to your <code>~/.zshrc</code> file. Find the <code>plugins</code> line and add <code>'zsh-autosuggestions'</code> to the list of plugins. It should look like this:</p> <pre><code>plugins=(\n  # other plugins\n  zsh-autosuggestions\n)\n</code></pre> <p>Save your changes and exit.</p>"},{"location":"articles/mac-terminal-setup/#auto-highlighting-plugin","title":"Auto-Highlighting Plugin","text":"<p>To install the auto-highlighting plugin, open your terminal and run:</p> <pre><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\n</code></pre> <p>Now, add this plugin to your <code>~/.zshrc</code> file in the same way you added the autocomplete plugin:</p> <pre><code>plugins=(\n  # other plugins\n  zsh-syntax-highlighting\n)\n</code></pre> <p>Save and exit the file.</p>"},{"location":"articles/mac-terminal-setup/#step-4-enable-the-plugins","title":"Step 4: Enable the Plugins","text":"<p>To enable the newly installed plugins, open your <code>~/.zshrc</code> file once again and make sure the <code>plugins</code> section looks like the following:</p> <pre><code>plugins=(\n  # other plugins\n  git\n  zsh-autosuggestions\n  zsh-syntax-highlighting\n  zsh-completions\n)\n</code></pre> <p>Save your changes and exit the file.</p>"},{"location":"articles/mac-terminal-setup/#step-5-restart-your-terminal","title":"Step 5: Restart Your Terminal","text":"<p>To apply the changes, close and reopen your terminal, or run the following command:</p> <pre><code>source ~/.zshrc\n\n# or\nomz reload \n</code></pre> <p>Your Mac terminal is now set up with Oh-My-Zsh, and you've customized it with your chosen theme and plugins. Enjoy your enhanced terminal experience!</p> <p>Feel free to explore more themes and plugins to further enhance your productivity and style in the terminal.</p> <p>If everything is setup properly then your terminal output will look like this.</p> <p></p>"},{"location":"articles/mac-terminal-setup/#resources","title":"Resources","text":"<ul> <li>Oh-My-Zsh Official Website</li> <li>Oh-My-Zsh Themes</li> </ul>"},{"location":"articles/mkdocs-setup/","title":"Create a Website Using Material for MkDocs: A Step-by-Step Guide","text":""},{"location":"articles/mkdocs-setup/#introduction-to-material-for-mkdocs","title":"Introduction to Material for MkDocs,","text":"<p>Material for MkDocs, a static site generator written in Python, simplifies the process of creating project documentation. it helps for IT professionals, developers, and writers for creating a website. Static websites are fast, secure, and easy to maintain, making them an ideal choice for documentation, blogs, and personal websites. </p> <p>In this article, I will guide you through the steps by step instructions on how to create a static website or documentation site using Material for MkDocs, an extension of the MKDocs project that brings a modern look and additional features to your site.</p>"},{"location":"articles/mkdocs-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of Python and Markdown.</li> <li>Access to a command-line interface.</li> </ul>"},{"location":"articles/mkdocs-setup/#step-1-installation","title":"Step 1: Installation","text":"<p>Install Homebrew (macOS users):</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Install Python 3:</p> <p>MkDocs requires a recent version of <code>Python</code> and the Python package manager, <code>pip</code>, to be installed on your system.</p> <pre><code>brew install python3\n\n# verify the installation\npython3 --version\n</code></pre> <p>(or use the respective command for your operating system).</p> <p>Upgrade PIP:</p> <pre><code>pip3 install --upgrade pip\n</code></pre> <p>This ensures you have the latest version of PIP.</p> <p>Install MKDocs and Related Packages:</p> <pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-material-extensions\n</code></pre> <p>These commands install MKDocs, the Material theme, and additional extensions.</p>"},{"location":"articles/mkdocs-setup/#step-2-creating-your-project","title":"Step 2: Creating Your Project","text":"<p>Initialize Your MKDocs Project:</p> <pre><code>mkdocs new my-project\n</code></pre> <p>output <pre><code>INFO    -  Creating project directory: my-project\nINFO    -  Writing config file: my-project/mkdocs.yml\nINFO    -  Writing initial docs: my-project/docs/index.md\n</code></pre></p> <p>Replace 'my-project' with your project name. This command creates a new directory with essential configuration files.</p> <p>Navigate to Your Project Directory:</p> <pre><code>cd my-project\n</code></pre> <p>Open the project in VS code</p> <pre><code>code .\n</code></pre> <p></p>"},{"location":"articles/mkdocs-setup/#step-3-run-the-project-locally","title":"Step 3: Run the Project Locally","text":"<p>MkDocs comes with a <code>built-in dev-server</code> that lets you preview your documentation as you work on it. Make sure you're in the same directory as the <code>mkdocs.yml</code> configuration file, and then start the server by running the <code>mkdocs serve</code> command:</p> <p><pre><code>mkdocs serve\n</code></pre> </p> <p>This command starts a local server. Access your site at http://127.0.0.1:8000 and see changes in real-time as you edit your documents.</p> <p></p> <p>The server automatically rebuilds your site when you save changes, allowing you to see updates in real-time.</p>"},{"location":"articles/mkdocs-setup/#step-4-add-content-configure-the-website","title":"Step 4: Add Content &amp; Configure the Website","text":"<p>MKDocs organizes content using Markdown files in the <code>docs</code> directory. The <code>index.md</code> file is the homepage of your website.</p> <p>Create Markdown Files:</p> <p>Write your content in Markdown format. You can create multiple files and organize them into directories as needed.</p> <p>Customization and Extensions</p> <p>Material for MkDocs supports extensive customization. Edit your <code>mkdocs.yml</code> to include features like a search bar, social media links, or Google Analytics. MkDocs also supports plugins, which can add functionalities like search engine optimization, PDF export, and more.</p> <p>Configure the Website</p> <p>Editing <code>mkdocs.yml</code> customizes your website\u2019s structure. Define your site name, theme (Material), and navigation structure. </p> <p>Here\u2019s a basic example:</p> <pre><code>site_name: My Awesome Project\ntheme: \n  name: material\nnav:\n  - Home: index.md\n  - About: about.md\n</code></pre> <p>Explore the Material for MkDocs documentation for customization options and add them to your <code>mkdocs.yml</code>.</p>"},{"location":"articles/mkdocs-setup/#step-5-building-and-deploying-your-site","title":"Step 5: Building and Deploying Your Site","text":"<p>Build Your Site:</p> <p>When you're ready to publish, build a static site with:</p> <pre><code>mkdocs build\n</code></pre> <p>This command compiles your Markdown files into a static HTML website in the <code>site</code> directory.</p> <p>This creates a <code>site</code> directory with your website\u2019s static HTML files. For deployment, you can upload these files to a web server or use services like GitHub Pages for hosting. MkDocs provides a simple command for deploying to GitHub Pages:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Deploy Your Site:</p> <p>Choose a hosting solution (e.g., GitHub Pages, GitLab Pages or any other hosting solution) and follow their instructions to deploy your MKDocs site.</p>"},{"location":"articles/mkdocs-setup/#step-6-customization-home-page","title":"Step 6: Customization Home Page","text":"<p>Your home page is the first thing visitors see. Make it informative and engaging. Edit the <code>index.md</code> file in the <code>docs</code> folder to add content. </p> <p>This is the file you are looking for: <code>overrides/home.html</code>. You'll want to copy it over to your own overrides directory. Make sure you've set your custom_dir in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  custom_dir: docs/overrides\n...\n</code></pre> <p>In the front matter of your <code>index.md</code>, you need to specify the template to use:</p> <pre><code>---\ntitle: Title\ntemplate: home.html\n---\n</code></pre> <p>Note</p> <p>One important thing that took me a while to realize: you need a newline at the end of your md file. If you don't have one, the content will not display. I guess it's processed as having null content if you don't include the newline.</p> <p></p>"},{"location":"articles/mkdocs-setup/#conclusion","title":"Conclusion","text":"<p>Material for MkDocs is an excellent tool for IT professionals and tech writers to create and maintain documentation websites. With its easy-to-use features and extensive customization options, it's an ideal choice for documenting software project documentation, writing online e-Book, and more.</p>"},{"location":"articles/mkdocs-setup/#references","title":"References","text":"<ul> <li>Material for MkDocs</li> <li>MkDocs </li> <li>Getting Started with MkDocs</li> </ul>"},{"location":"articles/namespace-stuck/","title":"A Kubernetes Namespace Stuck in the Terminating State","text":""},{"location":"articles/namespace-stuck/#symptom","title":"Symptom","text":"<p>If you are experiencing issues with deleting namespaces in Kubernetes (AKS). When we execute the <code>kubectl delete ns</code> command, it becomes unresponsive and gets stuck in the terminating state. Even if we try to abort the operation, it remains stuck in this state indefinitely.</p> <p>Symptom is - A Kubernetes namespace is stuck in the Terminating state.</p>"},{"location":"articles/namespace-stuck/#root-cause","title":"Root Cause","text":"<p><code>Finalizer Issue</code></p> <p>Finalizers are mechanisms that allow resources to perform cleanup actions before they're deleted. If a resource has a finalizer that fails to complete its task, it can block the deletion of the entire namespace.</p>"},{"location":"articles/namespace-stuck/#resolving-the-problem","title":"Resolving the Problem","text":"<p>To resolve this issue, you'll need to identify and manually delete  a Terminating Namespace from finalizers.</p> <ol> <li> <p>View the namespaces that are stuck in the Terminating state:</p> <pre><code>kubectl get namespaces\n</code></pre> </li> <li> <p>Select a terminating namespace and view the contents of the namespace to find out the finalizer:</p> <pre><code>kubectl get namespace &lt;terminating-namespace&gt; -o yaml\n</code></pre> <p>Your YAML contents might resemble the following output:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: 2018-11-19T18:48:30Z\n  deletionTimestamp: 2018-11-19T18:59:36Z\n  name: &lt;terminating-namespace&gt;\n  resourceVersion: \"1385077\"\n  selfLink: /api/v1/namespaces/&lt;terminating-namespace&gt;\n  uid: b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Terminating\n</code></pre> </li> <li> <p>Create a temporary JSON file:</p> <pre><code>kubectl get namespace &lt;terminating-namespace&gt; -o json &gt;tmp.json\n</code></pre> </li> <li> <p>Edit your <code>tmp.json</code> file. Remove the <code>kubernetes</code> value from the <code>finalizers</code> field and save the file.</p> <p>Your <code>tmp.json</code> file might resemble the following output:</p> <pre><code>{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Namespace\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2018-11-19T18:48:30Z\",\n        \"deletionTimestamp\": \"2018-11-19T18:59:36Z\",\n        \"name\": \"&lt;terminating-namespace&gt;\",\n        \"resourceVersion\": \"1385077\",\n        \"selfLink\": \"/api/v1/namespaces/&lt;terminating-namespace&gt;\",\n        \"uid\": \"b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\"\n    },\n    \"spec\": {\n        \"finalizers\": \n    },\n    \"status\": {\n        \"phase\": \"Terminating\"\n    }\n}\n</code></pre> </li> <li> <p>To set a temporary proxy IP and port, run the following command. Be sure to keep your terminal window open until you delete the stuck namespace:</p> <pre><code>kubectl proxy\n</code></pre> <p>Your proxy IP and port might resemble the following output:</p> <pre><code>Starting to serve on 127.0.0.1:8001\n</code></pre> </li> <li> <p>From a new terminal window, make an API call with your temporary proxy IP and port:</p> <pre><code>curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/&lt;terminating-namespace&gt;/finalize\n</code></pre> <p>Your output might resemble the following content:</p> <pre><code>{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"&lt;terminating-namespace&gt;\",\n    \"selfLink\": \"/api/v1/namespaces/&lt;terminating-namespace&gt;/finalize\",\n    \"uid\": \"b50c9ea4-ec2b-11e8-a0be-fa163eeb47a5\",\n    \"resourceVersion\": \"1602981\",\n    \"creationTimestamp\": \"2018-11-19T18:48:30Z\",\n    \"deletionTimestamp\": \"2018-11-19T18:59:36Z\"\n  },\n  \"spec\": {\n\n  },\n  \"status\": {\n    \"phase\": \"Terminating\"\n  }\n}\n</code></pre> <p>Note: The finalizer parameter is removed.</p> </li> <li> <p>Verify that the terminating namespace is removed:</p> <pre><code>kubectl get namespaces\n</code></pre> </li> </ol>"},{"location":"articles/namespace-stuck/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to successfully resolve the issue of Kubernetes namespace stuck in the terminating state and delete namespace.</p>"},{"location":"articles/oatuh2.0-flows/","title":"Single Sign-On - OAuth 2.0 flows","text":""},{"location":"articles/oatuh2.0-flows/#introduction","title":"Introduction","text":"<p>OAuth 2.0 defines several authorization flows, also known as <code>grant types</code>, to enable different use cases for securing access to resources. The choice of which OAuth 2.0 flow to use depends on the specific requirements and characteristics of the client application and the desired level of security. Here are the main OAuth 2.0 flows:</p> <p>In this article, I will explain the four distinct OAuth 2.0 grant types, providing a detailed exploration complete with sequence diagrams and interactive examples.  we'll also look into the interactions between the various entities involved and understanding of each grant type in details. </p>"},{"location":"articles/oatuh2.0-flows/#authorization-code-flow-authorization-code-grant","title":"Authorization Code Flow (Authorization Code Grant)","text":"<ul> <li>Use Case: Suitable for web applications running on a server.</li> <li>Flow:<ol> <li>The client redirects the user to the authorization server's authorization endpoint.</li> <li>The user authenticates and approves the requested permissions.</li> <li>The authorization server issues an authorization code and redirects the user back to the client with the code.</li> <li>The client exchanges the authorization code for an access token and, optionally, a refresh token by making a direct request to the authorization server's token endpoint.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Authorization Code Flow:</p> <p>This sequence diagram illustrates the interactions between the client, user, authorization server, and resource server in the Authorization Code Flow. The numbers represent the sequential order of the steps.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant User\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Initiate Authorization Request\n  AuthorizationServer--&gt;&gt;User: 2. Present Login Screen\n  User-&gt;&gt;AuthorizationServer: 3. Enter Credentials\n  AuthorizationServer--&gt;&gt;User: 4. Consent Screen\n  User-&gt;&gt;AuthorizationServer: 5. Grant Permissions\n  AuthorizationServer--&gt;&gt;Client: 6. Redirect with Authorization Code\n  Client-&gt;&gt;AuthorizationServer: 7. Exchange Authorization Code for Tokens\n  AuthorizationServer--&gt;&gt;Client: 8. Access Token, Refresh Token\n  Client-&gt;&gt;ResourceServer: 9. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 10. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Initiate Authorization Request (Client to Authorization Server): The client initiates the authorization process by redirecting the user to the authorization server's authorization endpoint.</p> </li> <li> <p>Present Login Screen (Authorization Server to User): The authorization server presents a login screen to the user to enter their credentials.</p> </li> <li> <p>Enter Credentials (User to Authorization Server): The user enters their credentials.</p> </li> <li> <p>Consent Screen (Authorization Server to User): If necessary, the authorization server presents a consent screen to the user to grant permissions to the client.</p> </li> <li> <p>Grant Permissions (User to Authorization Server): The user grants the requested permissions.</p> </li> <li> <p>Redirect with Authorization Code (Authorization Server to Client): The authorization server redirects the user back to the client with an authorization code.</p> </li> <li> <p>Exchange Authorization Code for Tokens (Client to Authorization Server): The client exchanges the received authorization code for an access token and, optionally, a refresh token.</p> </li> <li> <p>Access Token, Refresh Token (Authorization Server to Client): The authorization server responds with the access token and refresh token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#implicit-flow-implicit-grant","title":"Implicit Flow (Implicit Grant)","text":"<ul> <li>Use Case: Designed for user-agent-based clients (e.g., single-page applications) that cannot keep a client secret confidential.</li> <li>Flow:<ol> <li>The client redirects the user to the authorization server's authorization endpoint.</li> <li>The user authenticates and approves the requested permissions.</li> <li>The authorization server issues an access token directly in the redirect URI fragment.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Implicit Flow:</p> <p>This sequence diagram illustrates the interactions between the client, user, authorization server, and resource server in the Implicit Flow. The numbers represent the sequential order of the steps.</p> <p>Note</p> <p>Note that in the Implicit Flow, the access token is returned directly to the client in the URL fragment.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant User\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Initiate Authorization Request\n  AuthorizationServer--&gt;&gt;User: 2. Present Login Screen\n  User-&gt;&gt;AuthorizationServer: 3. Enter Credentials\n  AuthorizationServer--&gt;&gt;User: 4. Consent Screen\n  User-&gt;&gt;AuthorizationServer: 5. Grant Permissions\n  AuthorizationServer--&gt;&gt;Client: 6. Redirect with Access Token (in URL fragment)\n  Client-&gt;&gt;ResourceServer: 7. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 8. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Initiate Authorization Request (Client to Authorization Server): The client initiates the authorization process by redirecting the user to the authorization server's authorization endpoint.</p> </li> <li> <p>Present Login Screen (Authorization Server to User): The authorization server presents a login screen to the user to enter their credentials.</p> </li> <li> <p>Enter Credentials (User to Authorization Server): The user enters their credentials.</p> </li> <li> <p>Consent Screen (Authorization Server to User): If necessary, the authorization server presents a consent screen to the user to grant permissions to the client.</p> </li> <li> <p>Grant Permissions (User to Authorization Server): The user grants the requested permissions.</p> </li> <li> <p>Redirect with Access Token (in URL fragment) (Authorization Server to Client): The authorization server redirects the user back to the client with an access token directly included in the URL fragment.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol> <p>Here is the simplified example of sending an OpenID Connect authentication request to an Active Directory Federation Services (AD FS) endpoint and obtaining both the <code>id_token</code> and <code>access token</code>.</p> <p>OpenID Connect Authentication Request:</p> <p>Assuming your AD FS server is hosted at <code>https://adfs.example.com</code> and your client application is registered with the client ID <code>your-client-id</code>.</p> <p>The OpenID Connect authentication request might look like this:</p> <pre><code>GET https://adfs.example.com/adfs/oauth2/authorize\n    ?response_type=id_token token\n    &amp;client_id=your-client-id\n    &amp;redirect_uri=https://your-app-callback-url.com\n    &amp;scope=openid\n    &amp;nonce=your-nonce-value\n    &amp;response_mode=fragment\n    &amp;state=your-state-value\n</code></pre> <p>Explanation of parameters:</p> <ul> <li><code>response_type=id_token token</code>: Request both an ID token and an access token.</li> <li><code>client_id</code>: The client identifier registered with AD FS.</li> <li><code>redirect_uri</code>: The callback URL where AD FS will redirect the user after authentication.</li> <li><code>scope=openid</code>: Request OpenID Connect authentication.</li> <li><code>nonce</code>: A unique value to mitigate replay attacks.</li> <li><code>response_mode=form_post</code>: The response is sent as a form post.</li> <li><code>state</code>: A value used to maintain state between the request and the callback.</li> </ul> <p>response_mode=<code>fragment</code> or <code>form_post</code></p> <ul> <li><code>response_mode=fragment</code>: Specify that the response should be included in the URL fragment.</li> </ul> <p>OpenID Connect Authentication Response:</p> <p>Assuming a successful authentication, AD FS will redirect the user back to your specified <code>redirect_uri</code> with the tokens. The response might look like this:</p> <pre><code>POST https://your-app-callback-url.com\nContent-Type: application/x-www-form-urlencoded\n\nid_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjIwMTYtMDYtMjNUMTY6MDY6MDMuMjA1In0.eyJzdWIiOiIxMDAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEyMzQ1Njc4OTAxMjMiLCJhdWQiOiJodHRwczovL2V4YW1wbGUuY29tIiwibm9uY2UiOiJ5b3VyLW5vbmNlLXZhbHVlIiwiaXNzIjoiaHR0cHM6Ly9hZGZzLmV4YW1wbGUuY29tIiwiaWF0IjoxNjA5OTY1NTk3LCJleHAiOjE2MDk5NjU5OTcsImF0X2hhc2giOiJiZHNEdHBJZzRGSWtIOWw5elVZOWlnIiwic2lkIjoiYmNkS2JzWlVzQzZTUGwyazd6cTZsQT09In0.yu0uIwXVCMvZaLOdNbPiPbdgI90r-IA0Iy-l6QhH1ZyDrxP9dQAn6qGmBHXJrO15sZb3asHsqj6f3_7pVl7DFDDZXzHKFEHLJfR0deZ0OHoNlgUklrxr7tmqqTw07EYsOa_9CIsZD9id0PCTDAm0ZIyakO9BCL44O0UyvjNlHtMvXV8W4N24vQGEGjw0Cx2Nm7c__HZxS_5H0rUJL2FXFjjXgDrNhEFGjGziGjbOXwxzWc_W2AM-g-buQsN8wHw5kv8vh7mjPXYkAlKJWAKHHek5XlQDljJbWz7R1w5CfQ5MQ7CzrqX62NfeXeWZsGMKdfnAVQLkkMaOqA\n\n&amp;access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjIwMTYtMDYtMjNUMTY6MDY6MDMuMjA1In0.eyJleHAiOjE2MDk5NjYxOTcsImF1ZCI6Imh0dHBzOi8vZXhhbXBsZS5jb20iLCJpc3MiOiJodHRwczovL2FkZnMuZXhhbXBsZS5jb20iLCJzdWIiOiIxMDAxMjM0NTY3ODkwMTIzNDU2Nzg5MDEyMzQ1Njc4OTAxMjMiLCJhdWQiOiJodHRwczovL2V4YW1wbGUuY29tIiwianRpIjoiNjQyMjZkM2EtN2YyNi00ZjMxLWEzMDktZDM0Y2MxNzJhNTlkIiwiaWF0IjoxNjA5OTY1NTk3fQ.DGtIlQkyqmq6ZR0bF61F5qGvBX3XXayRvdx5tQQziP-t4_1f0Y6Vl5bkYuf6hI3PYINxlWs5cM1bhecvqSo-gegqHvz7EJzj7-03YSo-01KuTLm9VyISgo6XstGLyDOFhBr7v1agx2pM7OAz8AER8sI1AmJJ1-fE0P1o7j-jd-v1cD7I8KckACin64S8b8arIjiLiluZbj3TNg1YJf7Xa5nb5oQ96VCzw7BYBwtP9bu0l5YyQ4ILVsj_yw8OxFf0KXdLpF97QvaR-Iu3IwYXZ2sSZFzRUbxjoCJqkQzvldIBK7pFlHkTTrjZLgGhZS5WuR5kZm57pSbpmrOqCtHA\n</code></pre> <p>Explanation of parameters:</p> <ul> <li><code>id_token</code>: The ID token containing user information.</li> <li><code>access_token</code>: The access token used to access protected resources.</li> </ul> <p>Note</p> <p>The actual endpoints and parameters may vary based on your AD FS configuration and the OpenID Connect implementation.</p>"},{"location":"articles/oatuh2.0-flows/#resource-owner-password-credentials-flow","title":"Resource Owner Password Credentials Flow","text":"<ul> <li>Use Case: Suitable for trusted clients that can directly request and obtain the user's credentials.</li> <li>Flow:<ol> <li>The client directly requests the user's credentials (e.g., username and password).</li> <li>The client uses the user's credentials to request an access token directly from the authorization server.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Resource Owner Password Credentials Flow:</p> <p>This sequence diagram illustrates the interactions between the client, authorization server, and resource server in the Resource Owner Password Credentials Flow. </p> <p>Note</p> <p>It's important to note that this flow involves the client collecting and transmitting the user's credentials, so it should only be used by highly trusted clients, and it's generally not recommended for public or untrusted clients.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Request Access Token\n  AuthorizationServer--&gt;&gt;Client: 2. Request User Credentials\n  Client-&gt;&gt;AuthorizationServer: 3. User Credentials\n  AuthorizationServer--&gt;&gt;Client: 4. Access Token, Refresh Token\n  Client-&gt;&gt;ResourceServer: 5. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 6. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Request Access Token (Client to Authorization Server): The client requests an access token directly from the authorization server by providing its client credentials and the resource owner's (user's) credentials.</p> </li> <li> <p>Request User Credentials (Authorization Server to Client): The authorization server requests the user credentials (username and password) from the client.</p> </li> <li> <p>User Credentials (Client to Authorization Server): The client provides the user credentials (username and password) to the authorization server.</p> </li> <li> <p>Access Token, Refresh Token (Authorization Server to Client): Upon successful authentication, the authorization server responds with an access token and, optionally, a refresh token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#client-credentials-flow","title":"Client Credentials Flow","text":"<ul> <li>Use Case: Used when the client is the resource owner and wants to access its own resources.</li> <li>Flow:<ol> <li>The client directly requests an access token from the authorization server using its own credentials.</li> </ol> </li> </ul> <p>Here's a sequence diagram for the Client Credentials Flow:</p> <p>This sequence diagram illustrates the interactions between the client, authorization server, and resource server in the Client Credentials Flow. The Client Credentials Flow is used when the client is the resource owner and wants to access its own resources. It does not involve user authentication, making it suitable for machine-to-machine communication where the client is acting on its own behalf.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant AuthorizationServer\n  participant ResourceServer\n\n  Client-&gt;&gt;AuthorizationServer: 1. Request Access Token\n  AuthorizationServer--&gt;&gt;Client: 2. Respond with Access Token\n  Client-&gt;&gt;ResourceServer: 3. Access Protected Resource\n  ResourceServer--&gt;&gt;Client: 4. Return Protected Resource</code></pre> <p>Explanation of the steps:</p> <ol> <li> <p>Request Access Token (Client to Authorization Server): The client directly requests an access token from the authorization server by providing its client credentials (client ID and client secret).</p> </li> <li> <p>Respond with Access Token (Authorization Server to Client): The authorization server responds with an access token.</p> </li> <li> <p>Access Protected Resource (Client to Resource Server): The client uses the obtained access token to request access to a protected resource.</p> </li> <li> <p>Return Protected Resource (Resource Server to Client): The resource server returns the requested protected resource.</p> </li> </ol>"},{"location":"articles/oatuh2.0-flows/#conclusion","title":"Conclusion","text":"<p>OAuth 2.0 plays a crucial role in enabling Single Sign-On and secure authorization across various applications and services. The choice of the appropriate flow depends on the specific requirements of the client application, the level of security needed, and the characteristics of the user interaction. Continuous attention to security best practices is essential for maintaining a robust and trustworthy authentication and authorization system.</p>"},{"location":"articles/oatuh2.0-flows/#references","title":"References","text":"<ul> <li>AD FS OpenID Connect/OAuth flows and Application Scenarios</li> <li>OAuth 2.0 Specification</li> <li>OpenID Connect Specifications</li> <li>Security Assertion Markup Language (SAML)</li> <li>Mermaidv10.6.1 Live Editor</li> <li></li> </ul>"},{"location":"articles/pv-pvc-stuck/","title":"Delete PV(Persistent Volume) and PVC(Persistent Volume Claim) stuck in terminating state","text":"<p>If you are experiencing issues with deleting <code>Persistent Volume</code> or <code>Persistent Volume Claim</code> in Kubernetes (AKS). When we execute the <code>kubectl delete pv</code> or <code>kubectl delete pvc</code> command, it becomes unresponsive and gets stuck in the terminating state. Even if we try to abort the operation, it remains stuck in this state indefinitely.</p> <p>In this article, we will explore the symptoms, root cause, and the proper resolution to overcome this challenge.</p>"},{"location":"articles/pv-pvc-stuck/#symptom","title":"Symptom","text":"<p>Symptom is - A Kubernetes Persistent Volume or Persistent Volume Claim is stuck in the Terminating state because of not following the order during the deletion.</p> <p>The deletion of PV(Persistent Volume) and PVC(Persistent Volume claim) needs to follow in specific order else you will get stuck in terminatiion state. When you are planning to delete the Persistent Volume as well as Persistent Volume Claim then you must follow an order -</p> <ul> <li>First delete - Persistent Volume Claim</li> <li>Second delete- Persistent Volume</li> </ul> <p>Note</p> <p>You should never delete PV(Persistent Volume) without deleting its PVC(Persistent Volume Claim))</p>"},{"location":"articles/pv-pvc-stuck/#root-cause","title":"Root Cause","text":"<p><code>Finalizer Issue</code></p> <p>Each Kubernetes resource running in the cluster has Finalizers associated with it. Finalizers prevent accidental deletion of resources(Persistent Volume, Persistent Volume Claim), If you accidentally issue kubectl delete command on Kubernetes resource and if there is a finalizer associated with that resource then it is going to put the resource in <code>Read-Only</code> mode and prevent it from deletion.</p>"},{"location":"articles/pv-pvc-stuck/#resolving-the-problem","title":"Resolving the Problem","text":"<p>If you find your Kubernetes resources, such as Persistent Volumes (PVs) or Persistent Volume Claims (PVCs), stuck in the terminating state, resolving the issue involves removing the associated <code>Finalizer</code>. Follow these steps to successfully resolve the problem:</p>"},{"location":"articles/pv-pvc-stuck/#step-1-retrieve-information-about-resources","title":"Step-1: Retrieve information about resources","text":"<p>Use the following commands to retrieve information about your PVs and PVCs:</p> <pre><code># Get Persistent Volume Claims in a specific namespace\nkubectl get pvc -n namespace1\n\n# Get Persistent Volumes in a specific namespace\nkubectl get pv -n namespace1\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#step-2-remove-the-finalizer","title":"Step-2: Remove the Finalizer","text":"<p>Execute the following commands to remove the Finalizer from both the Persistent Volume and Persistent Volume Claim:</p> <pre><code># Patch the Persistent Volume to remove the Finalizer\nkubectl patch pv sample-app-pv -p '{\"metadata\":{\"finalizers\":null}}' -n namespace1\n\n# Patch the Persistent Volume Claim to remove the Finalizer\nkubectl patch pvc sample-app-pvc -p '{\"metadata\":{\"finalizers\":null}}' -n namespace1\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#step-3-delete-resources","title":"Step-3: Delete Resources","text":"<p>Now that the Finalizer has been removed, proceed to delete the resources in the correct order:</p> <p>Delete Persistent Volume Claim</p> <pre><code># Delete Persistent Volume Claim\nkubectl delete pvc sample-app-pvc -n namespace1\n</code></pre> <p>Delete Persistent Volume</p> <p>After the successful deletion of the Persistent Volume Claim, proceed to delete the Persistent Volume:</p> <pre><code># Delete Persistent Volume\nkubectl delete pv sample-app-pv -n namespace1\n</code></pre> <p>or</p> <p>Force Delete (if needed)</p> <p>If the regular deletion commands do not work, you can use the force delete option:</p> <pre><code># Force delete Persistent Volume Claim\nkubectl delete pvc --grace-period=0 --force --namespace namespace1 sample-app-pvc\n\n# Force delete Persistent Volume\nkubectl delete pv --grace-period=0 --force --namespace namespace1 sample-app-pv\n</code></pre>"},{"location":"articles/pv-pvc-stuck/#conclusion","title":"Conclusion","text":"<p>By following these steps, you should be able to successfully resolve the issue of Kubernetes resources stuck in the terminating state and delete both Persistent Volumes and Persistent Volume Claims.</p>"},{"location":"articles/single-sign-on/","title":"Single Sign-On - Introduction","text":""},{"location":"articles/single-sign-on/#introduction","title":"Introduction","text":"<p>If you're involved in implementing Single Sign-On (SSO) in your project, it's crucial to understand the fundamental concepts that will help you to implement the authentication mechanism. </p> <p>In this article, we'll explain the foundational elements of Single Sign-On. By looking into these core concepts, you'll gain a clear understanding that will help to implement Single Sign-On in your own application.</p> <p>Whether you're new to SSO or seeking a refresher, this exploration will help you with the essential knowledge needed to review the complexities of Single Sign-On and make informed decisions during the implementation process. </p>"},{"location":"articles/single-sign-on/#what-is-single-sign-on","title":"What is Single Sign-On?","text":"<p><code>Single Sign-On (SSO)</code> is a user authentication process that enables a user to access multiple applications with a single set of login credentials. Instead of requiring users to remember separate usernames and passwords for each application, SSO streamlines the login process by authenticating the user once and granting access to multiple services.</p>"},{"location":"articles/single-sign-on/#waht-are-the-benefits-of-sso","title":"Waht are the benefits of SSO?","text":"<p>Single Sign-On (SSO) offers several benefits for both users and organizations. Here are some key advantages of implementing SSO:</p> <ul> <li> <p>Enhanced User Experience: Users only need to remember and enter one set of credentials to access multiple applications and services, leading to a smoother and more efficient user experience.</p> </li> <li> <p>Time and Productivity Savings: SSO reduces the need for users to repeatedly log in, saving time and minimizing disruptions to workflow. This can result in increased productivity across an organization.</p> </li> <li> <p>Minimized Password Reset Requests: With fewer passwords to manage, organizations typically experience a decrease in password-related support requests and help desk calls, leading to cost savings.</p> </li> <li> <p>Centralized Access Management: SSO allows for centralized control and management of user access. Changes in user permissions or account status can be applied uniformly across all connected applications.</p> </li> <li> <p>Enhanced Security: SSO can enhance security by enforcing consistent authentication policies. Users are less likely to resort to insecure practices, such as writing down passwords, when dealing with multiple credentials.</p> </li> </ul>"},{"location":"articles/single-sign-on/#how-single-sign-on-works","title":"How Single Sign-On works?","text":"<p>Single Sign-On (SSO) works by allowing users to authenticate once and gain access to multiple applications or services without the need to re-enter credentials for each one. The fundamental idea is to streamline the authentication process and provide a seamless user experience across various systems. Here's a high-level overview of how SSO works:</p> <ol> <li> <p>User Attempts to Access a Service: </p> <ul> <li>When a user attempts to access an application or service that is part of the SSO system, they are redirected to the SSO system for authentication.</li> </ul> </li> <li> <p>SSO Authentication Request:</p> <ul> <li>The SSO system initiates an authentication request, prompting the user to provide their credentials (username and password) or use alternative authentication methods like multi-factor authentication (MFA).</li> </ul> </li> <li> <p>User Authentication:</p> <ul> <li>The user enters their credentials or completes the required authentication steps. The SSO system verifies the user's identity.</li> </ul> </li> <li> <p>Issuance of Authentication Token:</p> <ul> <li>Upon successful authentication, the SSO system issues an authentication token. This token serves as proof of the user's identity and is often in the form of a secure token like a JSON Web Token (JWT).</li> </ul> </li> <li> <p>Token Exchange (Optional):</p> <ul> <li>In some cases, depending on the SSO protocol used (such as OAuth 2.0 or SAML), the authentication token may be exchanged for an access token or a service-specific token.</li> </ul> </li> <li> <p>SSO Session Management:</p> <ul> <li>The SSO system manages the user's session, keeping track of the user's authenticated state. This session information is used to facilitate access to other services without additional authentication.</li> </ul> </li> <li> <p>User Accesses Another Service:</p> <ul> <li>If the user decides to access another application or service within the SSO environment, the SSO system recognizes the user's existing session and provides access without requesting credentials again.</li> </ul> </li> <li> <p>Logout Handling (Optional):</p> <ul> <li>When the user logs out, the SSO system can handle the logout process by terminating the user's session across all connected services. This ensures a complete logout experience.</li> </ul> </li> <li> <p>Token Expiry and Refresh (Optional):</p> <ul> <li>Authentication tokens may have a limited validity period. If needed, the SSO system can handle token expiration by either requiring re-authentication or using mechanisms like token refresh to obtain a new token without requiring the user's credentials.</li> </ul> </li> </ol>"},{"location":"articles/single-sign-on/#what-are-the-different-types-of-sso","title":"What are the different types of SSO?","text":"<p>Single Sign-On (SSO) comes in various types, depends on different use cases. Here are some common types of SSO:</p> <ul> <li> <p>Enterprise SSO: Primarily used within an organization, Enterprise SSO allows users to access various internal systems and applications using a single set of credentials. It enhances security and simplifies user management for IT administrators.</p> </li> <li> <p>Web SSO: Web SSO extends the SSO concept to web applications. Users can log in once to access multiple web services, making it prevalent in online platforms, social media, and cloud-based applications.</p> </li> <li> <p>Federated SSO: Federated SSO enables users to access resources across multiple organizations or domains. It relies on trust relationships between identity providers, allowing for seamless authentication in a distributed environment.</p> </li> </ul>"},{"location":"articles/single-sign-on/#popular-sso-protocols","title":"Popular SSO Protocols","text":"<p>Several popular Single Sign-On (SSO) protocols are widely used to implement SSO across various applications and services. Here are some of the most common SSO protocols:</p> <ul> <li> <p>OAuth 2.0: OAuth 2.0 is a widely adopted authorization framework that allows a user to grant a third-party application limited access to their resources without exposing their credentials. While OAuth 2.0 itself is not an authentication protocol, it is often used in conjunction with OpenID Connect to achieve both authentication and authorization in an SSO scenario.</p> </li> <li> <p>OpenID Connect: OpenID Connect is an identity layer built on top of OAuth 2.0. It extends OAuth 2.0 to provide a standardized way for clients to request and receive identity information about users. OpenID Connect is specifically designed for authentication and is commonly used for SSO.</p> </li> <li> <p>SAML (Security Assertion Markup Language): SAML is an XML-based standard for exchanging authentication and authorization data between parties, particularly in a web browser environment. It allows for the secure transfer of user identity information between an identity provider (IdP) and a service provider (SP), facilitating SSO.</p> </li> </ul>"},{"location":"articles/single-sign-on/#roles-of-single-sign-on","title":"Roles of Single Sign-On","text":"<p>The Single Sign-On (SSO) process involves several roles or components working together in a coordinated fashion to provide a secure, seamless, and efficient Single Sign-On experience for users across multiple applications and services. The main roles or components include:</p> <p>User (Resource Owner):</p> <ul> <li>Role: The end-user or resource owner is the individual seeking access to various applications and services without the need for multiple logins.</li> <li>Interaction: Initiates the authentication process by attempting to access a resource or application.</li> </ul> <p>Identity Provider (IdP):</p> <ul> <li>Role: The Identity Provider is responsible for authenticating the user and asserting their identity to other applications or services.</li> <li>Interaction: Verifies user credentials and issues authentication tokens (such as SAML assertions or JWTs) to signify a successful authentication.</li> </ul> <p>Service Provider (SP) / Resource Server::</p> <ul> <li>Role: The Service Provider is the application or service that the user wants to access. It relies on the Identity Provider's assertion to grant access to the user.</li> <li>Interaction: Receives the authentication token from the user and validates it with the Identity Provider to authorize access.</li> </ul> <p>Authorization Server:</p> <ul> <li>Role: In OAuth-based SSO systems, the Authorization Server is responsible for granting access tokens to client applications.</li> <li>Interaction: Issues access tokens after authenticating the user and obtaining their consent. The client application uses these tokens to access protected resources.</li> </ul>"},{"location":"articles/single-sign-on/#tokens-used-in-sso","title":"Tokens used in SSO","text":"<p>Let's explore the two types of tokens, <code>identity tokens</code>, and <code>access tokens</code>, delivered by the Authorization Server, both of which are commonly presented in the form of JSON Web Tokens (JWTs):</p> <p>ID Tokens:</p> <ul> <li>Purpose: ID tokens are primarily used in the context of OpenID Connect. They are meant to carry information about the authentication of the user.</li> <li>Content: An ID token contains claims about the identity of the authenticated user, such as their user ID, username, and possibly other information like email or profile information.</li> <li>Usage: ID tokens are typically used by the client application to obtain information about the authenticated user. They are not used to access protected resources but rather to identify the user.</li> <li>Example Scenario: After a user logs in through an OpenID Connect provider, the provider issues an ID token, which the client application can use to get information about the authenticated user.</li> </ul> <p>Access Tokens:</p> <ul> <li>Purpose: Access tokens are used to access protected resources on behalf of a user.</li> <li>Content: An access token represents the authorization granted to a client application to access specific resources on behalf of the user. It may contain information about the scope of access, expiration time, and other details.</li> <li>Usage: Access tokens are presented by the client to the resource server to gain access to protected resources. They are used in API calls to demonstrate that the client has been authorized to access the requested resources.</li> <li>Example Scenario: A user logs in and grants permission to a third-party application to access their profile information on a social media platform. The application receives an access token that it can then use to make API requests to retrieve the user's profile data.</li> </ul> <p>Example Identity Token (JWT): <pre><code>{\n  \"iss\": \"https://openid-provider.com\",\n  \"sub\": \"1234567890\",\n  \"aud\": \"your-client-id\",\n  \"exp\": 1632969781,\n  \"iat\": 1632966181,\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"picture\": \"https://example.com/john-doe.jpg\",\n  \"nonce\": \"nonce-value\"\n}\n</code></pre></p> <p>Token data details:</p> <ul> <li><code>sub</code> (Subject): Identifies the subject of the token (e.g., user ID).</li> <li><code>iss</code> (Issuer): Specifies the issuer of the token (e.g., Authorization Server).</li> <li><code>aud</code> (Audience): Indicates the audience for which the token is intended (e.g., client application or resource server).</li> <li><code>exp</code> (Expiration Time): Specifies the expiration time of the token.</li> <li><code>iat</code> (Issued At): Indicates the time at which the token was issued.</li> </ul> <p>Access Tokens:</p> <ul> <li>Description: Access tokens are used by the client application to access protected resources on behalf of the user. They represent the authorization granted by the user.</li> <li>Contents: Access tokens often include information about the granted permissions, scope, and expiration time.</li> <li>Use Case: Access tokens are presented by the client application to the resource server when making requests for protected resources. They serve as a proof of authorization.</li> </ul> <p>Example Access Token (JWT): <pre><code>{\n  \"iss\": \"https://authorization-server.com\",\n  \"sub\": \"1234567890\",\n  \"aud\": [\"https://api.example.com\", \"https://resources.example.com\"],\n  \"exp\": 1632969781,\n  \"iat\": 1632966181,\n  \"scope\": \"read write\",\n  \"jti\": \"a1b2c3d4e5f6\"\n}\n</code></pre> simplified example of what an access token might look like:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"read write\",\n  \"refresh_token\": \"rT_5f9d2a96-7ae3-4eae-b64d-6375942b9c2a\"\n}\n</code></pre> <p>Refresh Tokens:</p> <p>In Single Sign-On (SSO) systems, refresh tokens play a crucial role in extending the validity of access tokens and maintaining a user's authenticated session without requiring repeated user authentication. </p> <p>Access tokens, which grant access to resources, typically have a limited lifespan for security reasons. Once an access token expires, the user would need to re-authenticate to obtain a new one.</p> <p>When a user initially authenticates and obtains an access token, the Identity Provider or Authorization Server may also issue a refresh token along with the access token.</p> <p>These tokens play a crucial role in the OAuth 2.0 and OpenID Connect protocols, providing a secure and standardized way to convey identity and authorization information in a distributed and interoperable manner.</p>"},{"location":"articles/single-sign-on/#oauth-20-flows","title":"OAuth 2.0 flows","text":"<p>OAuth 2.0 flows, also known as OAuth 2.0 grant types, define the mechanism through which applications obtain authorization and access tokens to interact with protected resources on behalf of a user. Each flow is designed for specific use cases and scenarios, providing a standardized way for clients (applications) to request and obtain access to resources.</p> <p>The main OAuth 2.0 flows include:</p> <p>Authorization Code Grant Flow:</p> <ul> <li>Use Case: Web applications with a server-side component.</li> <li>Description: Involves the redirection of the user to the authorization server, where the user authenticates and grants permission. The authorization server returns an authorization code to the client, which is then exchanged for an access token.</li> </ul> <p>Implicit Grant Flow:</p> <ul> <li>Use Case: Browser-based applications (JavaScript applications).</li> <li>Description: Designed for client-side applications running in the user's browser. The access token is issued directly to the client without the need for an intermediate authorization code exchange.</li> </ul> <p>Client Credentials Grant Flow:</p> <ul> <li>Use Case: Confidential clients, such as backend servers or applications that can securely store client credentials.</li> <li>Description: The client (usually a server) directly requests an access token from the authorization server using its client credentials (client ID and secret). This flow is suitable for machine-to-machine communication.</li> </ul> <p>Resource Owner Password Credentials Grant Flow:</p> <ul> <li>Use Case: Highly trusted applications, such as native mobile apps.</li> <li>Description: Involves the resource owner (user) providing their username and password directly to the client. The client then uses these credentials to obtain an access token from the authorization server.</li> </ul> <p>These flows provide flexibility and cater to different types of applications and security requirements. The choice of a specific flow depends on the characteristics of the client application, the level of trust, and the security considerations of the overall system architecture.</p>"},{"location":"articles/single-sign-on/#json-web-token-jwt","title":"JSON Web Token (JWT)","text":"<p>JWTs are often used for authentication and authorization purposes in web applications and APIs. They can be sent between parties, and since they are self-contained, the recipient can verify the information within the token without needing to contact the issuer. JWTs are widely used in various protocols and frameworks, including OAuth 2.0 and OpenID Connect.</p> <p>JWTs are defined by the RFC 7519 standard and consist of three parts:</p> <p>1. Header: The header typically consists of two parts: the type of the token (JWT) and the signing algorithm being used, such as HMAC SHA256 or RSA.</p> <p>Example:    <pre><code>{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n</code></pre></p> <p>2. Payload: The payload contains the claims. Claims are statements about an entity (typically, the user) and additional data.</p> <p>Example:    <pre><code>{\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"iat\": 1516239022\n}\n</code></pre></p> <p>3. Signature: To create the signature part, you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that.</p> <p>Example (using HMAC SHA256): <pre><code>HMACSHA256(\n    base64UrlEncode(header) + \".\" +\n    base64UrlEncode(payload),\n    secret)\n</code></pre></p> <p>The resulting JWT looks like this: <pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwMCIsIm5hbWUiOiJKb2huIERvZSIsImlhdCI6MTUxNjIzOTAyMn0.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n</code></pre></p>"},{"location":"articles/single-sign-on/#conclusion","title":"Conclusion","text":"<p>Single Sign-On plays an importent role in the realm of identity and access management, providing a user-centric and secure approach to accessing multiple applications seamlessly. With benefits ranging from enhanced user experience to improved security and streamlined administration, SSO continues to play a vital role in shaping the future of digital identity.</p>"},{"location":"articles/single-sign-on/#references","title":"References","text":"<ul> <li>OAuth 2.0 Specification</li> <li>OpenID Connect Specifications</li> <li>Security Assertion Markup Language (SAML)</li> </ul>"},{"location":"articles/windows-terminal-setup/","title":"Setting up Windows Terminal with Oh-My-Posh","text":"<p>In this tutorial, I'll guide you through the process of customizing your Windows terminal using Oh-My-Posh. </p> <p><code>Oh My Posh</code> is a highly customizable prompt engine designed to elevate your command-line experience. With full support for colors, it allow users to create visually appealing and informative prompts in various shell environments.</p>"},{"location":"articles/windows-terminal-setup/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the following prerequisites in place:</p> <ul> <li>Windows 10 or later</li> <li>PowerShell - make sure latest version is installed</li> <li>Chocolatey</li> <li>A Windows Terminal</li> </ul>"},{"location":"articles/windows-terminal-setup/#step-1-install-windows-terminal","title":"Step 1: Install Windows Terminal","text":"<p>If you haven't already installed Windows Terminal, you can install from choco tool or get it from the Microsoft Store</p> <p>Microsoft Store:</p> <ul> <li>Open the Microsoft Store.</li> <li>Search for \"Windows Terminal\" and click on the application's page.</li> <li>Click the \"Get\" or \"Install\" button to download and install it.</li> </ul> <p>Once Windows Terminal is installed, you can open it by searching for \"Windows Terminal\" in the Windows Start menu.</p>"},{"location":"articles/windows-terminal-setup/#step-2-install-oh-my-posh","title":"Step 2: Install Oh-My-Posh","text":"<p>Open your terminal and run the following command:</p> <pre><code>choco install oh-my-posh\n</code></pre> <p></p> <p>This command will download and install Oh-My-Posh on your system. </p>"},{"location":"articles/windows-terminal-setup/#step-3-configure-oh-my-posh","title":"Step 3: Configure Oh My Posh","text":"<p>Once Oh-My-Posh is installed, you can configure your PowerShell prompt to use a custom theme. To configure your prompt, follow these steps:</p> <p>Open your PowerShell profile for editing</p> <pre><code>code $profile\n# or\nnotepad.exe $PROFILE\n</code></pre> <p>file will be empty initially, insert following</p> <pre><code>oh-my-posh.exe init pwsh | Invoke-Expression\n</code></pre> <p>Save and close your profile</p> <p>open a new Windows Terminal instance to see the updated prompt with your chosen theme.</p> <p></p>"},{"location":"articles/windows-terminal-setup/#step-4-install-nerd-font","title":"Step 4: Install Nerd Font","text":"<p>Nerd Fonts are specialized fonts that include a wide range of icons and symbols commonly used in programming and terminal applications. These fonts are popular among developers and users who want to enhance their terminal experience. Here are the steps to install a Nerd Font on your system:</p> <ul> <li> <p>Choose a Nerd Font: Before you can install a Nerd Font, you need to decide which Nerd Font variant you want to use.You can find a list of available Nerd Fonts on the Nerd Fonts GitHub repository. <code>DejaVu Sans Mono Nerd Font</code> - is my favorite</p> </li> <li> <p>Download the Nerd Font: Visit the following webstie to download your chosen of Nerd Font. https://www.nerdfonts.com/font-downloads</p> </li> <li> <p>Install the Nerd Font: Once you have downloaded the Nerd Font, click on <code>install</code> button to install it on your Windows system:      </p> </li> <li> <p>Configure Windows Terminal to Use the Nerd Font: To use the Nerd Font in Windows Terminal, you'll need to configure your terminal settings. Here's how to do it:</p> <p></p> <p></p> </li> </ul> <p>To apply the changes, close and reopen Windows Terminal. </p>"},{"location":"articles/windows-terminal-setup/#step-5-verify-the-oh-my-posh-install","title":"Step 5: Verify the Oh My Posh Install","text":"<p>To verify that Oh-My-Posh is correctly installed and configured, open Windows Terminal and ensure that you see your custom prompt with the selected theme. You should see a stylish and informative prompt that includes Git status, time, and other relevant information.</p> <p>Now, You've successfully set up Windows Terminal with Oh-My-Posh, enhancing your command-line experience on Windows.</p> <p></p>"},{"location":"articles/windows-terminal-setup/#conclusion","title":"Conclusion","text":"<p>By following these steps, you've successfully set up Windows Terminal with Oh-My-Posh and a Nerd Font. You now have a highly customizable and visually appealing terminal environment on your Windows system, perfect for coding and everyday tasks.</p> <p>Feel free to explore different Oh-My-Posh themes and Nerd Font variants to personalize your terminal even further.</p>"},{"location":"articles/windows-terminal-setup/#resources","title":"Resources","text":"<ul> <li>Oh My Posh - Official Website</li> <li>Nerd Fonts</li> <li>microsoft/terminal</li> <li>Installation</li> </ul>"},{"location":"awards/","title":"Awards &amp; Recognition","text":"<p>As a recognized expert in cloud architecture, AI/ML, Infrastructure as Code, and enterprise cloud-native solutions, I hold distinguished memberships in professional organizations and serve as a judge for international technology awards. These recognitions reflect peer validation of technical expertise and contributions to the field.</p>"},{"location":"awards/#professional-memberships","title":"Professional Memberships","text":"<ul> <li> <p>\ud83c\udf93 SAS Fellow</p> <p>International Society for Academic &amp; Scientific Society</p> <p>Level: Fellow Member Certificate: View Certificate Organization: SAS Society Awarded: 2025 Status: Active</p> <p>Recognition: Distinguished fellowship acknowledging scholarly contributions and research excellence in computer science and software engineering fields.</p> </li> <li> <p>\ud83d\udc51 IOASD Royal Fellow</p> <p>International Organization for Academic and Scientific Development</p> <p>Level: Royal Fellow Certificate: View Certificate Organization: IOASD Awarded: 2025 Status: Active</p> <p>Recognition: Highest level of fellowship recognizing extraordinary contributions to academic and scientific advancement in technology and research domains.</p> </li> <li> <p>\u26a1 IEEE Member</p> <p>Institute of Electrical and Electronics Engineers</p> <p>Level: Member Certificate: View Certificate Organization: IEEE Joined: 2025 Status: Active</p> <p>Recognition: Recognized by IEEE for significant contributions to engineering, research, and technical leadership. IEEE membership represents global recognition in electrical engineering, computer science, and information technology.</p> </li> </ul>"},{"location":"awards/#industry-judging-peer-reviewing-activities","title":"Industry Judging &amp; Peer Reviewing Activities","text":"<ul> <li> <p>\ud83c\udfc6 Globee Awards Judge</p> <p>Independent Judge - Technology &amp; Innovation Excellence</p> <p>Selected by Globee Awards to evaluate global enterprise submissions across cloud architecture, AI/ML, and infrastructure solutions.</p> <p>Judging Period: 2025 - Present</p> <p>Certificates: </p> <ul> <li>View Impact Certificate </li> <li>View Leadership Certificate </li> <li>View Business Certificate</li> </ul> <p>Evaluation Focus:</p> <p>Impact: Technology-driven transformation outcomes, AI/ML innovation results Leaders: Innovation leadership, strategic technology vision Business: Enterprise solutions excellence, operational efficiency</p> </li> <li> <p>\u2b50 Stevie Awards Member</p> <p>International Business &amp; Technology Awards</p> <p>Member of Stevie Awards, the world's premier business awards organization recognizing excellence in innovation and technology achievement.</p> <p>Joined: 2025 Status: Active</p> <p>Technology Leadership:</p> <p>Digital Transformation: Enterprise modernization strategies, cloud-native architecture adoption, technology-driven business impact AI/ML Innovation: Large Language Models (LLMs), AI Agents, intelligent automation, machine learning integration Architecture Excellence: Microservices platforms, distributed systems, API Gateway, event-driven architectures, emerging technologies adoption</p> </li> <li> <p>\ud83d\udcdd Academic Peer Review</p> <p>Peer Reviewer / Editorial Team</p> <p>Journals:</p> <ul> <li>International Numeric Journal of Machine Learning and Robots</li> <li>International Journal of Machine Learning and Artificial Intelligence</li> <li>International Meridian Journal</li> </ul> <p>Focus Areas: </p> <ul> <li>Cloud computing </li> <li>Cloud-native architecture </li> <li>Microservices </li> <li>Distributed systems </li> <li>AI/ML,  MLOps </li> <li>Data engineering </li> <li>Enterprise architecture</li> </ul> </li> </ul>"},{"location":"awards/#recognition-significance","title":"Recognition Significance","text":""},{"location":"awards/#peer-validation","title":"Peer Validation","text":"<p>Being selected as a judge for international technology awards represents peer recognition of technical authority and deep domain expertise. These judging roles involve:</p> <ul> <li>Evaluating Excellence: Reviewing submissions from Fortune 500 companies and global enterprises</li> <li>Technical Assessment: Assessing innovation, implementation quality, and business impact  </li> <li>Industry Standards: Contributing to international standards of technology excellence</li> <li>Sustained Recognition: Ongoing selection across multiple award programs and years</li> </ul>"},{"location":"awards/#professional-distinction","title":"Professional Distinction","text":"<p>Professional memberships at Fellow and Royal Fellow levels indicate:</p> <ul> <li>Academic Recognition: Peer validation of scholarly contributions and research quality in cloud computing, AI/ML, and distributed systems</li> <li>Scientific Contributions: Acknowledgment of advancing knowledge in enterprise cloud-native technologies, Infrastructure as Code, and modern application architectures</li> <li>Professional Standing: Membership in selective organizations requiring demonstrated excellence</li> <li>Global Network: Connections with international academic and scientific communities focused on emerging technologies</li> </ul>"},{"location":"awards/#why-these-recognitions-matter","title":"Why These Recognitions Matter","text":"<p>These honors aren't just credentials\u2014they represent trust and validation from the global technology community:</p> <p>For Organizations: When evaluating enterprise solutions, I bring perspective from reviewing hundreds of submissions from Fortune 500 companies across cloud, AI/ML, and infrastructure domains. This exposure to diverse implementations informs the architectural guidance I provide.</p> <p>For Engineers: My fellowship memberships connect me to international networks of researchers and practitioners, ensuring I stay at the forefront of emerging technologies and best practices. These connections enhance the tutorials, articles, and mentorship I provide through anjikeesari.com.</p> <p>For the Field: Judging international awards means I help shape industry standards and recognize excellence that advances our entire profession. It's a responsibility I take seriously\u2014celebrating innovation while maintaining rigorous technical standards.</p> <p>These recognitions confirm what I already believe: technology's greatest impact comes through sharing knowledge, mentoring teams, and contributing to collective advancement.</p>"},{"location":"awards/#lets-connect","title":"Let's Connect","text":"<p>Interested in collaboration, speaking opportunities, or award nominations? Get in touch \u2192</p>"},{"location":"contact/","title":"Contact","text":"<p>I'm always interested in connecting with fellow technology professionals, researchers, and organizations. Let's discuss Building AI Agents, Microservices Architecture, Cloud-Native Solutions, or explore collaboration opportunities in enterprise technology transformation.</p>"},{"location":"contact/#get-in-touch","title":"Get in Touch","text":"<ul> <li>Email: anjkeesari@gmail.com</li> <li>Location: San Ramon, California, USA</li> <li>Website: anjikeesari.com</li> </ul>"},{"location":"contact/#open-to-collaboration","title":"Open to Collaboration","text":"<p>I welcome opportunities in:</p> <ul> <li>AI Agents &amp; Intelligent Automation - Building AI agents, AI Agent Frameworks, and LLM integration</li> <li>Microservices Architecture - Designing and implementing cloud-native microservices, Kubernetes orchestration, event-driven systems</li> <li>Research partnerships in cloud architecture, AI/ML, and enterprise cloud-native solutions</li> <li>Technical judging for technology innovation awards and competitions</li> <li>Speaking engagements at conferences and corporate events</li> <li>Consulting on cloud migration, AI/ML architecture, and Infrastructure as Code</li> </ul> <p>Let's connect and explore how we can collaborate!</p>"},{"location":"developertools/","title":"Tools","text":"<p>Welcome to the Tools section - your comprehensive resource hub for modern cloud-native development, infrastructure automation, and AI engineering.</p> <p>This section provides practical guides, quick-reference cheat sheets, and setup instructions to accelerate your development workflow. Whether you're setting up a new workstation, learning Kubernetes, or building AI applications, you'll find battle-tested resources based on real-world production experience.</p>"},{"location":"developertools/#what-youll-find-here","title":"What You'll Find Here","text":""},{"location":"developertools/#setup-guides","title":"Setup Guides","text":"<p>Step-by-step installation guides for essential developer tools and software across multiple operating systems:</p> <ul> <li>Workstation Configuration - Hardware recommendations and best practices for 2025 developer workstations</li> <li>Windows Setup - Complete Windows development environment setup with Chocolatey automation</li> <li>macOS Setup - Comprehensive macOS development tools installation with Homebrew</li> </ul>"},{"location":"developertools/#network-tools","title":"Network Tools","text":"<p>Essential network troubleshooting and DNS tools for debugging connectivity issues, DNS resolution, and infrastructure diagnostics.</p>"},{"location":"developertools/#cheat-sheets","title":"Cheat Sheets","text":"<p>Quick-reference guides for the most commonly used developer tools and technologies:</p> <p>Version Control &amp; Development * Git - Version control commands and workflows * .NET CLI - .NET SDK commands, project management, EF Core</p> <p>Containers &amp; Orchestration * Docker - Container management commands * Dockerfile - Container image building best practices * Docker Compose - Multi-container application orchestration * Kubectl - Kubernetes cluster management * Helm - Kubernetes package manager * ArgoCD - GitOps continuous delivery</p> <p>Infrastructure &amp; Cloud * Terraform - Infrastructure as Code commands * Azure CLI - Azure cloud management * Azure ACR - Azure Container Registry operations</p> <p>Databases &amp; Data * PostgreSQL - Database management and operations * Redis - In-memory data store commands</p> <p>Network &amp; Diagnostics * Dig - DNS lookup and troubleshooting * Cloud Compare - Cloud provider feature comparison</p>"},{"location":"developertools/#ai-engineering","title":"AI Engineering","text":"<p>Resources for building AI-powered applications and understanding modern AI/ML technologies:</p> <ul> <li>AI Concepts - Fundamental AI/ML concepts and terminology</li> <li>Learning Roadmap - Structured path for AI engineering skill development</li> <li>Technology Stack - Tools and frameworks for AI application development</li> </ul>"},{"location":"developertools/#how-to-use-this-section","title":"How to Use This Section","text":"<p>For Quick Reference: Jump directly to any cheat sheet when you need a command syntax or pattern reminder.</p> <p>For Learning: Start with setup guides to establish your development environment, then explore cheat sheets for technologies you want to master.</p> <p>For AI Development: Follow the AI Engineering learning roadmap and reference the technology stack for tool selection.</p>"},{"location":"developertools/#contributing","title":"Contributing","text":"<p>Found an issue or want to suggest improvements? Contributions are welcome! Reach out via the Contact page.</p> <p>All content is regularly updated to reflect current best practices and tool versions.</p>"},{"location":"developertools/ai-engineering/ai-concepts/","title":"AI Concepts for Beginners","text":"<p>1. Introduction to Artificial Intelligence</p> <ul> <li>1.1 What is AI?</li> <li>1.2 History and Evolution of AI</li> <li>1.3 Applications of AI in Real Life</li> <li>1.4 Myths and Misconceptions about AI</li> </ul> <p>2. Types of AI</p> <ul> <li>2.1 Narrow AI vs General AI vs Super AI</li> <li>2.2 Reactive Machines vs Limited Memory vs Theory of Mind</li> <li>2.3 Strong AI vs Weak AI</li> </ul> <p>3. Key Fields of AI</p> <ul> <li>3.1 Machine Learning</li> <li>3.2 Deep Learning</li> <li>3.3 Natural Language Processing (NLP)</li> <li>3.4 Computer Vision</li> <li>3.5 Robotics</li> <li>3.6 Expert Systems</li> </ul> <p>4. Fundamentals of Machine Learning</p> <ul> <li>4.1 What is Machine Learning?</li> <li>4.2 Types of ML: Supervised, Unsupervised, Reinforcement Learning</li> <li>4.3 Key Concepts: Features, Labels, Training, Testing</li> <li>4.4 Overfitting and Underfitting</li> <li>4.5 Model Evaluation Metrics</li> </ul> <p>5. Introduction to Neural Networks</p> <ul> <li>5.1 What is a Neural Network?</li> <li>5.2 Architecture: Input, Hidden, and Output Layers</li> <li>5.3 Activation Functions</li> <li>5.4 Training with Backpropagation and Gradient Descent</li> </ul> <p>6. Working with Data</p> <ul> <li>6.1 Importance of Data in AI</li> <li>6.2 Data Collection and Cleaning</li> <li>6.3 Feature Engineering</li> <li>6.4 Data Preprocessing Techniques</li> </ul> <p>7. Tools and Programming Languages for AI</p> <ul> <li>7.1 Why Python is Popular in AI</li> <li>7.2 Key Libraries and Frameworks (NumPy, Pandas, scikit-learn, TensorFlow, PyTorch)</li> <li>7.3 Using Jupyter Notebooks and Google Colab</li> </ul> <p>8. AI Development Workflow</p> <ul> <li>8.1 Defining the Problem</li> <li>8.2 Preparing the Dataset</li> <li>8.3 Selecting and Training a Model</li> <li>8.4 Evaluating the Model</li> <li>8.5 Hyperparameter Tuning</li> <li>8.6 Model Deployment (Basic Overview)</li> </ul> <p>9. Hands-On Beginner Projects</p> <ul> <li>9.1 Spam Email Classifier</li> <li>9.2 Handwritten Digit Recognition (MNIST)</li> <li>9.3 Sentiment Analysis of Movie Reviews</li> <li>9.4 Basic Chatbot using NLP</li> <li>9.5 Image Classifier with CNNs</li> <li>9.6 House Price Predictor</li> <li>9.7 Stock Price Trend Classifier</li> <li>9.8 Rock, Paper, Scissors Game with Computer Vision</li> <li>9.9 Fake News Detector</li> <li>9.10 Music Genre Classifier</li> <li>9.11 Language Detection App</li> <li>9.12 Number Plate Reader (OCR)</li> <li>9.13 Personal Voice Assistant (Basic)</li> </ul>"},{"location":"developertools/ai-engineering/ai-learning-roadmap/","title":"AI Engineer 2025: Learning Roadmap","text":"<p>1. Foundations of AI</p> <ul> <li>1.1 What is Artificial Intelligence?</li> <li>1.2 AI vs ML vs DL: Understanding the Differences</li> <li>1.3 Applications of AI in Industry</li> <li>1.4 History and Evolution of AI</li> </ul> <p>2. Mathematics for AI</p> <ul> <li>2.1 Linear Algebra<ul> <li>Vectors</li> <li>Matrices</li> <li>Eigenvalues and eigenvectors</li> <li>Matrix operations</li> </ul> </li> <li>2.2 Calculus<ul> <li>Limits</li> <li>Differentiation</li> <li>Integration</li> <li>Multivariable calculus</li> <li>Vector calculus</li> </ul> </li> <li>2.3 Probability and Statistics<ul> <li>Probability theory</li> <li>Random variables</li> <li>Probability distributions</li> <li>Statistical inference</li> <li>Bayesian statistics</li> </ul> </li> <li>2.4 Optimization Techniques <ul> <li>Gradient Descent</li> <li>Cost Functions</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>Khan Academy \u2013 Linear Algebra </li> <li>FreeCodeCamp \u2013 Linear Algebra </li> <li>StatQuest with Josh Starmer \u2013 YouTube </li> <li>FreeCodeCamp \u2013 Calculus </li> <li>Probability for Machine Learning</li> </ul> <p>Why learn this? </p> <p>Mathematics is essential to understand how AI models learn and optimize themselves. These topics will help you understand the inner workings of learning algorithms.</p> <p>3. Programming for AI</p> <ul> <li>3.1 Python<ul> <li>Basic syntax &amp; Variables</li> <li>Data structures</li> <li>Control structures</li> <li>Functions and modules</li> <li>Object-oriented programming</li> </ul> </li> <li>3.1 AI-related libraries<ul> <li>NumPy</li> <li>Pandas</li> <li>Data Visualization (Matplotlib, Seaborn)</li> <li>TensorFlow</li> <li>PyTorch</li> <li>Jupyter Notebooks &amp; Google Colab</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>W3Schools Python Tutorial</li> <li>FreeCodeCamp Python Course </li> <li>RealPython Python Basics</li> </ul> <p>Why learn this?</p> <p>Python is the most widely used programming language in AI. You need it to implement algorithms and work with data.</p> <p>4. Machine Learning (ML)</p> <ul> <li>4.1 Supervised Learning <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Support vector machines</li> <li>Decision Trees</li> <li>Random forests</li> <li>Gradient boosting machines</li> </ul> </li> <li>4.2 Unsupervised Learning <ul> <li>Clustering (K-means, DBSCAN)</li> <li>Dimensionality reduction (PCA, t-SNE)</li> <li>Anomaly detection</li> </ul> </li> <li>4.3 Reinforcement learning<ul> <li>Markov decision processes</li> <li>Q-learning</li> <li>Deep Q-networks</li> <li>Policy gradients</li> <li>Actor-critic methods</li> </ul> </li> <li>4.2 Evaluation and validation<ul> <li>Training, validation, and test sets</li> <li>Cross-validation</li> <li>Model selection and hyperparameter tuning</li> <li>Performance metrics</li> </ul> </li> </ul> <p>Why learn this?</p> <p>This is where AI begins \u2014 enabling machines to learn patterns and make predictions.</p> <p>Tools:</p> <ul> <li>Scikit-learn, </li> <li>Google Colab, </li> <li>Real Datasets from Kaggle and UCI ML Repository</li> </ul> <p>Resources:</p> <ul> <li>Google\u2019s Machine Learning Crash Course </li> <li>Kaggle Intro to Machine Learning </li> <li>DataCamp ML with Python </li> </ul> <p>Projects:</p> <ul> <li>Titanic Survival Prediction \u2013 YouTube Tutorial </li> <li>House Price Prediction    </li> <li>Spam Email Classifier    </li> <li>Customer Segmentation</li> </ul> <p>5. Deep Learning(DL)</p> <ul> <li>5.1 Neural networks<ul> <li>Multilayer perceptrons</li> <li>Activation functions</li> <li>Backpropagation</li> <li>Optimization algorithms</li> </ul> </li> <li>5.2 Convolutional neural networks<ul> <li>Convolutional layers</li> <li>Pooling layers</li> <li>Architectures (LeNet, AlexNet, VGG, ResNet)</li> </ul> </li> <li>5.3 Recurrent neural networks (RNNs)<ul> <li>Long short-term memory (LSTM)</li> <li>Gated recurrent units (GRU)</li> <li>Sequence-to-sequence models</li> </ul> </li> <li>5.4 Generative models<ul> <li>Variational autoencoders (VAE)</li> <li>Generative adversarial networks (GAN)</li> <li>Transformer models (BERT, GPT-2, T5)</li> </ul> </li> </ul> <p>Resources:</p> <ul> <li>DeepLearning.AI on Coursera </li> <li>Stanford CS231n </li> <li>PyTorch YouTube Tutorials </li> </ul> <p>Projects:</p> <ul> <li>Handwritten Digit Recognition \u2013 YouTube Tutorial </li> <li>Sentiment Analysis using Recurrent Networks    </li> <li>Build a Chatbot using Sequence-to-Sequence Models</li> </ul> <p>6. Natural Language Processing (NLP)</p> <ul> <li>6.1 Text preprocessing<ul> <li>Tokenization</li> <li>Stemming and lemmatization</li> <li>Stopword removal</li> <li>Part-of-speech tagging</li> </ul> </li> <li>6.2 Feature extraction<ul> <li>Bag of words</li> <li>TF-IDF</li> <li>Word embeddings (Word2Vec, GloVe,FastText)</li> </ul> </li> <li>6.3 Text classification<ul> <li>Sentiment analysis</li> <li>Topic modeling</li> </ul> </li> <li>6.4 Sequence Modeling (RNNs, LSTMs)<ul> <li>Named entity recognition</li> <li>Text summarization</li> <li>Machine translation</li> </ul> </li> <li>6.5 Transformers (BERT, GPT, T5)</li> <li>6.6 Chatbots and Language Generation</li> </ul> <p>Tools:</p> <ul> <li>NLTK, spaCy, Hugging Face Transformers, TensorFlow, PyTorch</li> </ul> <p>Projects:</p> <ul> <li>Build a News Classifier using BERT    </li> <li>Chatbot with custom FAQ data using Transformers    </li> <li>Twitter Sentiment Classifier using Hugging Face pipeline</li> </ul> <p>7. Computer Vision</p> <ul> <li>7.1 Image Preprocessing Techniques<ul> <li>Filtering techniques</li> <li>Edge detection</li> <li>Feature extraction</li> </ul> </li> <li>7.2 CNN Architectures (VGG, ResNet, EfficientNet)</li> <li>7.3 Object Detection (YOLO, SSD, Faster R-CNN)<ul> <li>Sliding window approach</li> <li>Region-based CNN (R-CNN)</li> <li>YOLO (You Only Look Once)</li> </ul> </li> <li>7.4 Image Segmentation (U-Net, Mask R-CNN)<ul> <li>Semantic segmentation</li> <li>Instance segmentation</li> </ul> </li> <li>7.5 Face Recognition, OCR</li> <li>7.6 Pose estimation<ul> <li>2D pose estimation</li> <li>3D pose estimation</li> </ul> </li> </ul> <p>Tools:</p> <ul> <li>OpenCV, TensorFlow, PyTorch, Keras, PIL</li> </ul> <p>Projects:</p> <ul> <li>Object detection in webcam feed    </li> <li>OCR number plate reader    </li> <li>Real-time face mask detection</li> </ul> <p>8. Data Engineering for AI</p> <ul> <li>8.1 Data Collection &amp; Pipelines</li> <li>8.2 Data Cleaning and Imputation</li> <li>8.3 Feature Stores and Data Versioning</li> <li>8.4 Big Data Tools (Spark, Hadoop basics)</li> </ul> <p>Tools:</p> <ul> <li>Apache Airflow, Apache Kafka, DVC, Great Expectations, Spark (PySpark)</li> </ul> <p>9. Model Deployment &amp; MLOps</p> <ul> <li>9.1 Model Serialization (Pickle, ONNX, TorchScript)</li> <li>9.2 REST APIs for ML Models (Flask, FastAPI)</li> <li>9.3 Model Serving (TensorFlow Serving, TorchServe)</li> <li>9.4 Docker for AI Applications</li> <li>9.5 CI/CD for ML (GitHub Actions, Jenkins)</li> <li>9.6 MLflow &amp; Weights &amp; Biases for Experiment Tracking</li> <li>9.7 Monitoring and Scaling ML Systems</li> </ul> <p>Projects:</p> <ul> <li>Deploy a sentiment analysis model with FastAPI + Docker    </li> <li>Track model experiments with MLflow</li> </ul> <p>10. Cloud &amp; Edge AI</p> <ul> <li>10.1 AI on Cloud (AWS SageMaker, GCP Vertex AI, Azure ML)</li> <li>10.2 Using GPUs and TPUs</li> <li>10.3 Edge AI (TinyML, TensorFlow Lite, NVIDIA Jetson)</li> <li>10.4 Serverless AI Architectures</li> </ul> <p>Projects:</p> <ul> <li>Deploy model to Vertex AI endpoint    </li> <li>Run image classifier on Jetson Nano using TensorFlow Lite</li> </ul> <p>11. Responsible AI &amp; Ethics</p> <ul> <li>11.1 Fairness, Accountability, and Transparency</li> <li>11.2 Bias in Data and Models</li> <li>11.3 Privacy and Security in AI</li> <li>11.4 AI Regulation and Governance</li> </ul> <p>12. Real-World Projects</p> <ul> <li>12.1 Predictive Analytics (Time Series Forecasting)</li> <li>12.2 Image Classification &amp; Detection</li> <li>12.3 NLP Chatbot</li> <li>12.4 AI for Healthcare or Finance</li> <li>12.5 Recommender System</li> <li>12.6 Custom AI SaaS Product</li> </ul> <p>Platforms to Practice:</p> <ul> <li>Kaggle </li> <li>Google Colab </li> <li>Hugging Face Datasets </li> <li>PapersWithCode</li> </ul> <p>13. AI Books</p> <ul> <li>13.1 AI Engineering - by Chip Huyen<ul> <li>AI Engineering - Gitgub</li> </ul> </li> <li>13.2 Build a Large Language Model -  From Scratch - by Sebastian Raschka<ul> <li>Build a Large Language Model - Github</li> </ul> </li> <li>13.3 LLM Engineer's Handbook -  by Paul Iusztin, Maxime Labonne<ul> <li>LLM Engineer's Handbook - Github</li> <li>SylphAI-Inc - LLM-engineer-handbook - Github</li> </ul> </li> <li>13.4 Artificial Intelligence with Python by Prateek Joshi</li> <li>13.5 Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aur\u00e9lien G\u00e9ron</li> <li>13.6 Deep Learning with Python by Fran\u00e7ois Chollet</li> <li>13.7 Machine Learning Yearning by Andrew Ng</li> <li>13.8 Artificial Intelligence: A Modern Approach by Stuart Russell and Peter Norvig</li> </ul> <p>14. Courses</p> <ul> <li>14.1 Machine Learning by Andrew Ng on Coursera</li> <li>14.2 Deep Learning Specialization by Andrew Ng on Coursera</li> <li>14.3 Applied Data Science with Python Specialization on Coursera</li> <li>14.4 Introduction to Artificial Intelligence with Python on edX</li> </ul> <p>15. Articles</p> <ul> <li>15.1 A Beginner's Guide to AI/ML by Analytics Vidhya</li> <li>15.2 What is Artificial Intelligence? A Beginner\u2019s Guide by Builtin</li> </ul> <p>By completing this roadmap, you will not only understand how artificial intelligence works \u2014 but also build, deploy, and scale real-world AI applications confidently.</p>"},{"location":"developertools/ai-engineering/ai-technology-stack/","title":"AI Engineer 2025: Tools, Technologies, Frameworks, and Communities","text":"<p>Here is essential tools, frameworks, languages, and communities you need to master on to becoming a successful AI Engineer in 2025.</p> <p>1. Programming Languages for AI</p> <ul> <li>1.1 Python (Primary Language)</li> <li>1.2 SQL (for data querying and ETL)</li> <li>1.3 Bash/Shell Scripting (for automation)</li> <li>1.4 C++ (for performance-critical tasks, optional)</li> <li>1.5 JavaScript (for frontend AI integration, e.g., chatbots)</li> </ul> <p>2. Development Environments</p> <ul> <li>2.1 Jupyter Notebooks</li> <li>2.2 Google Colab</li> <li>2.3 VS Code with Python &amp; AI Extensions</li> <li>2.4 PyCharm Professional (for larger AI/ML codebases)</li> </ul> <p>3. Key Python Libraries &amp; Packages</p> <ul> <li>3.1 NumPy, Pandas \u2013 Data manipulation</li> <li>3.2 Matplotlib, Seaborn, Plotly \u2013 Visualization</li> <li>3.3 Scikit-learn \u2013 Classical ML</li> <li>3.4 OpenCV \u2013 Image processing</li> <li>3.5 NLTK, spaCy \u2013 Basic NLP tasks</li> <li>3.6 Hugging Face Transformers \u2013 State-of-the-art NLP/LLMs</li> <li>3.7 XGBoost, LightGBM, CatBoost \u2013 Gradient boosting models</li> <li>3.8 PyCaret \u2013 Low-code ML experimentation</li> </ul> <p>4. Deep Learning Frameworks</p> <ul> <li>4.1 TensorFlow (incl. Keras API)</li> <li>4.2 PyTorch (preferred in research and modern projects)</li> <li>4.3 FastAI (on top of PyTorch)</li> <li>4.4 JAX (for advanced, high-performance ML)</li> </ul> <p>5. Generative AI &amp; LLM Ecosystem</p> <ul> <li>5.1 Hugging Face Transformers</li> <li>5.2 LangChain \u2013 LLM application framework</li> <li>5.3 LlamaIndex \u2013 Indexing and querying over documents</li> <li>5.4 Vector Databases \u2013 FAISS, Weaviate, ChromaDB, Qdrant</li> <li>5.5 Open Source LLMs \u2013 Mistral, LLaMA3, Phi-3, Mixtral</li> <li>5.6 Ollama \u2013 Local model runner for open-source LLMs</li> </ul> <p>6. Data Engineering &amp; Processing Tools</p> <ul> <li>6.1 Apache Spark (PySpark) \u2013 Big Data</li> <li>6.2 Apache Kafka \u2013 Real-time data pipelines</li> <li>6.3 Airflow \u2013 Workflow orchestration</li> <li>6.4 DVC \u2013 Data version control</li> <li>6.5 Great Expectations \u2013 Data validation framework</li> </ul> <p>7. Model Training, Tuning &amp; Experimentation</p> <ul> <li>7.1 Hyperparameter Optimization \u2013 Optuna, Ray Tune</li> <li>7.2 Model Tracking \u2013 MLflow, Weights &amp; Biases (W\\&amp;B)</li> <li>7.3 Model Explainability \u2013 SHAP, LIME, Captum</li> <li>7.4 Checkpointing &amp; Model Saving \u2013 ONNX, TorchScript, Pickle</li> </ul> <p>8. Model Deployment &amp; MLOps</p> <ul> <li>8.1 REST APIs \u2013 Flask, FastAPI</li> <li>8.2 Model Serving \u2013 TensorFlow Serving, TorchServe</li> <li>8.3 Docker \u2013 Containerization for AI apps</li> <li>8.4 Kubernetes \u2013 Scalable model deployment</li> <li>8.5 CI/CD \u2013 GitHub Actions, Jenkins, Azure DevOps</li> <li>8.6 Model Monitoring \u2013 Prometheus, Grafana, Evidently AI</li> </ul> <p>9. Cloud &amp; Compute Platforms</p> <ul> <li>9.1 Google Cloud Platform \u2013 Vertex AI, Colab Pro</li> <li>9.2 AWS \u2013 SageMaker, EC2 GPU, Lambda</li> <li>9.3 Microsoft Azure \u2013 Azure Machine Learning</li> <li>9.4 RunPod, Lambda Labs \u2013 Pay-as-you-go GPUs</li> <li>9.5 Paperspace, Kaggle Kernels \u2013 Free GPU access for learning</li> </ul> <p>10. Frontend &amp; Interface for AI Apps</p> <ul> <li>10.1 Streamlit \u2013 Python-based dashboards</li> <li>10.2 Gradio \u2013 Rapid ML model demos</li> <li>10.3 Flask/FastAPI + React \u2013 Full-stack AI projects</li> <li>10.4 Next.js \u2013 Deploy LLM tools with modern UI</li> </ul> <p>11. Version Control &amp; Collaboration</p> <ul> <li>11.1 Git &amp; GitHub \u2013 Code and model versioning</li> <li>11.2 GitHub Actions \u2013 CI/CD pipelines</li> <li>11.3 Git LFS / DVC \u2013 Large model &amp; data versioning</li> <li>11.4 Hugging Face Spaces \u2013 Share models and apps</li> </ul> <p>12. Popular GitHub Projects to Explore &amp; Learn From</p> <ul> <li>12.1 fastai/fastbook \u2013 Deep learning curriculum</li> <li>12.2 huggingface/transformers \u2013 NLP &amp; LLM hub</li> <li>12.3 explosion/spaCy \u2013 Industrial NLP</li> <li>12.4 mistralai \u2013 Open-source LLMs (Mixtral, Mistral)</li> <li>12.5 mlflow/mlflow \u2013 ML lifecycle management</li> <li>12.6 automl/auto-sklearn \u2013 AutoML pipeline builder</li> <li>12.7 openai/whisper \u2013 Speech-to-text model</li> <li>12.8 llamaindex/llamaindex \u2013 RAG apps</li> <li>12.9 langchain-ai/langchain \u2013 LLM apps with memory</li> <li>12.10 openai/chatgpt-retrieval-plugin \u2013 RAG plugin architecture</li> </ul> <p>13. AI Communities &amp; Learning Platforms</p> <ul> <li>13.1 Hugging Face Community &amp; Discord</li> <li>13.2 Papers with Code (SOTA models and papers)</li> <li>13.3 Kaggle \u2013 Competitions, datasets, and notebooks</li> <li>13.4 Reddit \u2013 r/MachineLearning, r/learnmachinelearning</li> <li>13.5 GitHub \u2013 Following trending ML/AI repositories</li> <li>13.6 Discord Servers \u2013 MLOps Community, Deep Learning.ai</li> <li>13.7 Meetup &amp; Devpost \u2013 AI hackathons and local events</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/","title":"ArgoCD Commands","text":""},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used ArgoCD commands with examples.</p> <p><code>ArgoCD</code> is a popular tool for managing Kubernetes applications and deploying them in a declarative manner. ArgoCD provides a web UI, but it also has a command-line interface (CLI) that can be used to manage applications, repositories, and other resources.</p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Argocd CLI</li> <li>Azure login</li> <li>Select the subscription</li> <li>Connect to k8s Cluster</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#installing-argocd","title":"Installing ArgoCD","text":"<p>Use the following commands to install ArgoCD CLI in MacOS and Windows.</p> <pre><code># MacOS (using Homebrew):\nbrew install argocd\n\n# Windows OS (using Choco)\nchoco install argocd-cli\n\n# Verify the installation by running\nargocd version\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#connection-to-kubernetes-cluster","title":"Connection to Kubernetes cluster","text":"<p>Note: Make sure that you log in to Azure, select the Azure subscription, and connect to the Kubernetes cluster before running any <code>argocd</code> commands.</p> <pre><code># Azure login\naz login\n\n# Select the subscription\naz account set -s \"anji.keesari\"\naz account show --output table\n\n# Connect to k8s Cluster\n\n# Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-commands-description","title":"ArgoCD Commands &amp; description","text":"<p>Here are some common ArgoCD CLI commands and their purposes:</p> <ul> <li><code>argocd login:</code> This command logs in to an ArgoCD server and saves the session token locally.</li> <li><code>argocd app create:</code> This command creates a new application from a Git repository.</li> <li><code>argocd app get:</code> This command retrieves information about an existing application, such as its status and configuration.</li> <li><code>argocd app sync:</code> This command synchronizes an application's configuration with the desired state specified in its Git repository.</li> <li><code>argocd app delete:</code> This command deletes an application from ArgoCD.</li> <li><code>argocd app diff:</code> This command displays the differences between the current state of an application and the desired state specified in its Git repository.</li> <li><code>argocd app history:</code> This command lists the deployment history of an application in ArgoCD.</li> <li><code>argocd app rollback:</code> This command rolls back an application to a previous deployment revision.</li> <li><code>argocd repo add:</code> This command adds a Git repository to ArgoCD's list of managed repositories.</li> <li><code>argocd repo list:</code> This command lists all the Git repositories that ArgoCD is currently managing.</li> <li><code>argocd repo rm:</code> This command removes a Git repository from ArgoCD's list of managed repositories.</li> <li><code>argocd repo list-resources:</code> This command lists all the Kubernetes resources in a Git repository.</li> <li><code>argocd proj create:</code> This command creates a new project in ArgoCD, which can be used to group related applications and apply shared policies.</li> <li><code>argocd proj get:</code> This command retrieves information about an existing project, such as its applications and policies.</li> <li><code>argocd proj list:</code> This command lists all the projects in ArgoCD.</li> <li><code>argocd proj delete:</code> This command deletes a project from ArgoCD.</li> <li><code>argocd cluster add:</code> This command adds a new Kubernetes cluster to ArgoCD's list of managed clusters.</li> <li><code>argocd cluster list:</code> This command lists all the Kubernetes clusters that ArgoCD is currently managing.</li> <li><code>argocd cluster rm:</code> This command removes a Kubernetes cluster from ArgoCD's list of managed clusters.</li> <li><code>argocd account update-password:</code> This command allows you to change the password for your ArgoCD account.</li> <li><code>argocd account list:</code> This command lists all the user accounts that have access to ArgoCD.</li> <li><code>argocd version:</code> This command retrieves the current version of ArgoCD.</li> </ul>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-help","title":"ArgoCD help","text":"<p>This command provides general help and usage information about ArgoCD. It gives an overview of available commands and their usage.</p> <pre><code>argocd help\n# or\nargocd --help\n</code></pre> <p>Output:</p> <pre><code>Available Commands:\n  account     Manage account settings\n  admin       Contains a set of commands useful for Argo CD administrators and requires direct Kubernetes access\n  app         Manage applications\n  cert        Manage repository certificates and SSH known hosts entries\n  cluster     Manage cluster credentials\n  completion  output shell completion code for the specified shell (bash or zsh)\n  context     Switch between contexts\n  gpg         Manage GPG keys used for signature verification\n  help        Help about any command\n  login       Log in to Argo CD\n  logout      Log out from Argo CD\n  proj        Manage projects\n  relogin     Refresh an expired authenticate token\n  repo        Manage repository connection parameters\n  repocreds   Manage repository connection parameters\n  version     Print version information\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#argocd-commands-help","title":"ArgoCD commands help","text":"<p>This command provides detailed help and usage information about individual ArgoCD commands. It can be used to get specific information about any command's usage, options, and arguments.</p> <pre><code>argocd help app\n</code></pre> <p>Output:</p> <pre><code>Manage applications\n\nUsage:\n  argocd app [flags]\n  argocd app [command]        \n\nExamples:\n  # List all the applications.\n  argocd app list\n\n  # Get the details of a application\n  argocd app get my-app\n\n  # Set an override parameter\n  argocd app set my-app -p image.tag=v1.0.1\n\nAvailable Commands:\n  actions         Manage Resource actions\n  create          Create an application\n  delete          Delete an application\n  delete-resource Delete resource in an application\n  diff            Perform a diff against the target and live state.\n  edit            Edit application\n  get             Get application details\n  history         Show application deployment history\n  list            List applications\n  logs            Get logs of application pods\n  manifests       Print manifests of an application\n  patch           Patch application\n  patch-resource  Patch resource in an application\n  resources       List resource of application\n  rollback        Rollback application to a previous deployed version by History ID, omitted will Rollback to the previous version\n  set             Set application parameters\n  sync            Sync an application to its target state\n  terminate-op    Terminate running operation of an application\n  unset           Unset application parameters\n  wait            Wait for an application to reach a synced and healthy state\n</code></pre> <pre><code>argocd help repo\n</code></pre> <p>Output:</p> <pre><code>Manage repository connection parameters\n\nUsage:\n  argocd repo [flags]\n  argocd repo [command]\n\nAvailable Commands:\n  add         Add git repository connection parameters\n  get         Get a configured repository by URL\n  list        List configured repositories\n  rm          Remove repository credentials\n</code></pre> <pre><code>argocd help account\n</code></pre> <p>Output:</p> <pre><code>Manage account settings\n\nUsage:\n  argocd account [flags]\n  argocd account [command]\n\nAvailable Commands:\n  can-i           Can I\n  delete-token    Deletes account token\n  generate-token  Generate account token\n  get             Get account details\n  get-user-info   Get user info\n  list            List accounts\n  update-password Update an account's password\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#login-to-argocd","title":"Login to ArgoCD","text":"<p>Before running the next set of commands, you need to log in to ArgoCD.</p> <p>To log in to ArgoCD, you can use the <code>argocd login</code> command followed by the URL of your ArgoCD server and your credentials. Here's an example:</p> <pre><code>argocd login &lt;ARGOCD_SERVER&gt; [--insecure] [--username &lt;USERNAME&gt;] [--password &lt;PASSWORD&gt;]\n</code></pre> <p>Examples:</p> <p><pre><code># Login using domain name\nargocd login yourdomainname.com\n\n# Login using IP address of the ArgoCD service\nargocd login 20.241.96.132\n\n# Login to localhost\nargocd login localhost:8080\n</code></pre> Note: By default, the Argo CD API server is not exposed with an external IP. To access the API server, change the argocd-server service type to LoadBalancer:</p> <p><pre><code>kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n</code></pre> If your ArgoCD server is using a self-signed SSL certificate, you may need to use the --insecure flag to bypass SSL verification.</p> <p>Enter ArgoCD credentials: <pre><code>Username: admin\nPassword: &lt;your-password&gt;\n</code></pre></p> <p>Note: In bash, you may need to right-click to paste the password instead of using keyboard shortcuts.</p> <p>Output:</p> <pre><code>WARNING: server is not configured with TLS. Proceed (y/n)? y\nUsername: admin\nPassword: \n'admin:login' logged in successfully\nContext '20.241.96.132' updated\n</code></pre> <p>If you use a domain name URL, you will see output like this:</p> <pre><code>time=\"2022-11-20T09:44:31-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web.\"\nUsername: admin\nPassword: \n'admin:login' logged in successfully\nContext 'yourdomainname.com' updated\n</code></pre> <p>Note: The <code>--grpc-web</code> warning can be avoided by adding the <code>--grpc-web</code> flag to your argocd commands.</p> <p>Once you have logged in successfully, ArgoCD will save a session token locally so that you don't have to log in again for subsequent commands.</p>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#cluster-list","title":"Cluster List","text":"<p>This command lists the clusters connected to the ArgoCD server. It displays information about each cluster, such as name, server URL, and current context.</p> <pre><code>argocd cluster list\n</code></pre> <p>Output:</p> <pre><code>time=\"2022-11-20T09:48:31-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nSERVER                          NAME        VERSION  STATUS      MESSAGE  PROJECT\nhttps://kubernetes.default.svc  in-cluster  1.22     Successful\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#add-cluster","title":"Add Cluster","text":"<p>This command is used to add a new external Kubernetes cluster to the ArgoCD server. It requires specifying the cluster's context name, server URL, and authentication credentials.</p> <pre><code>argocd cluster add aks-cluster2-dev\n</code></pre> <p>Output:</p> <pre><code>WARNING: This will create a service account `argocd-manager` on the cluster referenced by context `aks-cluster2-dev` with full cluster level privileges. Do you want to continue [y/N]? y\n\n.\n.\n.\nCluster 'https://cluster2-dns-89d81b75.hcp.northcentralus.azmk8s.io:443' added\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#repository-list","title":"Repository List","text":"<p>This command lists the repositories configured in the ArgoCD server. It provides information about each repository, such as name, URL, and connection status.</p> <pre><code>argocd repo list\n</code></pre> <p>Output:</p> <pre><code>TYPE  NAME  REPO                                                  INSECURE  OCI    LFS    CREDS  STATUS      MESSAGE  PROJECT\ngit         https://github.com/argoproj/argocd-example-apps.git   false     false  false  false  Successful           default\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-list","title":"Application List","text":"<p>This command lists all applications managed by ArgoCD. It displays information about each application, including its name, project, health status, and synchronization status.</p> <pre><code>argocd app list\n</code></pre> <p>Output:</p> <pre><code>NAME        CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH       SYNCPOLICY  CONDITIONS  REPO                                                  PATH         TARGET\nguestbook   https://kubernetes.default.svc  default    default  Synced     Healthy      &lt;none&gt;      &lt;none&gt;      https://github.com/argoproj/argocd-example-apps.git   guestbook    HEAD \n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#app-resources","title":"App Resources","text":"<p>This command shows the Kubernetes resources associated with a specific application. It provides a detailed list of resources deployed by the application, including their types, names, and current status.</p> <pre><code>argocd app resources aspnetcore-webapp\n</code></pre> <p>Output:</p> <pre><code>time=\"2022-11-20T09:53:32-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nGROUP  KIND        NAMESPACE     NAME               ORPHANED\n       Service     sample  aspnetcore-webapp  No      \napps   Deployment  sample  aspnetcore-webapp  No      \n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-details","title":"Application Details","text":"<p>This command displays detailed information about a specific application. It shows information such as the application's project, repository, target revision, and synchronization status.</p> <pre><code>argocd app get aspnetcore-webapp\n</code></pre> <p>Output:</p> <pre><code>time=\"2022-11-20T09:54:28-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\ntime=\"2022-11-20T09:54:28-08:00\" level=warning msg=\"Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, \nuse flag --grpc-web.\"\nName:               aspnetcore-webapp\nProject:            development\nServer:             https://kubernetes.default.svc\nNamespace:          sample\nURL:                https://yourdomainname.com/applications/aspnetcore-webapp\nRepo:               https://dev.azure.com/keesari/microservices/_git/argocd\nTarget:             develop\nPath:               sample/aspnetcore-webapp\nSyncWindow:         Sync Allowed\nSync Policy:        Automated (Prune)\nSync Status:        Unknown\nHealth Status:      Healthy\n\nCONDITION        MESSAGE                                                   LAST TRANSITION\nComparisonError  rpc error: code = Unknown desc = authentication required  2022-11-19 21:07:07 -0800 PST\n\n\nGROUP  KIND        NAMESPACE     NAME               STATUS   HEALTH   HOOK  MESSAGE\n       Service     sample  aspnetcore-webapp  Unknown  Healthy        service/aspnetcore-webapp unchanged\napps   Deployment  sample  aspnetcore-webapp  Unknown  Healthy        deployment.apps/aspnetcore-webapp configured\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-delete","title":"Application Delete","text":"<p>This command is used to delete a specific application managed by ArgoCD. It removes the application and all associated resources from the Kubernetes cluster.</p> <pre><code>argocd app delete guestbook\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#application-sync","title":"Application Sync","text":"<p>This command triggers a synchronization of a specific application with its target state. It ensures that the application's deployed resources match the desired state defined in the repository.</p> <pre><code>argocd app sync guestbook\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#project-list","title":"Project List","text":"<p>This command lists the projects defined in ArgoCD. It provides information about each project, such as name, description, and application count.</p> <pre><code>argocd proj list\n</code></pre> <p>Output:</p> <pre><code>NAME                DESCRIPTION                                       DESTINATIONS    SOURCES  CLUSTER-RESOURCE-WHITELIST  NAMESPACE-RESOURCE-BLACKLIST  SIGNATURE-KEYS  ORPHANED-RESOURCES\ndefault                                                               *,*             *        */*                         &lt;none&gt;                        &lt;none&gt;          disabled\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#create-project","title":"Create Project","text":"<p>This command is used to create a new project in ArgoCD. It requires specifying the project name, optionally providing a description, and setting other project-specific configurations.</p> <pre><code>argocd proj create myproj\n</code></pre>"},{"location":"developertools/cheatsheets/argocd-cheat-sheet/#logout-argocd","title":"Logout ArgoCD","text":"<p>When you run <code>argocd logout</code>, ArgoCD will remove the session token that was saved when you logged in, so you will need to log in again with <code>argocd login</code> the next time you want to run any ArgoCD commands.</p> <pre><code>argocd logout 52.159.112.67\n</code></pre> <p>Output:</p> <pre><code>Logged out from '52.159.112.67'\n</code></pre> <p>These commands allow you to interact with ArgoCD, manage clusters, repositories, applications, projects, and perform various administrative tasks related to continuous deployment and GitOps workflows.</p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/","title":"Azure ACR Commands","text":""},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#introduction","title":"Introduction","text":"<p>The <code>az acr</code> commands are used for managing private registries with Azure Container Registries.</p> <p>This page contains a list of commonly used <code>az acr</code> commands.</p> <p>Note: Make sure that you log in to Azure and select the Azure subscription before running any <code>az acr</code> commands.</p>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#azure-login","title":"Azure Login","text":"<pre><code>az login\naz account list --output table\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#select-the-subscription","title":"Select the Subscription","text":"<pre><code>az account set -s \"anji.keesari\"\naz account show --output table\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#connect-to-container-registry","title":"Connect to Container Registry","text":"<pre><code>az acr login --name acr1dev\n</code></pre> <p>Output:</p> <pre><code>Login Succeeded\n</code></pre> <p>Troubleshoot:</p> <p>If you get the following error, run Docker Desktop to fix the issue:</p> <pre><code>You may want to use 'az acr login -n acr1dev --expose-token' to get an access token, which does not require Docker to be installed.\n2023-02-20 21:38:37.187022 An error occurred: DOCKER_COMMAND_ERROR\nerror during connect: This error may indicate that the docker daemon is not running.: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/json\": open //./pipe/docker_engine: The system cannot find the file specified.\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#get-the-login-server-address","title":"Get the Login Server Address","text":"<pre><code>az acr list -g \"rg-acr-dev\" --query \"[].{acrLoginServer:loginServer}\" --output table\n</code></pre> <p>Output:</p> <pre><code>AcrLoginServer\n-----------------------\nacr1dev.azurecr.io\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#import-container-images","title":"Import Container Images","text":"<pre><code>$acrName = \"acr1dev\"\n$imageName = \"mcr.microsoft.com/dotnet/aspnet:6.0\"\n\naz acr import --name $acrName --source $imageName --image $imageName\n</code></pre> <p>Or using direct parameters:</p> <pre><code>az acr import --name \"acr1dev\" --source \"mcr.microsoft.com/dotnet/sdk:6.0\" --image \"mcr.microsoft.com/dotnet/sdk:6.0\"\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#list-registries","title":"List Registries","text":"<p>Lists all the container registries under the current subscription.</p> <pre><code>az acr repository list --name acr1dev --output table\n</code></pre> <p>Output:</p> <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#show-tags","title":"Show Tags","text":"<p>Show tags of an image in the ACR.</p> <pre><code>az acr repository show-tags --name acr1dev --repository mcr.microsoft.com/dotnet/aspnet --output table\n</code></pre> <p>Output:</p> <pre><code>Result\n--------\n6.0\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#check-health","title":"Check Health","text":"<pre><code>az acr check-health -n \"acr1dev\" -y\n</code></pre> <p>Output:</p> <pre><code>Docker daemon status: available\nDocker version: 'Docker version 20.10.17, build a89b842, platform linux/amd64'\nDocker pull of 'mcr.microsoft.com/mcr/hello-world:latest' : OK\nAzure CLI version: 2.44.1\nDNS lookup to acr1dev.azurecr.io at IP 20.62.128.9 : OK\nChallenge endpoint https://acr1dev.azurecr.io/v2/ : OK\nFetch refresh token for registry 'acr1dev.azurecr.io' : OK\nFetch access token for registry 'acr1dev.azurecr.io' : OK\nHelm version: 3.8.2\n2023-02-20 21:58:29.062713 An error occurred: NOTARY_COMMAND_ERROR\nPlease verify if notary is installed.\n\nPlease refer to https://aka.ms/acr/errors#notary_command_error for more information.\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#helm-list","title":"Helm List","text":"<p>List all Helm charts in an Azure Container Registry.</p> <pre><code>az acr helm list -n 'acr1dev'\n</code></pre>"},{"location":"developertools/cheatsheets/az-acr-cheat-sheet/#references","title":"References","text":"<ul> <li>Azure ACR CLI Reference</li> </ul>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/","title":"Azure CLI Commands","text":""},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used Azure CLI commands with examples.</p> <p>The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. </p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#installing-azure-cli","title":"Installing Azure CLI","text":"<p>Use the following commands to install Azure CLI in Windows, MacOS and Linux environments.</p> <p><pre><code># MacOS (using Homebrew):\nbrew install azure-cli\n\n# Windows OS (using Choco)\nchoco install azure-cli\n\n# Verify the installation by running\naz --version\n\n# Update Azure CLI\naz upgrade\n</code></pre> For more information, refer to the official documentation:  - How to install the Azure CLI</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-login","title":"az login","text":"<pre><code>az login\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-account","title":"az account","text":"<pre><code>az account set -s \"anji-cloud\"\n\n# Get a list of subscriptions for the logged in account.\naz account list -o table \n\n# To view all the Azure subscription names and IDs for a specific Microsoft account,\naz account list --query \"[?user.name=='anjkeesari@gmail.com'].{Name:name, ID:id, Default:isDefault}\" --output Table\n\n# Get the details of a subscription.\n az account show\n az account show -o table\n\n# Get all subscriptions for a tenant.\naz account subscription list -o table\n\n# Get details about a specified subscription.\naz account subscription show --subscription-id \"85c49b84-b13d-4168-962c-8107c5b32b7e\"\n# or\naz account subscription show --id '85c49b84-b13d-4168-962c-8107c5b32b7e'\n\n# Get the tenants for your account.\naz account tenant list  \n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-group","title":"az group","text":"<pre><code># Create a new resource group\naz group create -l 'eastus' -n 'rg-demo'\n\n# List resource groups.\naz group list -o table\n\n# Check if a resource group exists.\naz group exists -n 'rg-demo'\n\n# Create a resource group lock\naz group lock create --lock-type ReadOnly -n lockName -g 'rg-demo'\naz group lock create --lock-type CanNotDelete -n lockName -g 'rg-demo'\n\n# List lock information in the resource group\naz group lock list -g 'rg-demo'\n\n# Show the details of a resource group lock\naz group lock show -n lockname -g 'rg-demo'\n\n# Delete a resource group lock\naz group lock delete -n lockName -g 'rg-demo'\n\n# Delete a resource group\naz group delete -n 'rg-demo'\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-aks","title":"az aks","text":"<p>Manage Azure Kubernetes Services - Reference</p> <pre><code># List managed Kubernetes clusters\naz aks list -o table\n\n# Get access credentials for a managed Kubernetes cluster\n\n# User authentication\naz aks get-credentials --name 'aks-cluster1-dev' --resource-group 'rg-aks-dev'\n\n# Admin authentication\naz aks get-credentials --name 'aks-cluster1-dev' --resource-group 'rg-aks-dev' --admin\n\n# Get the versions available for creating a managed Kubernetes cluster\naz aks get-versions --location westus2 -o table\n\n# Run a shell command\naz aks command invoke -n 'aks-cluster1-dev' -g 'rg-aks-dev' --command \"kubectl get namespaces\"\naz aks command invoke -n 'aks-cluster1-dev' -g 'rg-aks-dev' --command \"kubectl create namespace test\"\n\n# Download and install kubectl, the Kubernetes command-line tool\naz aks install-cli\n\n# List node pools in the managed Kubernetes cluster\naz aks nodepool list --cluster-name 'aks-cluster1-dev' -g 'rg-aks-dev'\naz aks nodepool list --cluster-name 'aks-cluster1-dev' -g 'rg-aks-dev' -o table\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-acr","title":"az acr","text":"<p>Manage private registries with Azure Container Registries. - Reference</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-ad","title":"az ad","text":"<p>Azure AD for Role Based Access Control - Reference</p>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#az-webapp","title":"az webapp","text":"<p>Manage web app logs - Reference</p> <pre><code># Start live log tracing for a web app\naz webapp log tail --name 'feedback-api-dev' --resource-group 'aklab-rg-dev'\n\n# Download a web app's log history as a zip file\naz webapp log download --name 'feedback-api-dev' --resource-group 'aklab-rg-dev' --log-file webapp_624221039.zip\n</code></pre>"},{"location":"developertools/cheatsheets/az-cli-cheat-sheet/#reference","title":"Reference","text":"<ul> <li>Azure CLI reference</li> </ul>"},{"location":"developertools/cheatsheets/cloud-compare-cheat-sheet/","title":"Cloud Comparison Cheat Sheet","text":""},{"location":"developertools/cheatsheets/cloud-compare-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive comparison cheat sheet of cloud services across the three major cloud providers: Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP).</p> <p>This guide helps you understand equivalent services across different cloud platforms, making it easier to navigate multi-cloud environments, migrate between providers, or choose the right platform for your needs. Keep in mind that the availability of certain services may vary depending on the region and specific requirements. Also, these cloud platforms are constantly evolving, so new services may be introduced over time.</p>"},{"location":"developertools/cheatsheets/cloud-compare-cheat-sheet/#cloud-services-comparison","title":"Cloud Services Comparison","text":"Category Azure Service AWS Service GCP Service Compute Azure Virtual Machines Amazon EC2 (Elastic Compute Cloud) Google Compute Engine Azure App Service AWS Elastic Beanstalk Google App Engine Azure Functions AWS Lambda Google Cloud Functions Azure Kubernetes Service (AKS) Amazon EKS (Elastic Kubernetes Service) Google Kubernetes Engine (GKE) Storage Azure Blob Storage Amazon S3 (Simple Storage Service) Google Cloud Storage (GCS) Azure Files Amazon EFS (Elastic File System) Google Cloud Filestore Azure Disk Storage Amazon EBS (Elastic Block Store) Google Compute Engine Persistent Disks Azure Data Lake Storage Amazon S3 (with appropriate configurations) Google Cloud Storage (GCS) Azure Queue Storage Amazon SQS (Simple Queue Service) Google Cloud Pub/Sub Database Azure SQL Database Amazon RDS (Relational Database Service) Cloud SQL Azure Cosmos DB Amazon DynamoDB Cloud Firestore / Cloud Bigtable Azure Database for MySQL Amazon RDS (MySQL) Cloud SQL for MySQL Azure Database for PostgreSQL Amazon RDS (PostgreSQL) Cloud SQL for PostgreSQL Azure Cache for Redis Amazon ElastiCache Cloud Memorystore Azure Synapse Analytics Amazon Redshift BigQuery Identity and Access Management Azure Active Directory (Microsoft Entra ID) AWS Directory Service Cloud Identity / Cloud IAM Azure Key Vault AWS Secrets Manager Cloud Key Management Service (KMS) Azure AD B2C Amazon Cognito Identity Platform Networking Azure Virtual Network Amazon VPC Virtual Private Cloud (VPC) Azure Load Balancer Elastic Load Balancing (ELB) Google Cloud Load Balancing Azure Application Gateway AWS Application Load Balancer Google Cloud Load Balancing Azure VPN Gateway AWS VPN Cloud VPN Azure CDN Amazon CloudFront Cloud CDN Azure ExpressRoute AWS Direct Connect Cloud Interconnect Analytics and Big Data Azure HDInsight Amazon EMR (Elastic MapReduce) Cloud Dataproc Azure Databricks Amazon EMR Cloud Dataproc Azure Stream Analytics Amazon Kinesis Cloud Dataflow Azure Data Factory AWS Glue Cloud Data Fusion Azure Analysis Services Amazon Redshift BigQuery Azure Data Lake Analytics Amazon Athena BigQuery Machine Learning Azure Machine Learning Amazon SageMaker AI Platform Azure Cognitive Services Amazon Rekognition / Amazon Polly / Amazon Comprehend Cloud Vision API / Cloud Text-to-Speech API / Cloud Natural Language API Development Tools Azure DevOps AWS CodePipeline / AWS CodeBuild / AWS CodeDeploy Cloud Build / Cloud Deployment Manager Azure DevTest Labs AWS Device Farm Firebase Test Lab Azure SDKs &amp; CLI AWS SDKs &amp; CLI Cloud SDK &amp; CLI Azure Logic Apps AWS Step Functions Cloud Workflows / Cloud Functions Azure API Management Amazon API Gateway Apigee API Platform Azure Functions AWS Lambda Google Cloud Functions Azure Repos AWS CodeCommit Cloud Source Repositories Management and Monitoring Azure Monitor Amazon CloudWatch Stackdriver Monitoring / Operations Azure Security Center AWS Security Hub Cloud Security Command Center Azure Automation AWS Systems Manager Cloud Scheduler Azure Resource Manager AWS CloudFormation Cloud Deployment Manager Azure Policy AWS Organizations Organization Policy Service Azure Advisor AWS Trusted Advisor Recommender Azure Service Health AWS Personal Health Dashboard Cloud Status Dashboard Azure Cost Management AWS Cost Explorer Cloud Billing Azure Backup AWS Backup Cloud Storage Transfer Service Security Azure Security Center AWS Security Hub Cloud Security Command Center Azure Sentinel AWS Security Hub / AWS GuardDuty Chronicle Security / Security Command Center Azure Key Vault AWS Secrets Manager Cloud Key Management Service (KMS) Azure Advanced Threat Protection AWS GuardDuty Cloud Security Scanner"},{"location":"developertools/cheatsheets/cloud-compare-cheat-sheet/#key-differences","title":"Key Differences","text":"<p>Azure - Best for enterprises already invested in Microsoft ecosystem (Windows Server, Active Directory, Office 365). Strong hybrid cloud capabilities with Azure Arc.</p> <p>AWS - Most mature and extensive service catalog. Largest market share and global infrastructure. Best for startups and organizations needing cutting-edge services.</p> <p>GCP - Strong in data analytics, machine learning, and Kubernetes. Leverages Google's expertise in big data and AI. Competitive pricing and innovative services.</p>"},{"location":"developertools/cheatsheets/cloud-compare-cheat-sheet/#references","title":"References","text":"<ul> <li>Azure Services</li> <li>AWS Services</li> <li>Google Cloud Services</li> <li>Microsoft Entra ID (formerly Azure AD)</li> </ul>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/","title":"dig Commands","text":""},{"location":"developertools/cheatsheets/dig-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used dig commands with examples.</p> <p>The <code>dig</code> command is a network administration tool used for querying <code>Domain Name System</code> (DNS) servers. It is commonly used on Unix-like operating systems, including Linux. The name <code>dig</code> stands for <code>domain information groper.</code></p> <p>This command is useful for retrieving various types of DNS information, such as IP addresses associated with domain names, mail exchange records, name server information, and more.</p>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#installing-dig","title":"Installing dig","text":"<p>Use the following commands to install dig in MacOS and Linux environments. The installation process varies depending on the operating system. </p>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#linux-debianubuntu","title":"Linux (Debian/Ubuntu):","text":"<p>On Debian-based systems, you can use the package manager, <code>apt</code>, to install <code>dnsutils</code>, which includes <code>dig</code>:</p> <pre><code>sudo apt update\nsudo apt install dnsutils\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#linux-red-hatcentosfedora","title":"Linux (Red Hat/CentOS/Fedora):","text":"<p>On Red Hat-based systems, you can use the <code>yum</code> or <code>dnf</code> package manager:</p> <pre><code># For CentOS 7 and earlier or RHEL 7 and earlier\nsudo yum install bind-utils\n\n# For CentOS 8, RHEL 8, and Fedora\nsudo dnf install bind-utils\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#macos","title":"macOS:","text":"<p>On macOS, you can use the package manager <code>brew</code> to install <code>dig</code>:</p> <pre><code># Install Homebrew if you haven't already\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install dig\nbrew install bind\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-help","title":"dig help","text":"<p>Displays general help information about dig, including a list of available commands and options.</p> <pre><code>dig -h\n# or\ndig -help\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-version","title":"dig version","text":"<p>Displays version information of dig.</p> <pre><code>dig -v\n# or\ndig version\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#dig-command-syntax","title":"dig Command Syntax","text":"<pre><code>dig [options] [domain]\n</code></pre> <ul> <li><code>[options]</code>: Additional parameters to customize the query.</li> <li><code>[domain]</code>: The domain you want to query.</li> </ul>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-a-domain-name","title":"Using dig for a domain name:","text":"<pre><code>dig anjikeesari.com\n</code></pre> <p>Output:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 26652\n;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  A\n\n;; ANSWER SECTION:\nanjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\nanjikeesari.com.    600 IN  A   185.199.108.153\n\n;; Query time: 137 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:28:58 PST 2023\n;; MSG SIZE  rcvd: 108\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-short-answers","title":"Using dig for Short Answers:","text":"<pre><code>dig +short anjikeesari.com\n</code></pre> <p>Output:</p> <pre><code>185.199.108.153\n185.199.109.153\n185.199.110.153\n185.199.111.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-detailed-answers","title":"Using dig for Detailed Answers:","text":"<pre><code>dig anjikeesari.com +noall +answer\n</code></pre> <p>Output:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com +noall +answer\n;; global options: +cmd\nanjikeesari.com.    600 IN  A   185.199.108.153\nanjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-specifying-name-servers","title":"Using dig for Specifying Name Servers:","text":"<pre><code>dig NS anjikeesari.com\n# or\ndig NS anjikeesari.com +short\n</code></pre> <p>Output:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; NS anjikeesari.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2372\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 5\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  NS\n\n;; ANSWER SECTION:\nanjikeesari.com.    3600    IN  NS  ns72.domaincontrol.com.\nanjikeesari.com.    3600    IN  NS  ns71.domaincontrol.com.\n\n;; ADDITIONAL SECTION:\nns71.domaincontrol.com. 35357   IN  A   97.74.105.46\nns71.domaincontrol.com. 35656   IN  AAAA    2603:5:2194::2e\nns72.domaincontrol.com. 35357   IN  A   173.201.73.46\nns72.domaincontrol.com. 35656   IN  AAAA    2603:5:2294::2e\n\n;; Query time: 40 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:36:48 PST 2023\n;; MSG SIZE  rcvd: 184\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-to-query-all-dns-record-types","title":"Using dig to Query All DNS Record Types:","text":"<pre><code>dig anjikeesari.com ANY\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-to-search-for-record-type","title":"Using dig to Search for Record Type:","text":"<p>Querying TXT records:</p> <pre><code>dig anjikeesari.com -t TXT\n</code></pre> <p>Output:</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; anjikeesari.com -t TXT\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 28965\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;anjikeesari.com.       IN  TXT\n\n;; AUTHORITY SECTION:\nanjikeesari.com.    600 IN  SOA ns71.domaincontrol.com. dns.jomax.net. 2023071000 28800 7200 604800 600\n\n;; Query time: 101 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1)\n;; WHEN: Wed Dec 27 15:34:22 PST 2023\n;; MSG SIZE  rcvd: 112\n</code></pre> <p>Querying A records:</p> <pre><code>dig +nocmd anjikeesari.com a +noall +answer\n</code></pre> <p>Output:</p> <pre><code>anjikeesari.com.    600 IN  A   185.199.109.153\nanjikeesari.com.    600 IN  A   185.199.110.153\nanjikeesari.com.    600 IN  A   185.199.111.153\nanjikeesari.com.    600 IN  A   185.199.108.153\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-to-trace-dns-path","title":"Using dig to Trace DNS Path:","text":"<pre><code>dig +trace anjikeesari.com\n# or\ndig anjikeesari.com +trace\n</code></pre> <p>Querying CNAME records:</p> <pre><code>dig +nocmd mail.google.com cname +noall +answer\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-mail-server-for-the-domain","title":"Using dig for Mail Server for the Domain:","text":"<pre><code>dig MX anjikeesari.com +short\n# or\ndig MX anjikeesari.com\n</code></pre> <p>Querying MX records:</p> <pre><code>dig +nocmd anjikeesari.com ms +noall +answer\n</code></pre>"},{"location":"developertools/cheatsheets/dig-cheat-sheet/#using-dig-for-reverse-dns-lookup","title":"Using dig for Reverse DNS Lookup:","text":"<pre><code>dig -x 185.199.108.153\n# or\ndig -x 185.199.108.153 +short\n</code></pre> <p>Output:</p> <pre><code>cdn-185-199-108-153.github.com.\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/","title":"Docker Commands","text":""},{"location":"developertools/cheatsheets/docker-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used <code>Docker</code> commands.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#installing-docker","title":"Installing Docker","text":"<p>Here are the commands to install Docker on different operating systems:</p> <pre><code># Ubuntu/Debian:\nsudo apt-get update\nsudo apt-get install docker.io\n\n# MacOS (using Homebrew):\nbrew install docker\n\n# Windows OS (using choco)\nchoco install docker-desktop\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-install-verify","title":"Docker Install Verify","text":"<p>To check if Docker is installed:</p> <pre><code>which docker\n\n# output\n/usr/bin/docker\n</code></pre> <p>Check the version installed on your machine:</p> <pre><code>docker --version\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#general-commands","title":"General Commands","text":"<p>Start the Docker daemon:</p> <pre><code>docker -d\n</code></pre> <p>Get help with Docker (can also use --help on all subcommands):</p> <pre><code>docker --help\n</code></pre> <p>Display system-wide information:</p> <pre><code>docker info\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-image","title":"Docker Image","text":"<p>A Docker image is a lightweight, standalone, and executable package that contains everything needed to run a piece of software, including the code, runtime, system tools, libraries, and dependencies..</p> <pre><code># List local images\ndocker images\n\n# Delete an image\ndocker rmi &lt;image_name&gt;\n\n# Remove all unused images\ndocker image prune\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-build","title":"Docker Build","text":"<p>Build an image from a Dockerfile:</p> <pre><code># Build an image from a Dockerfile and tag it with a specified name\ndocker build -t &lt;image_name&gt; .\n\n# Build an image and tag with naming conventions\ndocker build -t projectname/domainname/appname:yyyymmdd.sequence .\n# Example\ndocker build -t sample/aspnet-api:20230226.1 .\n\n# Build an image from a Dockerfile without the cache\ndocker build -t &lt;image_name&gt; . --no-cache\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-run","title":"Docker Run","text":"<pre><code># Create and run a container from an image with a custom name\ndocker run --name &lt;container_name&gt; &lt;image_name&gt;\n\n# Run a container and publish a container's port(s) to the host\ndocker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n\n# Run a container in the background\ndocker run -d &lt;image_name&gt;\n\n# Remove a stopped container\ndocker rm &lt;container_name&gt;\n\n# Example with auto-remove and port mapping\ndocker run --rm -p 8080:80 project1/domain1/app1:20230226.1\n</code></pre> <ul> <li>--rm: Automatically removes the container when it exits. Ensures the container is cleaned up after it finishes running. Useful for temporary or disposable containers.</li> <li>-p 8080:80: Maps the host machine's port 8080 to the container's port 80. Establishes a network connection between the host and container, allowing access to the containerized application via port 8080 on the host.</li> </ul> <p>Exit the container:</p> <pre><code>exit\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-push","title":"Docker Push","text":"<pre><code># Publish an image to Docker Hub\ndocker push &lt;username&gt;/&lt;image_name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-container","title":"Docker container","text":"<p>A Docker container is a lightweight, standalone, and executable runtime instance of a Docker image. It represents a running process that is isolated from the host system and other containers. Docker containers provide a consistent and reproducible environment for running applications. Containers are highly portable and can be easily moved and deployed across different environments, such as development, testing, staging, and production. </p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub is a cloud-based registry service provided by Docker that allows developers to store and share container images. It serves as a centralized repository for Docker images.</p> <pre><code># Login to Docker\ndocker login -u &lt;username&gt;\n\n# Publish an image to Docker Hub\ndocker push &lt;username&gt;/&lt;image_name&gt;\n\n# Search Docker Hub for an image\ndocker search &lt;image_name&gt;\n\n# Pull an image from Docker Hub\ndocker pull &lt;image_name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-network","title":"Docker Network","text":"<p>Create a new bridge network named \"network1\" that containers can connect to for networked communication:</p> <pre><code>docker network create -d bridge network1\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#clean-up-resources","title":"Clean up resources","text":"<p>you can use the <code>docker system prune</code> command to clean up all dangling or unused resources, including images, containers, volumes, and networks that are not tagged or connected to a running container. This command is helpful for freeing up disk space and removing unnecessary resources.</p> <pre><code># before cleaning up Docker, first check all the available resources using the following commands:\n\ndocker container ls\ndocker image ls\ndocker volume ls\ndocker network ls\ndocker info\n\ndocker system prune\n# or\ndocker system prune -a\n</code></pre> <p>If you need to clean up all containers and images locally in Docker Desktop, you can use the following commands:</p> <pre><code># To delete all containers including its volumes use,\ndocker rm -vf $(docker ps -aq)\n\n# To delete all volumes use,\ndocker volume rm $(docker volume ls -q)\n\n# To delete all the images,\ndocker rmi -f $(docker images -aq)\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-compose-commands","title":"Docker Compose Commands","text":"<p>Below are some commonly used Docker Compose commands:</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#starts-services","title":"Starts services","text":"<pre><code>docker-compose up\n</code></pre> <p>Starts the services defined in your <code>docker-compose.yml</code> file. It creates and starts containers as specified in the configuration.</p> <pre><code>docker-compose up -d\n</code></pre> <p>Starts the services in the background (detached mode).</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#stops-services","title":"Stops services","text":"<p><pre><code>docker-compose down\n</code></pre> Stops and removes containers, networks, volumes, and other services defined in your <code>docker-compose.yml</code> file.</p> <p><pre><code>docker-compose down -v\n</code></pre> Stops and removes containers, networks, volumes, and other services while also removing volumes.</p> <p><pre><code>docker-compose down --volumes --rmi all\n</code></pre> Stops and removes containers, networks, volumes, and other services, while also removing volumes and images.    </p> <pre><code>docker-compose stop\n</code></pre> <p>Stops the services defined in your <code>docker-compose.yml</code> file without removing them.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#lists-the-containers","title":"Lists the containers","text":"<pre><code>docker-compose ps\n</code></pre> <p>Lists the containers that are part of your Docker Compose setup, showing their status.</p> <p><pre><code>docker-compose ps -a\n</code></pre> Lists all containers, including stopped ones, that are part of your Docker Compose setup.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#displays-log","title":"Displays log","text":"<p><pre><code>docker-compose logs\n</code></pre> Displays log output from services. You can use the <code>-f</code> option to follow the logs in real-time.</p> <pre><code>docker-compose logs webserver\n</code></pre> <p>Displays logs for a specific service.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#executes-a-command","title":"Executes a command","text":"<p><pre><code>docker-compose exec webserver ls -l\n</code></pre> Executes a command in a running service container.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#builds-services","title":"Builds services","text":"<p><pre><code>docker-compose build\n</code></pre> Builds or rebuilds services defined in your <code>docker-compose.yml</code> file.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#restarts-services","title":"Restarts services","text":"<p><pre><code>docker-compose restart\n</code></pre> Restarts services.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#displays-configuration","title":"Displays configuration","text":"<p><pre><code>docker-compose config\n</code></pre> Validates and displays the configuration of your <code>docker-compose.yml</code> file.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#pauses-services","title":"Pauses services","text":"<p><pre><code>docker-compose pause\n</code></pre> Pauses all services. Containers remain running, but they stop processing requests.</p> <p><pre><code>docker-compose unpause\n</code></pre> Unpauses services after they have been paused.</p> <p><pre><code>docker-compose top\n</code></pre> Displays the running processes of a service.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#scales-service","title":"Scales service","text":"<p><pre><code>docker-compose scale webserver=3\n</code></pre> Scales a service to the specified number of instances.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#display-events","title":"Display events","text":"<pre><code>docker-compose events\n</code></pre> <p>Streams real-time events from your services.</p>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-compose-config","title":"Docker Compose Config","text":"<p>Parse, resolve and render compose file in canonical format:</p> <pre><code>docker-compose config\n</code></pre>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#docker-commands-summary","title":"Docker commands Summary","text":""},{"location":"developertools/cheatsheets/docker-cheat-sheet/#basic-commands","title":"Basic Commands","text":"<ul> <li><code>docker run [image]</code>: Start a new container from an image</li> <li><code>docker ps</code>: List all running containers</li> <li><code>docker stop [container]</code>: Stop a running container</li> <li><code>docker rm [container]</code>: Remove a container</li> <li><code>docker images</code>: List all available images</li> <li><code>docker pull [image]</code>: Download an image from a registry</li> <li><code>docker push [image]</code>: Upload an image to a registry</li> <li><code>docker build [options] [path]</code>: Build an image from a Dockerfile</li> </ul>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#advanced-commands","title":"Advanced Commands","text":"<ul> <li><code>docker exec [container] [command]</code>: Run a command inside a running container</li> <li><code>docker-compose up</code>: Start a Docker Compose application</li> <li><code>docker network [subcommand]</code>: Manage Docker networks</li> <li><code>docker volume [subcommand]</code>: Manage Docker volumes</li> <li><code>docker logs [container]</code>: View the logs of a container</li> <li><code>docker inspect [container]</code>: Inspect a container</li> <li><code>docker diff [container]</code>: Show changes to the filesystem of a container</li> <li><code>docker commit [container] [image]</code>: Create a new image from a container's changes</li> <li><code>docker save [image]</code>: Save an image to a tar archive</li> <li><code>docker load</code>: Load an image from a tar archive</li> </ul>"},{"location":"developertools/cheatsheets/docker-cheat-sheet/#references","title":"References","text":"<ul> <li>Overview of docker compose CLI</li> </ul>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/","title":"Docker Compose Commands","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#introduction","title":"Introduction","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, networks, and volumes. Then, with a single command, you create and start all the services from your configuration. This cheat sheet covers essential Docker Compose commands and configuration patterns for local development and testing environments.</p> <p>Related Cheat Sheets: Docker Commands | Dockerfile Reference | Kubernetes Commands</p>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#installation","title":"Installation","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#windows","title":"Windows","text":"<pre><code># Docker Desktop includes Docker Compose\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Verify installation\ndocker-compose --version\ndocker compose version\n\n# Note: Docker Desktop includes both 'docker-compose' (v1) and 'docker compose' (v2)\n# Prefer 'docker compose' (v2) for new projects\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#macos","title":"macOS","text":"<pre><code># Docker Desktop includes Docker Compose\n# Download from: https://www.docker.com/products/docker-desktop\n\n# Alternatively, install via Homebrew\nbrew install docker-compose\n\n# Verify installation\ndocker-compose --version\ndocker compose version\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#linux","title":"Linux","text":"<pre><code># Install Docker Compose v2 (plugin method - recommended)\nsudo apt-get update\nsudo apt-get install docker-compose-plugin\n\n# Verify installation\ndocker compose version\n\n# Alternative: Install standalone binary\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#basic-docker-composeyml-structure","title":"Basic docker-compose.yml Structure","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#minimal-example","title":"Minimal Example","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#common-docker-composeyml-template","title":"Common docker-compose.yml Template","text":"<pre><code>version: '3.8'\n\nservices:\n  # Web application service\n  web:\n    build:\n      context: ./web\n      dockerfile: Dockerfile\n    container_name: my-web-app\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n    volumes:\n      - ./web:/app\n      - /app/node_modules\n    depends_on:\n      - db\n      - redis\n    networks:\n      - app-network\n    restart: unless-stopped\n\n  # Database service\n  db:\n    image: postgres:16\n    container_name: postgres-db\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n      POSTGRES_DB: mydb\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n    networks:\n      - app-network\n    restart: unless-stopped\n\n  # Cache service\n  redis:\n    image: redis:7-alpine\n    container_name: redis-cache\n    ports:\n      - \"6379:6379\"\n    networks:\n      - app-network\n    restart: unless-stopped\n\nvolumes:\n  postgres-data:\n\nnetworks:\n  app-network:\n    driver: bridge\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#essential-commands","title":"Essential Commands","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#starting-and-stopping-services","title":"Starting and Stopping Services","text":"<pre><code># Start services in foreground\ndocker compose up\n\n# Start services in background (detached mode)\ndocker compose up -d\n\n# Start specific services\ndocker compose up web db\n\n# Build images before starting\ndocker compose up --build\n\n# Force recreate containers\ndocker compose up --force-recreate\n\n# Stop services (preserves containers)\ndocker compose stop\n\n# Stop and remove containers, networks\ndocker compose down\n\n# Stop and remove containers, networks, and volumes\ndocker compose down -v\n\n# Stop and remove containers, networks, volumes, and images\ndocker compose down -v --rmi all\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#building-images","title":"Building Images","text":"<pre><code># Build or rebuild services\ndocker compose build\n\n# Build specific service\ndocker compose build web\n\n# Build without cache\ndocker compose build --no-cache\n\n# Build with parallel execution\ndocker compose build --parallel\n\n# Pull images before building\ndocker compose build --pull\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#viewing-logs","title":"Viewing Logs","text":"<pre><code># View logs from all services\ndocker compose logs\n\n# Follow log output (tail -f)\ndocker compose logs -f\n\n# View logs for specific service\ndocker compose logs web\n\n# Follow logs for specific service\ndocker compose logs -f web\n\n# View last 100 lines\ndocker compose logs --tail=100\n\n# View logs with timestamps\ndocker compose logs -t\n\n# View logs for multiple services\ndocker compose logs web db\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#managing-containers","title":"Managing Containers","text":"<pre><code># List running containers\ndocker compose ps\n\n# List all containers (including stopped)\ndocker compose ps -a\n\n# Execute command in running container\ndocker compose exec web sh\ndocker compose exec web bash\ndocker compose exec db psql -U user -d mydb\n\n# Execute command without allocating TTY\ndocker compose exec -T web npm test\n\n# Run one-off command (creates new container)\ndocker compose run web npm install\ndocker compose run --rm web python manage.py migrate\n\n# Restart services\ndocker compose restart\n\n# Restart specific service\ndocker compose restart web\n\n# Pause services\ndocker compose pause\n\n# Unpause services\ndocker compose unpause\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#inspecting-and-monitoring","title":"Inspecting and Monitoring","text":"<pre><code># View configuration\ndocker compose config\n\n# Validate and view configuration\ndocker compose config --quiet\n\n# View service ports\ndocker compose port web 3000\n\n# View running processes\ndocker compose top\n\n# View resource usage\ndocker compose stats\n\n# View events\ndocker compose events\n\n# View images\ndocker compose images\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#service-configuration","title":"Service Configuration","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#build-configuration","title":"Build Configuration","text":"<pre><code>services:\n  web:\n    build:\n      context: ./app          # Build context directory\n      dockerfile: Dockerfile.dev  # Custom Dockerfile name\n      args:                   # Build arguments\n        - NODE_VERSION=18\n        - APP_ENV=development\n      target: development     # Multi-stage build target\n      cache_from:            # Cache source images\n        - myapp:latest\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#image-and-container-name","title":"Image and Container Name","text":"<pre><code>services:\n  web:\n    image: myapp:latest       # Pull or use this image\n    container_name: my-web-container  # Custom container name\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#ports-mapping","title":"Ports Mapping","text":"<pre><code>services:\n  web:\n    ports:\n      - \"3000:3000\"           # HOST:CONTAINER\n      - \"8080:80\"\n      - \"127.0.0.1:5432:5432\" # Bind to specific interface\n      - \"3000-3005:3000-3005\" # Range mapping\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#environment-variables","title":"Environment Variables","text":"<pre><code>services:\n  web:\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n      - API_KEY=${API_KEY}    # From .env file or shell\n    env_file:\n      - .env                  # Load from .env file\n      - .env.production\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#volumes","title":"Volumes","text":"<pre><code>services:\n  web:\n    volumes:\n      # Named volume\n      - app-data:/app/data\n\n      # Bind mount (host:container)\n      - ./src:/app/src\n      - ./config:/app/config:ro  # Read-only\n\n      # Anonymous volume\n      - /app/node_modules\n\n      # Tmpfs mount (in-memory)\n      - type: tmpfs\n        target: /app/cache\n\nvolumes:\n  app-data:                   # Define named volume\n    driver: local\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#networking","title":"Networking","text":"<pre><code>services:\n  web:\n    networks:\n      - frontend\n      - backend\n    network_mode: \"bridge\"    # or \"host\", \"none\"\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true            # Isolated network\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#dependencies","title":"Dependencies","text":"<pre><code>services:\n  web:\n    depends_on:\n      - db\n      - redis\n\n  # With health check conditions (v2.1+)\n  api:\n    depends_on:\n      db:\n        condition: service_healthy\n      redis:\n        condition: service_started\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#health-checks","title":"Health Checks","text":"<pre><code>services:\n  web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#restart-policy","title":"Restart Policy","text":"<pre><code>services:\n  web:\n    restart: always           # always, on-failure, unless-stopped, no\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#resource-limits","title":"Resource Limits","text":"<pre><code>services:\n  web:\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#common-use-cases","title":"Common Use Cases","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#full-stack-web-application","title":"Full-Stack Web Application","text":"<pre><code>version: '3.8'\n\nservices:\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:3000\"\n    environment:\n      - REACT_APP_API_URL=http://localhost:5000\n    volumes:\n      - ./frontend/src:/app/src\n    depends_on:\n      - backend\n\n  backend:\n    build: ./backend\n    ports:\n      - \"5000:5000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/appdb\n      - REDIS_URL=redis://redis:6379\n    volumes:\n      - ./backend:/app\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:16\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n      POSTGRES_DB: appdb\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n\nvolumes:\n  postgres-data:\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#microservices-architecture","title":"Microservices Architecture","text":"<pre><code>version: '3.8'\n\nservices:\n  api-gateway:\n    build: ./gateway\n    ports:\n      - \"8080:8080\"\n    environment:\n      - AUTH_SERVICE_URL=http://auth-service:3001\n      - USER_SERVICE_URL=http://user-service:3002\n      - ORDER_SERVICE_URL=http://order-service:3003\n    networks:\n      - frontend\n      - backend\n\n  auth-service:\n    build: ./services/auth\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/auth\n    networks:\n      - backend\n\n  user-service:\n    build: ./services/user\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/users\n    networks:\n      - backend\n\n  order-service:\n    build: ./services/order\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/orders\n    networks:\n      - backend\n\n  db:\n    image: postgres:16\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n  backend:\n\nvolumes:\n  postgres-data:\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#testing-environment","title":"Testing Environment","text":"<pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    environment:\n      - NODE_ENV=test\n      - DATABASE_URL=postgresql://test:test@db:5432/testdb\n    depends_on:\n      - db\n    command: npm test\n\n  db:\n    image: postgres:16\n    environment:\n      POSTGRES_USER: test\n      POSTGRES_PASSWORD: test\n      POSTGRES_DB: testdb\n    tmpfs:\n      - /var/lib/postgresql/data  # In-memory database for tests\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#environment-variables_1","title":"Environment Variables","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#using-env-file","title":"Using .env File","text":"<p>Create a <code>.env</code> file in the same directory as <code>docker-compose.yml</code>:</p> <pre><code># Database configuration\nPOSTGRES_USER=myuser\nPOSTGRES_PASSWORD=mypassword\nPOSTGRES_DB=mydb\n\n# Application configuration\nNODE_ENV=development\nAPI_PORT=3000\nAPI_KEY=your-secret-key\n\n# External service URLs\nREDIS_URL=redis://redis:6379\n</code></pre> <p>Reference in <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  web:\n    environment:\n      - NODE_ENV=${NODE_ENV}\n      - API_PORT=${API_PORT}\n      - API_KEY=${API_KEY}\n\n  db:\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=${POSTGRES_DB}\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#multiple-environment-files","title":"Multiple Environment Files","text":"<pre><code>services:\n  web:\n    env_file:\n      - .env.common\n      - .env.development\n      - .env.local\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#networking_1","title":"Networking","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#default-network","title":"Default Network","text":"<pre><code># Services automatically join a default network\nservices:\n  web:\n    image: nginx\n  db:\n    image: postgres\n# Both can communicate using service names: http://web, postgresql://db:5432\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#custom-networks","title":"Custom Networks","text":"<pre><code>services:\n  frontend:\n    networks:\n      - frontend-net\n\n  backend:\n    networks:\n      - frontend-net\n      - backend-net\n\n  db:\n    networks:\n      - backend-net\n\nnetworks:\n  frontend-net:\n    driver: bridge\n  backend-net:\n    driver: bridge\n    internal: true  # No external access\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#external-networks","title":"External Networks","text":"<pre><code>services:\n  web:\n    networks:\n      - existing-network\n\nnetworks:\n  existing-network:\n    external: true\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#volumes_1","title":"Volumes","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#named-volumes","title":"Named Volumes","text":"<pre><code>services:\n  db:\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n\nvolumes:\n  postgres-data:\n    driver: local\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#bind-mounts","title":"Bind Mounts","text":"<pre><code>services:\n  web:\n    volumes:\n      - ./app:/usr/src/app        # Relative path\n      - /opt/data:/data:ro        # Absolute path, read-only\n      - ~/configs:/etc/configs    # Home directory\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#volume-management-commands","title":"Volume Management Commands","text":"<pre><code># List volumes\ndocker compose volume ls\n\n# Remove volumes\ndocker compose down -v\n\n# Create volume manually\ndocker volume create my-volume\n\n# Inspect volume\ndocker volume inspect my-volume\n\n# Remove unused volumes\ndocker volume prune\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#multi-file-compose","title":"Multi-File Compose","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#override-files","title":"Override Files","text":"<pre><code># docker-compose.yml (base)\nversion: '3.8'\nservices:\n  web:\n    image: myapp:latest\n    environment:\n      - NODE_ENV=production\n\n# docker-compose.override.yml (automatically merged)\nversion: '3.8'\nservices:\n  web:\n    environment:\n      - DEBUG=true\n    ports:\n      - \"3000:3000\"\n\n# Use both files (automatic)\ndocker compose up\n\n# Explicitly specify files\ndocker compose -f docker-compose.yml -f docker-compose.dev.yml up\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<pre><code># Production\ndocker compose -f docker-compose.yml -f docker-compose.prod.yml up\n\n# Development\ndocker compose -f docker-compose.yml -f docker-compose.dev.yml up\n\n# Testing\ndocker compose -f docker-compose.yml -f docker-compose.test.yml up\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#view-service-status","title":"View Service Status","text":"<pre><code># Check running services\ndocker compose ps\n\n# Check service logs\ndocker compose logs &lt;service-name&gt;\n\n# Check service health\ndocker compose ps --format json | jq '.[].Health'\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#container-not-starting","title":"Container Not Starting","text":"<pre><code># View detailed logs\ndocker compose logs -f &lt;service-name&gt;\n\n# Check container status\ndocker compose ps -a\n\n# Inspect container\ndocker inspect &lt;container-name&gt;\n\n# Execute command in container\ndocker compose exec &lt;service-name&gt; sh\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Find process using port\n# Windows\nnetstat -ano | findstr :&lt;port&gt;\n\n# Linux/Mac\nlsof -i :&lt;port&gt;\nsudo ss -tulpn | grep :&lt;port&gt;\n\n# Change port in docker-compose.yml\nports:\n  - \"3001:3000\"  # Use different host port\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#network-issues","title":"Network Issues","text":"<pre><code># List networks\ndocker network ls\n\n# Inspect network\ndocker network inspect &lt;network-name&gt;\n\n# Test connectivity between services\ndocker compose exec web ping db\ndocker compose exec web nc -zv db 5432\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#volume-permission-issues","title":"Volume Permission Issues","text":"<pre><code># Run container as specific user\nservices:\n  web:\n    user: \"1000:1000\"  # UID:GID\n    volumes:\n      - ./data:/app/data\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#clear-everything-and-restart","title":"Clear Everything and Restart","text":"<pre><code># Stop and remove everything\ndocker compose down -v\n\n# Remove all containers, images, volumes\ndocker compose down -v --rmi all\n\n# Rebuild and start fresh\ndocker compose build --no-cache\ndocker compose up -d\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#performance-issues","title":"Performance Issues","text":"<pre><code># Check resource usage\ndocker compose stats\n\n# Limit resources\nservices:\n  web:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n          cpus: '0.5'\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#best-practices","title":"Best Practices","text":""},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#development-best-practices","title":"Development Best Practices","text":"<ol> <li>Use .env files for environment-specific configuration</li> <li>Use bind mounts for live code reloading during development</li> <li>Use named volumes for persistent data</li> <li>Set container names for easier debugging</li> <li>Use health checks to ensure services are ready</li> <li>Use depends_on to define service dependencies</li> <li>Keep docker-compose.yml in version control</li> <li>Exclude .env files from version control (use .env.example)</li> </ol>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#production-considerations","title":"Production Considerations","text":"<ol> <li>Don't use bind mounts in production</li> <li>Use specific image tags instead of <code>latest</code></li> <li>Set restart policies to <code>always</code> or <code>unless-stopped</code></li> <li>Use secrets for sensitive data</li> <li>Implement health checks for all services</li> <li>Set resource limits for containers</li> <li>Use multi-stage builds to reduce image size</li> <li>Separate production compose file from development</li> </ol>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Don't commit .env files with secrets</li> <li>Use Docker secrets for sensitive data</li> <li>Run containers as non-root user when possible</li> <li>Use read-only volumes where appropriate</li> <li>Limit network exposure (use internal networks)</li> <li>Keep images updated regularly</li> <li>Scan images for vulnerabilities</li> </ol>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#useful-commands-summary","title":"Useful Commands Summary","text":"<pre><code># Lifecycle\ndocker compose up -d                 # Start services\ndocker compose down                  # Stop and remove services\ndocker compose restart               # Restart services\n\n# Building\ndocker compose build                 # Build images\ndocker compose build --no-cache      # Build without cache\n\n# Logs and monitoring\ndocker compose logs -f               # Follow logs\ndocker compose ps                    # List containers\ndocker compose top                   # Show running processes\ndocker compose stats                 # Show resource usage\n\n# Execution\ndocker compose exec &lt;service&gt; sh     # Execute shell in service\ndocker compose run --rm &lt;service&gt; cmd # Run one-off command\n\n# Cleanup\ndocker compose down -v               # Remove volumes\ndocker compose down --rmi all        # Remove images\ndocker system prune -a --volumes     # Clean everything\n</code></pre>"},{"location":"developertools/cheatsheets/docker-compose-cheat-sheet/#references","title":"References","text":"<ul> <li>Docker Compose Documentation</li> <li>Compose File Reference</li> <li>Docker Commands Cheat Sheet</li> <li>Dockerfile Best Practices</li> <li>Kubernetes vs Docker Compose</li> <li>Docker Compose GitHub Repository</li> </ul> <p>Last Updated: December 30, 2025</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/","title":"Dockerfile Commands","text":""},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used <code>Dockerfile</code> commands</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text document that contains instructions for building a Docker image. Docker can automatically build images by interpreting instructions from a Dockerfile. This page outlines the commands available for use within a Dockerfile.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#1-from","title":"1. FROM","text":"<p>Specifies the base image for your Docker image. <pre><code>FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]\n</code></pre> Example: <pre><code>FROM mcr.microsoft.com/dotnet/sdk:5.0 AS build\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#2-run","title":"2. RUN","text":"<p>Executes commands in the shell of the container. <pre><code>RUN &lt;command&gt;\n</code></pre> Example: <pre><code>RUN apt-get update &amp;&amp; apt-get install -y \\\n    git\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#3-copy","title":"3. COPY","text":"<p>Copies files or directories from the build context to the container's filesystem. <pre><code>COPY &lt;src&gt; &lt;dest&gt;\n</code></pre> Example: <pre><code>COPY . /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#4-workdir","title":"4. WORKDIR","text":"<p>Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it. <pre><code>WORKDIR /path/to/directory\n</code></pre> Example: <pre><code>WORKDIR /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#5-cmd","title":"5. CMD","text":"<p>Specifies the default command to run when the container starts. <pre><code>CMD [\"executable\", \"param1\", \"param2\"]\n</code></pre> Example: <pre><code>CMD [\"dotnet\", \"MyApi.dll\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#6-entrypoint","title":"6. ENTRYPOINT","text":"<p>Specifies the command to run when the container starts, allowing arguments to be passed. <pre><code>ENTRYPOINT [\"executable\", \"param1\", \"param2\"]\n</code></pre> Example: <pre><code>ENTRYPOINT [\"dotnet\", \"MyApi.dll\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#7-expose","title":"7. EXPOSE","text":"<p>Informs Docker that the container listens on specific network ports at runtime. <pre><code>EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]\n</code></pre> Example: <pre><code>EXPOSE 80\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#8-env","title":"8. ENV","text":"<p>Sets environment variables. <pre><code>ENV &lt;key&gt; &lt;value&gt;\n</code></pre> Example: <pre><code>ENV ASPNETCORE_ENVIRONMENT=Production\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#9-arg","title":"9. ARG","text":"<p>Defines build-time variables. <pre><code>ARG &lt;name&gt;[=&lt;default value&gt;]\n</code></pre> Example: <pre><code>ARG CONNECTION_STRING\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#10-volume","title":"10. VOLUME","text":"<p>Creates a mount point and marks it as holding externally mounted volumes from native host or other containers. <pre><code>VOLUME /path/to/volume\n</code></pre> Example: <pre><code>VOLUME /var/log/app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#11-label","title":"11. LABEL","text":"<p>Adds metadata to an image. <pre><code>LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...\n</code></pre> Example: <pre><code>LABEL maintainer=\"John Doe &lt;john@example.com&gt;\"\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#12-user","title":"12. USER","text":"<p>Sets the user or UID to use when running the image. <pre><code>USER &lt;username | UID&gt;\n</code></pre> Example: <pre><code>USER appuser\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#13-healthcheck","title":"13. HEALTHCHECK","text":"<p>Defines a command to periodically check the container's health. <pre><code>HEALTHCHECK [OPTIONS] CMD &lt;command&gt;\n</code></pre> Example: <pre><code>HEALTHCHECK --interval=30s --timeout=3s \\\n  CMD curl -f http://localhost/health || exit 1\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#14-onbuild","title":"14. ONBUILD","text":"<p>Adds a trigger instruction when the image is used as the base for another build. <pre><code>ONBUILD &lt;INSTRUCTION&gt;\n</code></pre> Example: <pre><code>ONBUILD COPY . /app\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#15-stopsignal","title":"15. STOPSIGNAL","text":"<p>Sets the system call signal that will be sent to the container to exit. <pre><code>STOPSIGNAL signal\n</code></pre> Example: <pre><code>STOPSIGNAL SIGTERM\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#16-shell","title":"16. SHELL","text":"<p>Overrides the default shell used for the shell form of commands. <pre><code>SHELL [\"executable\", \"parameters\"]\n</code></pre> Example: <pre><code>SHELL [\"/bin/bash\", \"-c\"]\n</code></pre></p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#cmd-vs-entrypoint","title":"CMD vs ENTRYPOINT","text":"<p>CMD and ENTRYPOINT are used to specify the default command to run when a container is started. However, they have different behaviors and can be used together in different ways depending on the requirements of your Docker image.</p> <ul> <li> <p>CMD:</p> </li> <li> <p>Sets default command and/or parameters.</p> </li> <li>Can be overridden from the command line.</li> <li> <p>Last <code>CMD</code> instruction takes effect if multiple are present.</p> </li> <li> <p>ENTRYPOINT:</p> </li> <li> <p>Specifies main executable to run.</p> </li> <li>Allows arguments to be passed.</li> <li>Arguments passed to <code>docker run</code> are appended to the <code>ENTRYPOINT</code> command.</li> <li>Last <code>ENTRYPOINT</code> instruction takes effect if multiple are present.</li> </ul> <p>Best Practices:</p> <ul> <li>Use CMD for default command and parameters.</li> <li>Use ENTRYPOINT for main executable, allowing additional arguments.</li> </ul> <p>Example:</p> <pre><code>ENTRYPOINT [\"dotnet\", \"MyApi.dll\"]\n</code></pre> <p>In this example, <code>dotnet MyApi.dll</code> is the main executable, with any additional arguments passed when running the container.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#copy-vs-add","title":"COPY vs ADD","text":"<p>COPY and ADD are used to copy files and directories from the host machine into the container's filesystem. While they have similar functionalities, there are some differences between them.</p> <p>COPY Instruction:</p> <p>The COPY instruction copies files or directories from the build context (i.e., the directory containing the Dockerfile) into the container's filesystem. It can copy local files/directories as well as files/directories from URLs. However, it does not support extracting files from compressed archives (e.g., .tar.gz).</p> <p>ADD Instruction:</p> <p>The ADD instruction has the same functionality as COPY, but it also supports additional features such as extracting compressed archives (e.g., .tar.gz) and copying files from URLs. However, because of these additional features, it's considered less predictable and is recommended to use COPY instead unless the extra functionality of ADD is specifically required.</p> Feature COPY ADD Functionality Copies files/directories from build context Same as COPY, plus supports additional features like extracting compressed archives and copying files from URLs Predictability More predictable and straightforward Provides additional functionality but less predictable Best Practice Preferred for basic file copying tasks Use sparingly, only when additional features are needed <p>In summary, <code>COPY</code> is preferred for basic file copying tasks due to its predictability, while <code>ADD</code> offers additional functionality but should be used with caution.</p>"},{"location":"developertools/cheatsheets/dockerfile-cheat-sheet/#references","title":"References","text":"<ul> <li>Dockerfile reference</li> </ul>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/","title":".NET CLI Commands","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#introduction","title":"Introduction","text":"<p>The .NET CLI (Command-Line Interface) is a cross-platform toolchain for developing, building, running, and publishing .NET applications. It provides a consistent experience across Windows, macOS, and Linux, making it essential for modern .NET development. This cheat sheet covers essential .NET CLI commands for project creation, package management, building, testing, publishing, and Entity Framework Core operations.</p> <p>Related Cheat Sheets: Docker Commands</p>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#installation","title":"Installation","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#windows","title":"Windows","text":"<pre><code># Install via winget\nwinget install Microsoft.DotNet.SDK.8\n\n# Install via Chocolatey\nchoco install dotnet-sdk\n\n# Download installer from: https://dotnet.microsoft.com/download\n\n# Verify installation\ndotnet --version\ndotnet --info\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#macos","title":"macOS","text":"<pre><code># Install via Homebrew\nbrew install --cask dotnet-sdk\n\n# Download installer from: https://dotnet.microsoft.com/download\n\n# Verify installation\ndotnet --version\ndotnet --info\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#linux","title":"Linux","text":"<pre><code># Ubuntu/Debian\nwget https://packages.microsoft.com/config/ubuntu/22.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\nsudo apt-get update\nsudo apt-get install -y dotnet-sdk-8.0\n\n# Fedora/CentOS/RHEL\nsudo dnf install dotnet-sdk-8.0\n\n# Verify installation\ndotnet --version\ndotnet --info\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#getting-started","title":"Getting Started","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#version-information","title":"Version Information","text":"<pre><code># Display .NET SDK version\ndotnet --version\n\n# Display detailed .NET information\ndotnet --info\n\n# List installed SDKs\ndotnet --list-sdks\n\n# List installed runtimes\ndotnet --list-runtimes\n\n# Check for updates\ndotnet sdk check\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#help-and-documentation","title":"Help and Documentation","text":"<pre><code># General help\ndotnet --help\ndotnet -h\n\n# Command-specific help\ndotnet new --help\ndotnet build --help\ndotnet run --help\n\n# List available project templates\ndotnet new list\ndotnet new --list\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#project-management","title":"Project Management","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#creating-projects","title":"Creating Projects","text":"<pre><code># Create console application\ndotnet new console -n MyConsoleApp\ndotnet new console -o MyConsoleApp\n\n# Create web API\ndotnet new webapi -n MyWebApi\ndotnet new webapi -o MyWebApi --framework net8.0\n\n# Create ASP.NET Core web app (MVC)\ndotnet new mvc -n MyWebApp\ndotnet new mvc -o MyWebApp\n\n# Create Blazor Server app\ndotnet new blazorserver -n MyBlazorApp\n\n# Create Blazor WebAssembly app\ndotnet new blazorwasm -n MyBlazorWasmApp\n\n# Create class library\ndotnet new classlib -n MyLibrary\ndotnet new classlib -o MyLibrary --framework netstandard2.0\n\n# Create xUnit test project\ndotnet new xunit -n MyTests\ndotnet new xunit -o MyApp.Tests\n\n# Create NUnit test project\ndotnet new nunit -n MyTests\n\n# Create MSTest test project\ndotnet new mstest -n MyTests\n\n# Create Razor Pages app\ndotnet new razor -n MyRazorApp\n\n# Create worker service\ndotnet new worker -n MyWorkerService\n\n# Create solution file\ndotnet new sln -n MySolution\n\n# Create with specific framework\ndotnet new console -n MyApp --framework net8.0\ndotnet new console -n MyApp --framework net6.0\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#solution-management","title":"Solution Management","text":"<pre><code># Create new solution\ndotnet new sln -n MySolution\n\n# Add project to solution\ndotnet sln add MyProject/MyProject.csproj\ndotnet sln add MyProject1/MyProject1.csproj MyProject2/MyProject2.csproj\n\n# List projects in solution\ndotnet sln list\n\n# Remove project from solution\ndotnet sln remove MyProject/MyProject.csproj\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#project-references","title":"Project References","text":"<pre><code># Add project reference\ndotnet add reference ../MyLibrary/MyLibrary.csproj\ndotnet add MyProject/MyProject.csproj reference MyLibrary/MyLibrary.csproj\n\n# Remove project reference\ndotnet remove reference ../MyLibrary/MyLibrary.csproj\n\n# List project references\ndotnet list reference\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#package-management","title":"Package Management","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#adding-packages","title":"Adding Packages","text":"<pre><code># Add NuGet package\ndotnet add package Newtonsoft.Json\ndotnet add package Microsoft.EntityFrameworkCore\n\n# Add specific version\ndotnet add package Newtonsoft.Json --version 13.0.3\n\n# Add with prerelease versions\ndotnet add package Microsoft.AspNetCore.Components --prerelease\n\n# Add package to specific project\ndotnet add MyProject/MyProject.csproj package Serilog\n\n# Add package from specific source\ndotnet add package MyPackage --source https://api.nuget.org/v3/index.json\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#removing-packages","title":"Removing Packages","text":"<pre><code># Remove package\ndotnet remove package Newtonsoft.Json\ndotnet remove package Microsoft.EntityFrameworkCore\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#listing-packages","title":"Listing Packages","text":"<pre><code># List packages in project\ndotnet list package\n\n# List outdated packages\ndotnet list package --outdated\n\n# List vulnerable packages\ndotnet list package --vulnerable\n\n# List deprecated packages\ndotnet list package --deprecated\n\n# Include transitive packages\ndotnet list package --include-transitive\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#restoring-packages","title":"Restoring Packages","text":"<pre><code># Restore all dependencies\ndotnet restore\n\n# Restore for specific project\ndotnet restore MyProject/MyProject.csproj\n\n# Restore with specific source\ndotnet restore --source https://api.nuget.org/v3/index.json\n\n# Force restore (ignore cache)\ndotnet restore --force\n\n# Restore with no cache\ndotnet restore --no-cache\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#building","title":"Building","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#build-commands","title":"Build Commands","text":"<pre><code># Build project\ndotnet build\n\n# Build with Release configuration\ndotnet build --configuration Release\ndotnet build -c Release\n\n# Build without restoring\ndotnet build --no-restore\n\n# Build specific project\ndotnet build MyProject/MyProject.csproj\n\n# Build solution\ndotnet build MySolution.sln\n\n# Build with verbose output\ndotnet build --verbosity detailed\ndotnet build -v d\n\n# Build for specific runtime\ndotnet build --runtime win-x64\ndotnet build --runtime linux-x64\ndotnet build --runtime osx-arm64\n\n# Build with specific framework\ndotnet build --framework net8.0\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#clean","title":"Clean","text":"<pre><code># Clean build outputs\ndotnet clean\n\n# Clean with configuration\ndotnet clean --configuration Release\n\n# Clean specific project\ndotnet clean MyProject/MyProject.csproj\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#running-applications","title":"Running Applications","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#run-commands","title":"Run Commands","text":"<pre><code># Run project\ndotnet run\n\n# Run with specific configuration\ndotnet run --configuration Release\ndotnet run -c Release\n\n# Run with arguments\ndotnet run -- arg1 arg2\ndotnet run -- --environment Production\n\n# Run specific project\ndotnet run --project MyProject/MyProject.csproj\n\n# Run without building\ndotnet run --no-build\n\n# Run without restoring\ndotnet run --no-restore\n\n# Run with specific framework\ndotnet run --framework net8.0\n\n# Run with environment variable\ndotnet run --environment Production\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#watch-mode","title":"Watch Mode","text":"<pre><code># Run with hot reload (watch mode)\ndotnet watch run\n\n# Watch and run tests\ndotnet watch test\n\n# Watch with specific project\ndotnet watch --project MyProject/MyProject.csproj run\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#testing","title":"Testing","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\ndotnet test\n\n# Run tests with verbose output\ndotnet test --verbosity normal\ndotnet test -v n\n\n# Run tests without building\ndotnet test --no-build\n\n# Run tests for specific project\ndotnet test MyTests/MyTests.csproj\n\n# Run tests with code coverage\ndotnet test --collect:\"XPlat Code Coverage\"\n\n# Run tests and generate results file\ndotnet test --logger \"trx;LogFileName=test-results.trx\"\n\n# Run specific test\ndotnet test --filter FullyQualifiedName=MyNamespace.MyClass.MyTest\n\n# Run tests matching pattern\ndotnet test --filter Name~Integration\ndotnet test --filter Category=Unit\n\n# Run tests in parallel\ndotnet test --parallel\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#test-filtering","title":"Test Filtering","text":"<pre><code># Filter by test name\ndotnet test --filter \"FullyQualifiedName~MyTest\"\n\n# Filter by category/trait\ndotnet test --filter \"Category=Integration\"\ndotnet test --filter \"Priority=1\"\n\n# Filter by namespace\ndotnet test --filter \"FullyQualifiedName~MyNamespace\"\n\n# Multiple filters\ndotnet test --filter \"(Category=Unit)|(Category=Integration)\"\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#publishing","title":"Publishing","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#publish-commands","title":"Publish Commands","text":"<pre><code># Publish project\ndotnet publish\n\n# Publish with Release configuration\ndotnet publish --configuration Release\ndotnet publish -c Release\n\n# Publish to specific output directory\ndotnet publish --output ./publish\ndotnet publish -o ./publish\n\n# Publish for specific runtime (self-contained)\ndotnet publish --runtime win-x64 --self-contained\ndotnet publish --runtime linux-x64 --self-contained\ndotnet publish --runtime osx-arm64 --self-contained\n\n# Publish framework-dependent\ndotnet publish --runtime win-x64 --self-contained false\ndotnet publish --runtime win-x64 --no-self-contained\n\n# Publish single file\ndotnet publish --runtime win-x64 --self-contained -p:PublishSingleFile=true\n\n# Publish with trimming\ndotnet publish --runtime win-x64 --self-contained -p:PublishTrimmed=true\n\n# Publish ReadyToRun\ndotnet publish --runtime win-x64 --self-contained -p:PublishReadyToRun=true\n\n# Publish to specific framework\ndotnet publish --framework net8.0\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#publishing-for-docker","title":"Publishing for Docker","text":"<pre><code># Publish for Linux container\ndotnet publish --configuration Release --runtime linux-x64 --self-contained false --output ./publish\n\n# Publish for Alpine Linux\ndotnet publish --configuration Release --runtime linux-musl-x64 --self-contained false --output ./publish\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#entity-framework-core","title":"Entity Framework Core","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#installation_1","title":"Installation","text":"<pre><code># Install EF Core tools globally\ndotnet tool install --global dotnet-ef\n\n# Update EF Core tools\ndotnet tool update --global dotnet-ef\n\n# Verify installation\ndotnet ef --version\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#migrations","title":"Migrations","text":"<pre><code># Add migration\ndotnet ef migrations add InitialCreate\ndotnet ef migrations add AddUserTable\n\n# Add migration with specific context\ndotnet ef migrations add InitialCreate --context MyDbContext\n\n# Add migration for specific project\ndotnet ef migrations add InitialCreate --project MyProject\n\n# List migrations\ndotnet ef migrations list\n\n# Remove last migration\ndotnet ef migrations remove\n\n# Generate SQL script for migration\ndotnet ef migrations script\ndotnet ef migrations script --output migration.sql\n\n# Generate SQL for specific migration range\ndotnet ef migrations script InitialCreate AddUserTable\ndotnet ef migrations script 0 InitialCreate\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#database-operations","title":"Database Operations","text":"<pre><code># Update database to latest migration\ndotnet ef database update\n\n# Update to specific migration\ndotnet ef database update InitialCreate\n\n# Revert all migrations\ndotnet ef database update 0\n\n# Drop database\ndotnet ef database drop\ndotnet ef database drop --force\n\n# Get database connection string\ndotnet ef dbcontext info\n\n# List DbContext types\ndotnet ef dbcontext list\n\n# Generate DbContext from existing database (scaffold)\ndotnet ef dbcontext scaffold \"Server=localhost;Database=MyDb;User=sa;Password=Pass;\" Microsoft.EntityFrameworkCore.SqlServer\n\n# Scaffold with specific tables\ndotnet ef dbcontext scaffold \"ConnectionString\" Microsoft.EntityFrameworkCore.SqlServer --table Users --table Orders\n\n# Scaffold to specific output directory\ndotnet ef dbcontext scaffold \"ConnectionString\" Microsoft.EntityFrameworkCore.SqlServer --output-dir Models --context-dir Data\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#ef-core-with-projects","title":"EF Core with Projects","text":"<pre><code># Specify startup project and project with DbContext\ndotnet ef migrations add InitialCreate --startup-project MyApi --project MyData\n\n# Update database with specific projects\ndotnet ef database update --startup-project MyApi --project MyData\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#global-tools","title":"Global Tools","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#installing-tools","title":"Installing Tools","text":"<pre><code># Install tool globally\ndotnet tool install --global dotnet-ef\ndotnet tool install --global dotnet-aspnet-codegenerator\n\n# Install tool locally (project-specific)\ndotnet new tool-manifest\ndotnet tool install dotnet-ef\n\n# Install specific version\ndotnet tool install --global dotnet-ef --version 8.0.0\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#managing-tools","title":"Managing Tools","text":"<pre><code># List installed global tools\ndotnet tool list --global\n\n# List local tools\ndotnet tool list\n\n# Update tool\ndotnet tool update --global dotnet-ef\n\n# Uninstall tool\ndotnet tool uninstall --global dotnet-ef\n\n# Restore local tools\ndotnet tool restore\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#nuget-package-creation","title":"NuGet Package Creation","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#pack-and-push","title":"Pack and Push","text":"<pre><code># Create NuGet package\ndotnet pack\n\n# Pack with specific configuration\ndotnet pack --configuration Release\n\n# Pack with version\ndotnet pack --configuration Release /p:Version=1.0.0\n\n# Pack to specific output directory\ndotnet pack --output ./packages\n\n# Push package to NuGet.org\ndotnet nuget push MyPackage.1.0.0.nupkg --api-key YOUR_API_KEY --source https://api.nuget.org/v3/index.json\n\n# Push to private feed\ndotnet nuget push MyPackage.1.0.0.nupkg --source https://pkgs.dev.azure.com/myorg/_packaging/myfeed/nuget/v3/index.json\n\n# Add NuGet source\ndotnet nuget add source https://api.nuget.org/v3/index.json --name nuget.org\n\n# List NuGet sources\ndotnet nuget list source\n\n# Remove NuGet source\ndotnet nuget remove source mySource\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#code-formatting","title":"Code Formatting","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#format-commands","title":"Format Commands","text":"<pre><code># Format code\ndotnet format\n\n# Format and verify only\ndotnet format --verify-no-changes\n\n# Format specific files/folders\ndotnet format --include Program.cs\n\n# Format with specific severity\ndotnet format --severity info\ndotnet format --severity warn\ndotnet format --severity error\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#common-workflows","title":"Common Workflows","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#creating-web-api-project","title":"Creating Web API Project","text":"<pre><code># Create solution and projects\ndotnet new sln -n MyApiSolution\ndotnet new webapi -n MyApi\ndotnet new xunit -n MyApi.Tests\ndotnet new classlib -n MyApi.Data\n\n# Add projects to solution\ndotnet sln add MyApi/MyApi.csproj\ndotnet sln add MyApi.Tests/MyApi.Tests.csproj\ndotnet sln add MyApi.Data/MyApi.Data.csproj\n\n# Add project references\ncd MyApi\ndotnet add reference ../MyApi.Data/MyApi.Data.csproj\ncd ../MyApi.Tests\ndotnet add reference ../MyApi/MyApi.csproj\n\n# Add required packages\ncd ../MyApi\ndotnet add package Microsoft.EntityFrameworkCore.SqlServer\ndotnet add package Serilog.AspNetCore\ndotnet add package Swashbuckle.AspNetCore\n\n# Build and run\ncd ..\ndotnet build\ndotnet run --project MyApi\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#entity-framework-setup","title":"Entity Framework Setup","text":"<pre><code># Install EF Core tools\ndotnet tool install --global dotnet-ef\n\n# Add EF Core packages\ndotnet add package Microsoft.EntityFrameworkCore.SqlServer\ndotnet add package Microsoft.EntityFrameworkCore.Design\ndotnet add package Microsoft.EntityFrameworkCore.Tools\n\n# Create initial migration\ndotnet ef migrations add InitialCreate\n\n# Update database\ndotnet ef database update\n\n# Scaffold existing database\ndotnet ef dbcontext scaffold \"Server=localhost;Database=MyDb;Trusted_Connection=True;\" Microsoft.EntityFrameworkCore.SqlServer --output-dir Models\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#docker-publishing","title":"Docker Publishing","text":"<pre><code># Publish for Docker container\ndotnet publish --configuration Release --runtime linux-x64 --self-contained false --output ./publish\n\n# Create Dockerfile\n# FROM mcr.microsoft.com/dotnet/aspnet:8.0\n# WORKDIR /app\n# COPY ./publish .\n# ENTRYPOINT [\"dotnet\", \"MyApi.dll\"]\n\n# Build Docker image\ndocker build -t myapi:latest .\n\n# Run container\ndocker run -p 8080:8080 myapi:latest\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#cicd-build-script","title":"CI/CD Build Script","text":"<pre><code># Restore, build, test, publish\ndotnet restore\ndotnet build --configuration Release --no-restore\ndotnet test --configuration Release --no-build --verbosity normal\ndotnet publish --configuration Release --output ./publish --no-build\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#configuration-and-environment","title":"Configuration and Environment","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#user-secrets","title":"User Secrets","text":"<pre><code># Initialize user secrets\ndotnet user-secrets init\n\n# Set secret\ndotnet user-secrets set \"ConnectionStrings:DefaultConnection\" \"Server=localhost;Database=MyDb;\"\ndotnet user-secrets set \"ApiKey\" \"my-secret-key\"\n\n# List all secrets\ndotnet user-secrets list\n\n# Remove secret\ndotnet user-secrets remove \"ApiKey\"\n\n# Clear all secrets\ndotnet user-secrets clear\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#environment-variables","title":"Environment Variables","text":"<pre><code># Run with environment variable (Windows)\n$env:ASPNETCORE_ENVIRONMENT=\"Production\"; dotnet run\n\n# Run with environment variable (Linux/Mac)\nASPNETCORE_ENVIRONMENT=Production dotnet run\n\n# Set for current session (Windows)\n$env:ASPNETCORE_ENVIRONMENT=\"Development\"\n\n# Set for current session (Linux/Mac)\nexport ASPNETCORE_ENVIRONMENT=Development\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#build-issues","title":"Build Issues","text":"<pre><code># Clean and rebuild\ndotnet clean\ndotnet restore\ndotnet build\n\n# Force package restore\ndotnet restore --force --no-cache\n\n# Build with diagnostic output\ndotnet build --verbosity diagnostic\n\n# Check for SDK issues\ndotnet --info\ndotnet --list-sdks\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#runtime-issues","title":"Runtime Issues","text":"<pre><code># List installed runtimes\ndotnet --list-runtimes\n\n# Check framework compatibility\ndotnet --info\n\n# Run with detailed logging\ndotnet run --verbosity detailed\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#package-issues","title":"Package Issues","text":"<pre><code># Clear NuGet cache\ndotnet nuget locals all --clear\n\n# List package sources\ndotnet nuget list source\n\n# Restore with specific source\ndotnet restore --source https://api.nuget.org/v3/index.json\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#ef-core-issues","title":"EF Core Issues","text":"<pre><code># Verify EF Core tools installation\ndotnet ef --version\n\n# Check DbContext configuration\ndotnet ef dbcontext info\n\n# Generate migration SQL without applying\ndotnet ef migrations script --output migration.sql\n\n# Drop and recreate database\ndotnet ef database drop --force\ndotnet ef database update\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Check what's using port (Windows)\nnetstat -ano | findstr :5000\n\n# Kill process (Windows)\ntaskkill /PID &lt;process_id&gt; /F\n\n# Run on different port\ndotnet run --urls \"http://localhost:5001\"\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#best-practices","title":"Best Practices","text":""},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#project-structure","title":"Project Structure","text":"<ol> <li>Use solution files for multi-project applications</li> <li>Separate concerns with class libraries (Data, Business, API)</li> <li>Create test projects for each main project</li> <li>Use project references instead of binary references</li> <li>Keep .csproj files clean - use Directory.Build.props for shared settings</li> </ol>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#package-management_1","title":"Package Management","text":"<ol> <li>Pin package versions in production</li> <li>Regularly update packages with <code>dotnet list package --outdated</code></li> <li>Check for vulnerabilities with <code>dotnet list package --vulnerable</code></li> <li>Use central package management for multi-project solutions</li> <li>Audit dependencies before production deployment</li> </ol>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#building-and-publishing","title":"Building and Publishing","text":"<ol> <li>Use Release configuration for production builds</li> <li>Enable ReadyToRun for faster startup: <code>-p:PublishReadyToRun=true</code></li> <li>Trim unused code for smaller deployments: <code>-p:PublishTrimmed=true</code></li> <li>Self-contained for environments without .NET runtime</li> <li>Framework-dependent for environments with .NET runtime (smaller size)</li> </ol>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#entity-framework","title":"Entity Framework","text":"<ol> <li>Always create migrations for schema changes</li> <li>Generate SQL scripts for production deployments</li> <li>Use separate migration projects for large solutions</li> <li>Test migrations in staging before production</li> <li>Keep migrations in source control</li> </ol>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#development-workflow","title":"Development Workflow","text":"<ol> <li>Use <code>dotnet watch</code> for hot reload during development</li> <li>Run tests frequently with <code>dotnet test</code></li> <li>Format code regularly with <code>dotnet format</code></li> <li>Use user secrets for local development sensitive data</li> <li>Enable detailed errors in development environment</li> </ol>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#useful-commands-summary","title":"Useful Commands Summary","text":"<pre><code># Project lifecycle\ndotnet new webapi -n MyApi             # Create project\ndotnet restore                         # Restore packages\ndotnet build                           # Build project\ndotnet run                             # Run application\ndotnet test                            # Run tests\ndotnet publish -c Release              # Publish for deployment\n\n# Package management\ndotnet add package PackageName         # Add package\ndotnet list package --outdated         # Check for updates\ndotnet remove package PackageName      # Remove package\n\n# Entity Framework\ndotnet ef migrations add Name          # Create migration\ndotnet ef database update              # Apply migrations\ndotnet ef migrations list              # List migrations\n\n# Tools\ndotnet tool install --global dotnet-ef # Install global tool\ndotnet tool list --global              # List installed tools\n\n# Cleaning and maintenance\ndotnet clean                           # Clean build outputs\ndotnet nuget locals all --clear        # Clear NuGet cache\n</code></pre>"},{"location":"developertools/cheatsheets/dotnet-cheat-sheet/#references","title":"References","text":"<ul> <li>.NET CLI Documentation</li> <li>.NET SDK Download</li> <li>Entity Framework Core</li> <li>NuGet Package Manager</li> <li>Docker Commands</li> <li>.NET API Reference</li> </ul> <p>Last Updated: December 30, 2025</p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/","title":"Git Commands","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Git commands with examples. </p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#installing-git","title":"Installing git","text":"<p>Here are the commands to install Git on different operating systems:</p> <pre><code># Ubuntu/Debian:\nsudo apt-get install git\n\n# MacOS (using Homebrew):\nbrew install git\n\n# Windows OS (using choco)\nchoco install git\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#setting-up-git-configuration","title":"Setting up git configuration:","text":"<p>To begin, it's important to configure your Git settings, associating your name and email with your commits. Use the following commands to set your name and email respectively:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#caching-credentials","title":"Caching credentials:","text":"<p>Typing in login credentials repeatedly can be time consuming. To streamline this process, you can store your credentials in the cache using the command:</p> <pre><code>git config --global credential.helper cache\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#enable-automatic-coloring-of-git-output","title":"Enable automatic coloring of Git output","text":"<p>This command is used to enable automatic coloring of Git output in the command line interface. Enabling this option enhances the readability of Git's output by applying different colors to various elements.</p> <pre><code>git config --global color.ui auto\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#checking-git-configuration","title":"Checking git configuration:","text":"<p>To verify your Git configuration, including your username and email, use the following command:</p> <pre><code>git config -l\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#initializing-git","title":"Initializing git","text":"<p>Before diving into Git commands, you need to initialize a new Git repository locally in your project's root directory. Execute the command:</p> <pre><code>git init\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-clone","title":"Git clone","text":"<p>To work on an existing Git repository, you can clone it using the command</p> <pre><code>git clone &lt;repository-url&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#adding-files-to-the-staging-area","title":"Adding files to the staging area:","text":"<p>To stage changes and prepare them for commit, use the git add command. You can add specific files or entire directories to the staging area using the following commands:</p> <pre><code>git add &lt;file-name&gt;             # Add a specific file\ngit add .                       # Add all changes in the current directory (excluding deletions)\ngit add test*                   # Add all files starting with 'test' in the current directory\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#committing-changes","title":"Committing changes:","text":"<p>Committing changes captures a snapshot of your code at a specific point in time. Use the following commands to commit your changes:</p> <pre><code>git commit -m \"(message)\"       # Commits the changes with a custom message\ngit commit -am \"(message)\"      # Adds all changes to staging and commits them with a custom message\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-log","title":"Git log","text":"<p>To view the commit history of a repository, use the <code>git log</code> command. It provides you with an overview of past commits and their respective details. also, you can use <code>git log -p</code> to see the commit history along with the changes made to each file.</p> <pre><code># shows the commit history for the current repository:\ngit log\n# commits' history including all files and their changes:\ngit log -p\n\npress q any time to quit\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#commit-details","title":"Commit details","text":"<p>Use this command to see a specific commit in details</p> <p><pre><code>git show commit-id\n</code></pre> Note: replace <code>commit-id</code> with the id of the commit that you can find in the git log</p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-status","title":"Git status","text":"<p>This command will show the status of the current repository including staged, unstaged, and untracked files.</p> <pre><code>git status\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#undoing-changes","title":"Undoing changes:","text":"<p>If you have already pushed a commit to a remote repository and want to undo it, you need to create a new commit that undoes the changes. The following command will create a new commit that undoes the changes introduced by the specified commit:</p> <p><pre><code>git revert &lt;commit-id&gt;\n</code></pre> Replace  with the ID of the commit you want to undo. <p>If you have already committed changes and want to undo the most recent commit, you have a few options depending on your desired outcome:</p> <ul> <li>Undo the commit and keep the changes as unstaged modifications:</li> </ul> <pre><code>git reset HEAD^\n</code></pre> <ul> <li>Undo the commit and completely discard the changes:</li> </ul> <pre><code>git reset --hard HEAD^\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#viewing-differences","title":"Viewing differences","text":"<p>To compare the differences between versions, you can use the git diff command. It displays the changes made to files since the last commit. </p> <pre><code>git diff\n</code></pre> <p>This will show the line-by-line differences between the current state of the files and the last committed version.</p>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#pushing-changes","title":"Pushing changes","text":"<p>To push your local commits to a remote repository, you need to use following command.</p> <pre><code>git push origin &lt;branch-name&gt;\n\n# if you haven't set the upstream branch yet, you can use this\n\ngit push --set-upstream origin aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#pulling-changes","title":"Pulling changes","text":"<p>Use this command to incorporate the latest changes from a remote repository into your local repository.</p> <pre><code>git pull origin &lt;branch-name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-fetch","title":"Git fetch","text":"<p>To fetch the latest changes from the remote repository without merging them into your local branches.</p> <pre><code>git fetch\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#creating-a-new-branch","title":"Creating a new branch:","text":"<p>To create a new branch in Git, you can use the git branch command followed by the name of the branch you want to create. </p> <pre><code>git branch &lt;branch-name&gt;\n\n# Creates a new branch, `aspnet-api` is name of the branch here\ngit branch aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#switching-branch","title":"Switching branch:","text":"<p>To switch to a different branch in your Git repository, you can utilize the <code>git checkout</code> command followed by the name of the branch you want to switch to. </p> <pre><code>git checkout &lt;branch-name&gt;\n\n# Switched to branch 'aspnet-api'\ngit checkout aspnet-api\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#git-switch-modern-alternative","title":"Git switch (Modern alternative)","text":"<p>Starting from Git 2.23, you can use the <code>git switch</code> command as a clearer alternative to <code>git checkout</code> for switching branches.</p> <pre><code>git switch &lt;branch-name&gt;\n\n# Switch to branch 'aspnet-api'\ngit switch aspnet-api\n\n# Create and switch to a new branch\ngit switch -c &lt;new-branch-name&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#list-branches","title":"List branches","text":"<p>It will show a list of all branches and mark the current branch with an asterisk and highlight it in green.</p> <pre><code># Shows the list of all branches.\ngit branch  \n# List all local branches in repository. With -a: show all branches (with remote).\ngit branch -a \n\n# press q to quit\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#get-remote-urls","title":"Get remote URLs","text":"<p>You can see all remote repositories for your local repository with this command:</p> <pre><code>git remote -v\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#more-info-about-a-remote-repo","title":"More info about a remote repo","text":"<p>How to get more info about a remote repo in Git:</p> <pre><code>git remote show origin\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#add-a-remote-repository","title":"Add a remote repository","text":"<p>To add a new remote repository to your local Git repository:</p> <pre><code>git remote add &lt;remote-name&gt; &lt;remote-url&gt;\n\n# Example: Add origin remote\ngit remote add origin https://github.com/username/repository.git\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#remove-a-remote-repository","title":"Remove a remote repository","text":"<p>To remove a remote repository from your local Git repository:</p> <pre><code>git remote remove &lt;remote-name&gt;\n\n# Example: Remove origin remote\ngit remote remove origin\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#merging-branches","title":"Merging branches","text":"<p>In Git, merging allows you to combine the changes from one branch into another. To merge a branch into another branch, you can use the git merge command followed by the name of the branch you want to merge. Here's an example:</p> <pre><code>git merge &lt;branch-name&gt;\n\n# For instance, if you want to merge the changes from the develop branch into the main branch\n# cd to the folder\ngit checkout main\ngit merge develop\n</code></pre> <p>After performing the merge, it's a good practice to check the status of your repository using git status to ensure that the merge was successful and there are no conflicts to resolve. also, you can view the commit history using git log to see the merged commits and their details.</p> <pre><code>git status\ngit log\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#delete-branch","title":"Delete branch","text":"<p>To delete a branch in Git, you can use either of the following commands:</p> <pre><code>git branch --delete &lt;branch-name&gt;\ngit branch -d &lt;branch-name&gt;\n\nexample\n# git branch to see list of branches before delete\ngit branch\n# delete the branch\ngit branch --delete &lt;branch-name&gt;\n# git branch again to see list of branches after delete\ngit branch\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#branch-from-a-previous-commit","title":"Branch from a previous commit","text":"<p>To create a new branch in Git using a specific commit hash, you can use the git branch command followed by the name of the branch and the commit hash</p> <pre><code>git branch &lt;branch-name&gt; &lt;commit-hash&gt;\n# Step 1: Create the branch from the commit hash\n\ngit branch new_branch 07615d50afde24d21e2180b90d3a0a58ec131980\n\n# this will create the local branch\n\n# Step 2: Switch to the new branch &amp; commit\n\ngit commit -am \"(message)\" \n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#rollback-an-old-commit","title":"Rollback an old commit","text":"<p>You can revert an old commit using its commit id. </p> <pre><code>git revert &lt;commit_id&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#how-to-resolve-merge-conflicts-using-git-commands","title":"How to resolve merge conflicts using git commands","text":"<p>Resolving merge conflicts in Git involves editing the conflicted files to choose which changes to keep and which to discard, and then committing the resolved changes. Here's a step-by-step guide:</p> <ol> <li> <p>Check the status of your repository to see if there are any merge conflicts:</p> <p><pre><code>git status\n</code></pre> If there are merge conflicts, you will see a message indicating which files have conflicts.</p> </li> <li> <p>Open the conflicted files in a text editor and look for the conflict markers. The markers will look something like this:</p> <p><pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the content from the current branch.\n=======\nThis is the content from the branch you are merging.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch-name&gt;\n</code></pre> 1. Decide what changes you want to keep and remove the conflict markers and any unnecessary content. The final content should only include the changes you want to keep.</p> </li> <li> <p>Stage the changes using git add:     <pre><code>git add &lt;file-name&gt;\n</code></pre></p> </li> <li>Commit the changes to the repository:     <pre><code>git commit -m \"Resolved merge conflicts\"\n</code></pre></li> <li>Push the changes to the remote repository if necessary:     <pre><code>git push origin &lt;branch-name&gt;\n</code></pre></li> </ol>"},{"location":"developertools/cheatsheets/git-cheat-sheet/#temporary-commits","title":"Temporary commits","text":"<p>In Git, you can use temporary commits to store modified, tracked files temporarily, allowing you to switch branches without losing your changes. This is a useful technique when you want to work on a different branch but are not ready to commit your changes yet.</p> <ul> <li>Stash your changes:  This will create a temporary commit that stores your modifications, allowing you to switch branches. <pre><code>git stash\n</code></pre></li> <li>Git stash list Running this command will show you the stash ID, along with a description that includes the branch name and commit message. <pre><code>git stash list\n</code></pre></li> <li>Git stash pop:</li> </ul> <p>This command is used to apply the changes from the top of the stash stack and remove that stash from the stack.</p> <pre><code>git stash pop\n</code></pre> <ul> <li>Git stash drop: </li> </ul> <p>This command allows you to discard a stash from the stash stack. It permanently removes a stash and its changes, freeing up space in the stack.</p> <pre><code>git stash drop\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/","title":"Helm Commands","text":""},{"location":"developertools/cheatsheets/helm-cheat-sheet/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive cheat sheet of commonly used Helm commands with examples.</p> <p>Helm is a package manager for Kubernetes that helps you manage, install, and upgrade applications and services in a Kubernetes cluster.</p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#basic-concepts","title":"Basic Concepts","text":"<p>Having a recap of these Helm concepts will greatly assist you in following this article and understanding the Helm commands more easily.</p> <ul> <li>A Helm chart is a package containing all the necessary files, templates, and metadata required to describe and deploy an application on a Kubernetes cluster. </li> <li>Helm repositories are locations where Helm charts are stored and can be accessed for installation. Helm supports both public and private repositories. Public repositories, such as the official Helm Hub, provide a wide range of community-contributed charts. Private repositories can be set up within organizations to distribute custom charts internally.</li> <li>-The Helm Hub is a centralized repository maintained by the Helm community. It serves as a catalog of Helm charts contributed by the community members. The Helm Hub provides a convenient way to discover and search for charts that you can use in your applications. You can find charts for various applications, databases, services, and more on the Helm Hub.</li> <li>A Helm release represents an instance of a chart deployed on a Kubernetes cluster. When you install a Helm chart, it creates a release with a unique name. Each release has its own set of resources, configuration values, and version history. Releases are managed by Helm, allowing you to upgrade, rollback, and delete them easily. Helm maintains a release history, allowing you to view and manage the different versions of a release.</li> </ul>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster configured</li> <li>Helm package manager installed</li> </ul>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#installing-helm","title":"Installing Helm","text":"<p>Use the following commands to install Helm package manager in MacOS and Windows.</p> <pre><code># MacOS (using Homebrew):\nbrew install helm\n\n# Windows OS (using Choco)\nchoco install kubernetes-helm\n\n#  verify the installation by running \nhelm version\n</code></pre> <p>For more information, refer to the official Helm documentation:  - https://helm.sh/docs/intro/install/</p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#azure-login","title":"Azure login","text":"<p>Login into azure first.</p> <pre><code>az login\naz account list --output table\n\n# Select the subscription\naz account set -s \"&lt;subscription-name&gt;\"\n\n# Connect to Azure Kubernetes Service Cluster with User Role\naz aks get-credentials -g \"&lt;resource-group&gt;\" -n \"&lt;cluster-name&gt;\"\n\n# Verify the cluster info\nkubectl cluster-info\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-version","title":"Helm Version","text":"<p>See the installed version of Helm.</p> <pre><code>helm version\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-help","title":"Helm --help","text":"<p>Display the general help output for Helm</p> <pre><code>helm --help\nhelm [command] --help \n\n# examples\nhelm repo --help\nhelm repo list --help\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-repositories","title":"Helm repositories","text":"<p>List the available Helm repositories. </p> <p><pre><code>helm repo list\n</code></pre> output</p> <p><pre><code>NAME                    URL\nbitnami                 https://charts.bitnami.com/bitnami\nrunix                   https://helm.runix.net\ningress-nginx           https://kubernetes.github.io/ingress-nginx\njetstack                https://charts.jetstack.io\nprometheus-community    https://prometheus-community.github.io/helm-charts\napache-solr             https://solr.apache.org/charts\nazure-marketplace       https://marketplace.azurecr.io/helm/v1/repo\nbitnami-azure           https://marketplace.azurecr.io/helm/v1/repo\n</code></pre> Update list of Helm charts from repositories</p> <p><pre><code>helm repo update\n</code></pre> output <pre><code>Hang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"jaegertracing\" chart repository\n...Successfully got an update from the \"jetstack\" chart repository\n...Successfully got an update from the \"runix\" chart repository\n...Successfully got an update from the \"prometheus-community\" chart repository\n...Successfully got an update from the \"apache-solr\" chart repository\n...Successfully got an update from the \"azure-marketplace\" chart repository\n...Successfully got an update from the \"bitnami-azure\" chart repository\n...Successfully got an update from the \"bitnami\" chart repository\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#search-helm-repo","title":"Search helm repo","text":"<p><code>Helm search repo</code> searches the repositories that you have added to your local helm client (with helm repo add)</p> <pre><code>helm search &lt;chart-name&gt;\n# Examples\nhelm search repo bitnami\nhelm search repo azure-marketplace\nhelm search repo wordpress\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#search-helm-hub","title":"Search Helm Hub","text":"<p>The <code>helm search hub</code> command allows you to search for Helm charts specifically in the official Helm Hub repository (Artifact Hub). The Helm Hub is a centralized repository of publicly available charts maintained by the Helm community.</p> <pre><code>helm search hub &lt;chart-name&gt;\n# Examples\nhelm search hub hub\nhelm search hub bitnami\nhelm search hub microsoft\nhelm search hub wordpress \nhelm search hub prometheus\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#add-a-repository","title":"Add a repository","text":"<p>Add a repository from the internet</p> <pre><code>helm repo add [repository-name] [url]\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#remove-a-repository","title":"Remove a repository","text":"<p>Remove a repository from your system:</p> <pre><code>helm repo remove [repository-name]\nhelm repo remove bitnami\n</code></pre> <p>output <pre><code>\"bitnami\" has been removed from your repositories\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#list-installed-helm-charts","title":"List Installed Helm Charts","text":"<p>List Installed Helm Charts in default namespace</p> <pre><code>helm ls\n</code></pre> <p>output</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nmy-minio        default         1               2022-12-05 16:45:31.6927425 -0800 PST   deployed        minio-11.10.16  2022.11.11\n</code></pre> <p>List Installed Helm Charts from all namespace</p> <pre><code>helm ls -aA\n</code></pre> <p>output</p> <pre><code>NAME                                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncert-manager                            cert-manager    1               2022-11-09 17:54:00.8093352 -0800 PST   deployed        cert-manager-v1.10.0                    v1.10.0\ncsi-secrets-store-provider-azure        kube-system     1               2022-12-09 16:37:01.010911 -0800 PST    deployed        csi-secrets-store-provider-azure-1.3.0  1.3.0\n</code></pre> <p>List Installed Helm Charts in specific namespace</p> <pre><code>helm ls -n cert-manager\n</code></pre> <p>output</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ncert-manager    cert-manager    1               2022-11-09 17:54:00.8093352 -0800 PST   deployed        cert-manager-v1.10.0    v1.10.0\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#install-helm-chart","title":"Install helm chart","text":"<p>The <code>helm install</code> command is used to deploy a chart onto a Kubernetes cluster. It takes a chart package as input and deploys the associated resources, such as pods, services, and config maps, to the cluster.</p> <pre><code>helm install &lt;release-name&gt; &lt;chart&gt;\n</code></pre> <ul> <li><code>&lt;release-name&gt;</code> is the name you choose for the release of the chart. It is used to uniquely identify the deployed resources.</li> <li><code>&lt;chart&gt;</code> refers to the chart package or the path to the chart directory. This can be either a local chart or a chart from a remote repository.</li> </ul> <p>Install a chart from a remote repository:</p> <p><pre><code>helm install my-release stable/mysql\n</code></pre> This command installs the mysql chart from the stable repository with the release name my-release. It deploys the MySQL database to the cluster.</p> <p>Install a chart from a local directory:</p> <pre><code>helm install my-release ./my-chart\n</code></pre> <p>This command installs a chart located in the my-chart directory with the release name my-release. It deploys the resources defined in the chart to the cluster.</p> <p>Install a chart with custom configuration values:</p> <pre><code>helm install my-release stable/mysql --set mysqlRootPassword=secretpassword\n</code></pre> <p>Install a release in a specific namespace:</p> <pre><code>helm install -name release-name charts-name --namespace sample\n</code></pre> <p>Override the default values with those specified in a file of your choice:</p> <pre><code>helm install [app-name] [chart] --values [yaml-file/url]\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-dev.yaml\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-test.yaml\nhelm install -name helm-release-name helm-charts --namespace sample --values helm-charts/values-prod.yaml\n</code></pre> <p>Run a test installation to validate and verify the chart:</p> <pre><code>helm install [release-name] --dry-run --debug\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#uninstall-helm-chart","title":"Uninstall helm chart","text":"<p>To uninstall a Helm chart and delete the associated resources from your Kubernetes cluster, you can use the helm uninstall command. </p> <pre><code>helm uninstall [release]\n\n# Example\nhelm uninstall helm-release-name -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#upgrading-helm-charts","title":"Upgrading Helm Charts","text":"<p>To upgrade a Helm chart to a new version or with updated configuration, you can use the helm upgrade command. </p> <pre><code>helm upgrade [release] [chart]\nhelm upgrade -name helm-release-name helm-charts --namespace sample\n</code></pre> <p>Instruct Helm to rollback changes if the upgrade fails:</p> <pre><code>helm upgrade [release] [chart] --atomic\n</code></pre> <p>Upgrade a release. If it does not exist on the system, install it:</p> <pre><code>helm upgrade [release] [chart] --install\n</code></pre> <p>Upgrade to a specified version:</p> <pre><code>helm upgrade [release] [chart] --version [version-number]\n\n# first get the info\nhelm list --namespace sample\n\nhelm upgrade -name helm-release-name helm-charts --version 1.0.0 --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#rollback-helm-chart","title":"Rollback helm chart","text":"<p>The <code>helm rollback</code> command allows you to revert a failed or undesired Helm release upgrade to a previous version. This feature is especially useful when an upgrade introduces issues or unexpected behavior, enabling you to quickly restore the previous working state.</p> <pre><code>helm rollback [release] [revision]\n# first list installed helm charts \nhelm list --namespace sample\n# rollback to specific version.\nhelm rollback -name helm-release-name 18 --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#release-monitoring","title":"Release Monitoring","text":"<p>The helm list command enables listing releases in a Kubernetes cluster</p> <p>List all available releases in the current namespace:</p> <pre><code>helm list\n</code></pre> <p>List all available releases across <code>all namespaces</code>:</p> <p><pre><code>helm ls -aA\nhelm list --all-namespaces\n</code></pre> List all releases in a <code>specific namespace</code>:</p> <pre><code>helm list --namespace [namespace]\nhelm list --namespace sample\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-status","title":"helm status","text":"<p>See the <code>status</code> of a specific release:</p> <pre><code>helm status [release]\nhelm status argocd --namespace argocd\n</code></pre> <p>output</p> <pre><code>NAME: argocd\nLAST DEPLOYED: Fri Jan 20 20:18:33 2023\nNAMESPACE: argocd\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nIn order to access the server UI you have the following options:\n\n1. kubectl port-forward service/argocd-server -n argocd 8080:443\n\n    and then open the browser on http://localhost:8080 and accept the certificate\n\n2. enable ingress in the values file `server.ingress.enabled` and either\n      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough\n      - Set the `configs.params.\"server.insecure\"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts\n\n\nAfter reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:\n\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-history","title":"Helm history","text":"<p>Display the release <code>history</code></p> <pre><code>helm history [release]\nhelm history argocd --namespace argocd\n</code></pre> <p>output  <pre><code>REVISION        UPDATED                         STATUS    CHART                                                                                                                                                   APP VERSION      DESCRIPTION\n1               Fri Jan 20 20:18:33 2023        deployed  argo-cd-5.13.7                                                                                                                                          v2.5.2           Install complete\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#chart-management","title":"Chart Management","text":"<p>Create new helm chart </p> <pre><code>helm create my-chart\n</code></pre> <p>output</p> <p><pre><code>Creating my-chart\n</code></pre> Run tests (lint) to examine a chart and identify possible issues:</p> <p><pre><code>helm lint my-chart\n</code></pre> output <pre><code>==&gt; Linting my-charts\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre></p> <p>Inspect a chart and list its contents: <pre><code>helm show all my-chart\n</code></pre> output</p> <pre><code>apiVersion: v2\nappVersion: 1.0.0\ndescription: A Helm chart for project1 projects for AKS cluster\nname: helm-release-name\ntype: application\nversion: \"20220823.12\"\n.\n.\n.\n</code></pre> <p>Display the chart\u2019s values: <pre><code>helm show values my-charts\n</code></pre></p> <p>Download a chart:</p> <pre><code>helm pull my-charts\n</code></pre> <p>Display a list of a chart\u2019s dependencies: <pre><code>helm dependency list my-charts\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#helm-client-environment","title":"Helm client environment","text":"<p>See information about the Helm client environment:</p> <pre><code>helm env\n</code></pre> <p>output <pre><code>HELM_BIN=\"C:\\ProgramData\\chocolatey\\lib\\kubernetes-helm\\tools\\windows-amd64\\helm.exe\"\nHELM_CACHE_HOME=\"C:\\Users\\&lt;username&gt;\\AppData\\Local\\Temp\\helm\"\nHELM_CONFIG_HOME=\"C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\helm\"\nHELM_DATA_HOME=\"C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\helm\"\nHELM_DEBUG=\"false\"\nHELM_KUBEAPISERVER=\"\"\nHELM_KUBEASGROUPS=\"\"\nHELM_KUBEASUSER=\"\"\nHELM_KUBECAFILE=\"\"\nHELM_KUBECONTEXT=\"\"\nHELM_KUBETOKEN=\"\"\nHELM_MAX_HISTORY=\"10\"\nHELM_NAMESPACE=\"default\"\nHELM_PLUGINS=\"C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\helm\\plugins\"\nHELM_REGISTRY_CONFIG=\"C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\helm\\registry\\config.json\"\nHELM_REPOSITORY_CACHE=\"C:\\Users\\&lt;username&gt;\\AppData\\Local\\Temp\\helm\\repository\"\nHELM_REPOSITORY_CONFIG=\"C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\helm\\repositories.yaml\"\n</code></pre></p>"},{"location":"developertools/cheatsheets/helm-cheat-sheet/#commands-summary","title":"Commands Summary","text":"<pre><code>completion  generate autocompletion scripts for the specified shell\ncreate      create a new chart with the given name\ndependency  manage a chart's dependencies\nenv         helm client environment information\nget         download extended information of a named release\nhelp        Help about any command\nhistory     fetch release history\ninstall     install a chart\nlint        examine a chart for possible issues\nlist        list releases\npackage     package a chart directory into a chart archive\nplugin      install, list, or uninstall Helm plugins\npull        download a chart from a repository and (optionally) unpack it in local directory\npush        push a chart to remote\nregistry    login to or logout from a registry\nrepo        add, list, remove, update, and index chart repositories\nrollback    roll back a release to a previous revision\nsearch      search for a keyword in charts\nshow        show information of a chart\nstatus      display the status of the named release\ntemplate    locally render templates\ntest        run tests for a release\nuninstall   uninstall a release\nupgrade     upgrade a release\nverify      verify that a chart at the given path has been signed and is valid\nversion     print the client version information\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/","title":"Kubectl Commands","text":""},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used Kubectl commands with examples.</p> <p>Kubectl is the command line configuration tool for Kubernetes that communicates with a Kubernetes API server. Using Kubectl allows you to create, inspect, update, and delete Kubernetes objects.</p>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#installing-kubectl","title":"Installing Kubectl","text":"<p>Use the following commands to install kubectl on Linux, macOS, and Windows.</p> <pre><code># Linux\nsudo apt-get update\nsudo apt-get install -y kubectl\n\n# MacOS (using Homebrew):\nbrew install kubectl\n\n# Windows OS (using Choco)\nchoco install kubernetes-cli\n\n# Verify the installation by running\nkubectl version\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#connection-to-kubernetes-cluster","title":"Connection to Kubernetes Cluster","text":"<p>We need to establish a connection to a Kubernetes cluster before we can utilize the power of kubectl commands. In this article, I will demonstrate the process using Azure Kubernetes Service (AKS), but keep in mind that you can connect to any Kubernetes cluster like Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE), or any other Kubernetes platform to execute kubectl commands.</p> <pre><code># Log in to Azure first\naz login\n\naz account list\n# or\naz account list --output table\n\n# Select the subscription\naz account set -s \"&lt;subscription-name&gt;\"\n\n# Display information about the currently logged-in Azure subscription\naz account show\n# or\naz account show --output table\n\n# Connect to Cluster\n\n# Connect to Azure Kubernetes Service Cluster with User Role\naz aks get-credentials -g \"&lt;resource-group&gt;\" -n \"&lt;cluster-name&gt;\"\n\n# Connect to Azure Kubernetes Service Cluster with Admin Role\naz aks get-credentials -g \"&lt;resource-group&gt;\" -n \"&lt;cluster-name&gt;\" --admin\n\n# Display detailed information about an AKS cluster\naz aks show -g \"&lt;resource-group&gt;\" -n \"&lt;cluster-name&gt;\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#cluster-information","title":"Cluster Information","text":"<p>This command provides an overview of the cluster information, including the cluster endpoint, certificate authority, and other relevant details.</p> <pre><code>kubectl cluster-info\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#version","title":"Version","text":"<p>Retrieves the Kubernetes version information for the client, server, and other components.</p> <pre><code>kubectl version\nkubectl version --short\nkubectl version --short --client\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-help","title":"kubectl --help","text":"<p>This command provides a comprehensive overview of the kubectl command-line tool's usage, available commands, and options. It displays a detailed help message that guides you through the various functionalities and usage patterns of kubectl.</p> <pre><code>kubectl --help\nkubectl logs --help\nkubectl exec --help\nkubectl describe --help\nkubectl port-forward --help\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#set-an-alias-for-kubectl","title":"Set an Alias for kubectl","text":"<p>Set an alias for kubectl in PowerShell:</p> <pre><code># Set Alias\nNew-Alias -Name 'k' -Value 'kubectl'\n\n# Verify\nk get pods -n sample\n</code></pre> <p>Set an alias for kubectl in Bash:</p> <pre><code># Set Alias\nalias k=kubectl\n\n# Verify\nk get pods -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#context-and-configuration","title":"Context and Configuration","text":"<p>Kubernetes context and configuration are crucial concepts for managing multiple clusters and switching between them using kubectl.</p> <p>Configuration file:</p> <p>Kubectl uses a configuration file, usually located at <code>~/.kube/config</code> by default, to store cluster information, authentication details, and other settings. The configuration file is written in YAML format and can contain multiple contexts, each representing a different cluster.</p> <pre><code># Show Merged kubeconfig settings.\nkubectl config view\n\n# display the first user\nkubectl config view -o jsonpath='{.users[].name}'\n\n # get a list of users\nkubectl config view -o jsonpath='{.users[*].name}'\n\n# display list of contexts\nkubectl config get-contexts\n\n# display the current-context\nkubectl config current-context\n\n# set the default context to my-cluster-name\nkubectl config use-context my-cluster-name\n\n# set a cluster entry in the kubeconfig\nkubectl config set-cluster my-cluster-name\n\n# delete a cluster entry in the kubeconfig\nkubectl config delete-context my-cluster-name\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#listing-resources","title":"Listing Resources","text":"<p>Use the <code>kubectl get</code> command followed by the resource type you want to list.</p> <pre><code># List all pods in the default namespace:\nkubectl get pods\n\n# List all pods in a specific namespace:\nkubectl get pods -n sample\n\n# List all pods in all namespaces:\nkubectl get pods --all-namespaces\n# or\nkubectl get pods -A\n\n# List all services in the default namespace:\nkubectl get services\n\n# List all nodes in the cluster:\nkubectl get nodes\n\n# List all deployments in a namespace:\nkubectl get deployments -n sample\n\n# List all replica sets:\nkubectl get replicasets\n\n# List all persistent volumes:\nkubectl get pv\n\n# List all persistent volume claims in a namespace:\nkubectl get pvc -n sample\n\n# List all config maps in a namespace:\nkubectl get configmaps -n sample\n\n# List all secrets in a namespace:\nkubectl get secrets -n sample\n\n# List all namespaces:\nkubectl get namespaces\n\n# List all events in a namespaces:\nkubectl get events -n sample\n\n# List all ingress in a namespaces:\nkubectl get ingress -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#creating-a-resource","title":"Creating a Resource","text":"<p>Create a resource such as a service, deployment, job, or namespace using the <code>kubectl create</code> command.</p> <pre><code># Create a new namespace\nkubectl create namespace sample\n\n# Create a Deployment\nkubectl create deployment nginx-deployment  --image=nginx -n sample\n\n# Create a Service\nkubectl create service clusterip my-service --tcp=80:8080\n\n# Create a Secret from literal values:\nkubectl create secret generic my-secret --from-literal=username=admin --from-literal=password=pass123\n\n# Create a PersistentVolume\nkubectl create persistentvolume my-pv --capacity=1Gi --host-path=/data\n\n# Create a PersistentVolumeClaim\nkubectl create persistentvolumeclaim my-pvc --namespace=my-namespace --storageClassName=standard --request=1Gi\n\n# Create a resource from a YAML file\nkubectl create -f my-filename.yaml\n# or\nkubectl apply -f my-filename.yaml\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#applying-and-updating-a-resource","title":"Applying and Updating a Resource","text":"<pre><code># Create a resource from a YAML file\nkubectl apply -f my-filename.yaml\n\n# Create from multiple files\nkubectl apply -f ./file1.yaml -f ./file1.yaml \n\n# create resource(s) in all manifest files in folder\nkubectl apply -f ./folder1\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#creating-vs-apply","title":"Creating vs Apply","text":"<p><code>kubectl create</code> is used for creating new resources, while <code>kubectl apply</code> is used for creating and updating resources. <code>kubectl apply</code> provides a more flexible and incremental approach to managing resource configurations.</p>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#viewing-and-finding-resources","title":"Viewing and Finding Resources","text":"<pre><code># List all pods in the current namespace, with more details\nkubectl get pods -o wide\n# Get a pod's YAML\nkubectl get pod my-pod -o yaml\n\n# List Services Sorted by Name\nkubectl get services -n my-namespace --sort-by=.metadata.name\n\n# List pods Sorted by Restart Count\nkubectl get pods -n my-namespace --sort-by='.status.containerStatuses[0].restartCount'\n\n# List PersistentVolumes sorted by capacity\nkubectl get pv -n my-namespace --sort-by=.spec.capacity.storage\n\n# Get all worker nodes (use a selector to exclude results that have a label\n# named 'node-role.kubernetes.io/control-plane')\nkubectl get node --selector='!node-role.kubernetes.io/control-plane'\n\n# List Events sorted by timestamp\nkubectl get events -n my-namespace --sort-by=.metadata.creationTimestamp\n\n# List all warning events\nkubectl events -n my-namespace --types=Warning\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#updating-resources","title":"Updating Resources","text":"<pre><code># Rolling update \"www\" containers of \"frontend\" deployment, updating the image\nkubectl set image deployment/frontend www=image:v2               \n# Check the history of deployments including the revision\nkubectl rollout history deployment/frontend                      \n# Rollback to the previous deployment\nkubectl rollout undo deployment/frontend                         \n# Rollback to a specific revision\nkubectl rollout undo deployment/frontend --to-revision=2         \n# Watch rolling update status of \"frontend\" deployment until completion\nkubectl rollout status -w deployment/frontend                    \n# Rolling restart of the \"frontend\" deployment\nkubectl rollout restart deployment/frontend                      \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#patching-resources","title":"Patching Resources","text":"<pre><code># Update a container's image; spec.containers[*].name is required because it's a merge key\nkubectl patch pod my-pod -n my-namespace -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}'\n\n# Update a container's image using a json patch with positional arrays\nkubectl patch pod my-pod -n my-namespace --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]'\n\n# Disable a deployment livenessProbe using a json patch with positional arrays\nkubectl patch deployment my-deployment -n my-namespace --type json   -p='[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]'\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#editing-resources","title":"Editing Resources","text":"<pre><code># Edit the service named my-service\nkubectl edit svc/my-service -n my-namespace\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#scaling-resources","title":"Scaling Resources","text":"<pre><code># Scale a replicaset named 'my-rs' to 3\nkubectl scale --replicas=2 rs/my-rs -n my-namespace\n\n# Scale a resource specified in \"my-file.yaml\" to 3\nkubectl scale --replicas=3 -f my-file.yaml    \n\n# If the deployment named my-deployment's size is 2, scale my-deployment to 3\nkubectl scale --current-replicas=1 --replicas=3 deployment/nginx-deployment -n sample\nkubectl scale --current-replicas=3 --replicas=1 deployment/nginx-deployment -n sample\nkubectl get deployments -n sample # use this for verify\n\n# Scale multiple replication controllers\nkubectl scale --replicas=5 rc/my-rc1 rc/my-rc1 rc/my-rc1                   \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#deleting-resources","title":"Deleting Resources","text":"<pre><code># Delete pods and services with same names \"aspnet-api\" and \"aspnet-api\"\nkubectl delete pod,service aspnet-api aspnet-api -n sample\n\n# Delete a pod with no grace period\nkubectl delete pod unwanted --now\n\n# Delete pods and services with same names \"baz\" and \"foo\"\nkubectl delete pod,service my-pod1, my-pod2\n\n# Delete pods and services with label name=my-Label\nkubectl delete pods,services -l name=my-Label \n\n# Delete all pods and services in namespace my-namespace,\nkubectl -n my-namespace delete pod,svc --all                             \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-logs","title":"kubectl logs","text":"<pre><code>kubectl logs pod/my-pod -n my-namespace\nkubectl logs svc/my-svc -n my-namespace\n\nkubectl logs pods/sample-server-f486b9bd7-wtw9f -n sample\n\n# Fetches the logs generated by the pod in the last 2 minutes\nkubectl logs --since=2m pods/sample-server-f486b9bd7-wtw9f -n sample\n</code></pre> <p>PowerShell examples:</p> <pre><code># Logs from all the pods\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs $_.Split()[0] -n sample }\n\n## logs from all the pods since 5 hours\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs --since=5h $_.Split()[0] -n sample }\n\n# show log output if any word \"exception\" in it\nkubectl get pods -n sample --no-headers=true | ForEach-Object { kubectl logs --since=5m $_.Split()[0] -n sample } | Select-String \"exception\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#port-forward","title":"Port-Forward","text":"<pre><code>kubectl port-forward my-pod 5000:6000 \nkubectl port-forward service/aspnet-api 80:80 -n sample\nkubectl port-forward svc/pgadmin4 -n pgadmin4 80:80\n\n# listen on local port 5000 and forward to port 5000 on Service backend\nkubectl port-forward svc/my-service 5000                  \n\n# listen on local port 5000 and forward to Service target port with name &lt;my-service-port&gt;\nkubectl port-forward svc/my-service 5000:my-service-port  \n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#interacting-with-running-pods","title":"Interacting with Running Pods","text":"<pre><code>kubectl exec -n my-namespace -it my-pod -- bash\nkubectl exec -n my-namespace -it my-pod -- ls /\n\n# list files\n# root@my-pod:/app# ls\n# root@my-pod:/app# exit\n\n# print environment variables\nkubectl exec -n my-namespace my-pod -- printenv\n\n# describe pod\nkubectl describe pod/nginx-deployment-9456bbbf9-ngv7f -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#show-metrics","title":"Show Metrics","text":"<pre><code># Show node metrics:\nkubectl top nodes\n\n# Show pod metrics:\nkubectl top pods\n\n# --sort-by flag with memory \nkubectl top nodes --sort-by=memory \nkubectl top pods --sort-by=memory -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#formatting-output","title":"Formatting Output","text":"<p>You can format the output of commands to customize the information displayed.</p> <pre><code>kubectl get pods -n sample -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName\n\n# all pods\nkubectl get pods -n sample -o=yaml\nkubectl get pods -n sample -o=json\nkubectl get pods -n sample -o=name\n\n# single pod\nkubectl get pod/aspnet-api-6699db6d4b-66d7m -n sample -o=yaml\n\nkubectl describe pod/aspnet-api-6699db6d4b-66d7m -o go-template='{{range .status.containerStatuses}}{{.name}}:{{.restartCount}}{{\"\\n\"}}{{end}}'\n\n# Show labels of all pods\nkubectl get pods --show-labels\n\nkubectl get pods -n sample -o wide\n\n# All images running in a cluster\nkubectl get pods -A -o=custom-columns='IMAGE:spec.containers[*].image'\n\n# All images running in a namespace\nkubectl get pods -n sample -o=custom-columns='IMAGE:spec.containers[*].image'\n\n# All container name running in a namespace\nkubectl get pods -n sample -o=custom-columns='NAME:spec.containers[*].name'\n\n# All images running in namespace: sample, grouped by Pod\nkubectl get pods -n sample --output=custom-columns=\"NAME:.metadata.name,IMAGE:.spec.containers[*].image\"\n\n# This will get all container with the namespace in a pretty format:\nkubectl get pods --all-namespaces -o=custom-columns=NameSpace:.metadata.namespace,NAME:.metadata.name,CONTAINERS:.spec.containers[*].name\n\n# Using grep (Linux/macOS)\n# kubectl describe pod aspnet-api-6699db6d4b-66d7m -n sample | grep \"Container ID\"\n\n# Using PowerShell (Windows)\nkubectl describe pod aspnet-api-6699db6d4b-66d7m -n sample | Select-String \"Container ID\"\n\n# All pods in a namespace\nkubectl describe pods -n sample  | Select-String \"name|Container ID\"\n\n# display NAME and CONTAINERID\nkubectl get pods -n sample -o=custom-columns='NAME:.metadata.name,CONTAINERID:.status.containerStatuses[*].containerID'\n\nkubectl logs aspnet-api-6699db6d4b-66d7m -n sample\nkubectl logs azure-vote-back-6dbbb4bccc-dnwmg -n sample\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#command-invoke","title":"Command Invoke","text":"<p>Use command invoke to access a private Azure Kubernetes Service (AKS) cluster. Reference: AKS Command Invoke</p> <pre><code>  az aks command invoke --resource-group 'rg-rgname-dev' --name 'aks-aksname-dev' --command \"kubectl get namespaces\"\n  az aks command invoke --resource-group 'rg-rgname-dev' --name 'aks-aksname-dev' --command \"kubectl create namespace test\"\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#kubectl-api-resources","title":"kubectl api-resources","text":"<p>The <code>kubectl api-resources</code> command allows you to view the available resource types in your Kubernetes cluster. It provides a list of the supported resource types along with their short names, API group, and whether they are namespaced or not.</p> <pre><code># All namespaced resources\nkubectl api-resources --namespaced=true\nkubectl api-resources -o name\nkubectl api-resources -o wide\nkubectl api-resources --verbs=list,get\nkubectl api-resources | more\n# Use Ctrl + C to exit\n</code></pre> <p>Output:</p> <pre><code>NAME                               SHORTNAMES               APIVERSION                             NAMESPACED   KIND\nbindings                                                    v1                                     true         Binding\ncomponentstatuses                  cs                       v1                                     false        ComponentStatus\nconfigmaps                         cm                       v1                                     true         ConfigMap\nendpoints                          ep                       v1                                     true         Endpoints\nevents                             ev                       v1                                     true         Event\nlimitranges                        limits                   v1                                     true         LimitRange\nnamespaces                         ns                       v1                                     false        Namespace\nnodes                              no                       v1                                     false        Node\npersistentvolumeclaims             pvc                      v1                                     true         PersistentVolumeClaim\npersistentvolumes                  pv                       v1                                     false        PersistentVolume\npods                               po                       v1                                     true         Pod\npodtemplates                                                v1                                     true         PodTemplate\nreplicationcontrollers             rc                       v1                                     true         ReplicationController\nresourcequotas                     quota                    v1                                     true         ResourceQuota\nsecrets                                                     v1                                     true         Secret\nserviceaccounts                    sa                       v1                                     true         ServiceAccount\nservices                           svc                      v1                                     true         Service\nazureassignedidentities                                     aadpodidentity.k8s.io/v1               true         AzureAssignedIdentity\nazureidentities                                             aadpodidentity.k8s.io/v1               true         AzureIdentity\nazureidentitybindings                                       aadpodidentity.k8s.io/v1               true         AzureIdentityBinding\nazurepodidentityexceptions                                  aadpodidentity.k8s.io/v1               true         AzurePodIdentityException\nchallenges                                                  acme.cert-manager.io/v1                true         Challenge\norders                                                      acme.cert-manager.io/v1                true         Order\nmutatingwebhookconfigurations                               admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration\nvalidatingwebhookconfigurations                             admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration\ncustomresourcedefinitions          crd,crds                 apiextensions.k8s.io/v1                false        CustomResourceDefinition\napiservices                                                 apiregistration.k8s.io/v1              false        APIService\ncontrollerrevisions                                         apps/v1                                true         ControllerRevision\ndaemonsets                         ds                       apps/v1                                true         DaemonSet\ndeployments                        deploy                   apps/v1                                true         Deployment\nreplicasets                        rs                       apps/v1                                true         ReplicaSet\nstatefulsets                       sts                      apps/v1                                true         StatefulSet\ntokenreviews                                                authentication.k8s.io/v1               false        TokenReview\nlocalsubjectaccessreviews                                   authorization.k8s.io/v1                true         LocalSubjectAccessReview\nselfsubjectaccessreviews                                    authorization.k8s.io/v1                false        SelfSubjectAccessReview\nselfsubjectrulesreviews                                     authorization.k8s.io/v1                false        SelfSubjectRulesReview\nsubjectaccessreviews                                        authorization.k8s.io/v1                false        SubjectAccessReview\nhorizontalpodautoscalers           hpa                      autoscaling/v2                         true         HorizontalPodAutoscaler\ncronjobs                           cj                       batch/v1                               true         CronJob\njobs                                                        batch/v1                               true         Job\ncertificaterequests                cr,crs                   cert-manager.io/v1                     true         CertificateRequest\ncertificates                       cert,certs               cert-manager.io/v1                     true         Certificate\nclusterissuers                                              cert-manager.io/v1                     false        ClusterIssuer\nissuers                                                     cert-manager.io/v1                     true         Issuer\ncertificatesigningrequests         csr                      certificates.k8s.io/v1                 false        CertificateSigningRequest\nconfigs                            config                   config.gatekeeper.sh/v1alpha1          true         Config\nk8sazurev1blockdefault                                      constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1BlockDefault\nk8sazurev1ingresshttpsonly                                  constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1IngressHttpsOnly\nk8sazurev1serviceallowedports                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV1ServiceAllowedPorts\nk8sazurev2blockautomounttoken                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2BlockAutomountToken\nk8sazurev2blockhostnamespace                                constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2BlockHostNamespace\nk8sazurev2containerallowedimages                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2ContainerAllowedImages\nk8sazurev2noprivilege                                       constraints.gatekeeper.sh/v1beta1      false        K8sAzureV2NoPrivilege\nk8sazurev3allowedcapabilities                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3AllowedCapabilities\nk8sazurev3allowedusersgroups                                constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3AllowedUsersGroups\nk8sazurev3containerlimits                                   constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3ContainerLimits\nk8sazurev3disallowedcapabilities                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3DisallowedCapabilities\nk8sazurev3enforceapparmor                                   constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3EnforceAppArmor\nk8sazurev3hostfilesystem                                    constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3HostFilesystem\nk8sazurev3hostnetworkingports                               constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3HostNetworkingPorts\nk8sazurev3noprivilegeescalation                             constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3NoPrivilegeEscalation\nk8sazurev3readonlyrootfilesystem                            constraints.gatekeeper.sh/v1beta1      false        K8sAzureV3ReadOnlyRootFilesystem\nleases                                                      coordination.k8s.io/v1                 true         Lease\nendpointslices                                              discovery.k8s.io/v1                    true         EndpointSlice\nevents                             ev                       events.k8s.io/v1                       true         Event\nflowschemas                                                 flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema\nprioritylevelconfigurations                                 flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration\nclustertriggerauthentications      cta,clustertriggerauth   keda.sh/v1alpha1                       false        ClusterTriggerAuthentication\nscaledjobs                         sj                       keda.sh/v1alpha1                       true         ScaledJob\nscaledobjects                      so                       keda.sh/v1alpha1                       true         ScaledObject\ntriggerauthentications             ta,triggerauth           keda.sh/v1alpha1                       true         TriggerAuthentication\nnodes                                                       metrics.k8s.io/v1beta1                 false        NodeMetrics\npods                                                        metrics.k8s.io/v1beta1                 true         PodMetrics\ningressclasses                                              networking.k8s.io/v1                   false        IngressClass\ningresses                          ing                      networking.k8s.io/v1                   true         Ingress\nnetworkpolicies                    netpol                   networking.k8s.io/v1                   true         NetworkPolicy\nruntimeclasses                                              node.k8s.io/v1                         false        RuntimeClass\npoddisruptionbudgets               pdb                      policy/v1                              true         PodDisruptionBudget\nclusterrolebindings                                         rbac.authorization.k8s.io/v1           false        ClusterRoleBinding\nclusterroles                                                rbac.authorization.k8s.io/v1           false        ClusterRole\nrolebindings                                                rbac.authorization.k8s.io/v1           true         RoleBinding\nroles                                                       rbac.authorization.k8s.io/v1           true         Role\npriorityclasses                    pc                       scheduling.k8s.io/v1                   false        PriorityClass\nsecretproviderclasses                                       secrets-store.csi.x-k8s.io/v1          true         SecretProviderClass\nsecretproviderclasspodstatuses                              secrets-store.csi.x-k8s.io/v1          true         SecretProviderClassPodStatus\nvolumesnapshotclasses              vsclass,vsclasses        snapshot.storage.k8s.io/v1             false        VolumeSnapshotClass\nvolumesnapshotcontents             vsc,vscs                 snapshot.storage.k8s.io/v1             false        VolumeSnapshotContent\nvolumesnapshots                    vs                       snapshot.storage.k8s.io/v1             true         VolumeSnapshot\nconstraintpodstatuses                                       status.gatekeeper.sh/v1beta1           true         ConstraintPodStatus\nconstrainttemplatepodstatuses                               status.gatekeeper.sh/v1beta1           true         ConstraintTemplatePodStatus\ncsidrivers                                                  storage.k8s.io/v1                      false        CSIDriver\ncsinodes                                                    storage.k8s.io/v1                      false        CSINode\ncsistoragecapacities                                        storage.k8s.io/v1                      true         CSIStorageCapacity\nstorageclasses                     sc                       storage.k8s.io/v1                      false        StorageClass\nvolumeattachments                                           storage.k8s.io/v1                      false        VolumeAttachment\nconstrainttemplates                constraints              templates.gatekeeper.sh/v1             false        ConstraintTemplate\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#help-summary","title":"Help Summary","text":"<pre><code>Basic Commands (Beginner):\n  create          Create a resource from a file or from stdin\n  expose          Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service\n  run             Run a particular image on the cluster\n  set             Set specific features on objects\n\nBasic Commands (Intermediate):\n  explain         Get documentation for a resource\n  get             Display one or many resources\n  edit            Edit a resource on the server\n  delete          Delete resources by file names, stdin, resources and names, or by resources and label selector\n\nDeploy Commands:\n  rollout         Manage the rollout of a resource\n  scale           Set a new size for a deployment, replica set, or replication controller\n  autoscale       Auto-scale a deployment, replica set, stateful set, or replication controller\n\nCluster Management Commands:\n  certificate     Modify certificate resources.\n  cluster-info    Display cluster information\n  top             Display resource (CPU/memory) usage\n  cordon          Mark node as unschedulable\n  uncordon        Mark node as schedulable\n  drain           Drain node in preparation for maintenance\n  taint           Update the taints on one or more nodes\n\nTroubleshooting and Debugging Commands:\n  describe        Show details of a specific resource or group of resources\n  logs            Print the logs for a container in a pod\n  attach          Attach to a running container\n  exec            Execute a command in a container\n  port-forward    Forward one or more local ports to a pod\n  proxy           Run a proxy to the Kubernetes API server\n  cp              Copy files and directories to and from containers\n  auth            Inspect authorization\n  debug           Create debugging sessions for troubleshooting workloads and nodes\n  events          List events\n\nAdvanced Commands:\n  diff            Diff the live version against a would-be applied version\n  apply           Apply a configuration to a resource by file name or stdin\n  patch           Update fields of a resource\n  replace         Replace a resource by file name or stdin\n  wait            Experimental: Wait for a specific condition on one or many resources\n  kustomize       Build a kustomization target from a directory or URL.\n\nSettings Commands:\n  label           Update the labels on a resource\n  annotate        Update the annotations on a resource\n  completion      Output shell completion code for the specified shell (bash, zsh, fish, or powershell)\n\nOther Commands:\n  alpha           Commands for features in alpha\n  api-resources   Print the supported API resources on the server\n  api-versions    Print the supported API versions on the server, in the form of \"group/version\"\n  config          Modify kubeconfig files\n  plugin          Provides utilities for interacting with plugins\n  version         Print the client and server version information\n</code></pre>"},{"location":"developertools/cheatsheets/kubectl-cheat-sheet/#references","title":"References","text":"<ul> <li>Official Kubectl Cheat Sheet</li> <li>Kubectl Commands Reference</li> </ul>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/","title":"PostgreSQL Commands","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#introduction","title":"Introduction","text":"<p>PostgreSQL is a powerful, open-source object-relational database system with over 35 years of active development. It's known for reliability, feature robustness, and performance. This cheat sheet provides essential commands for database administration, development, and troubleshooting in PostgreSQL environments, including Azure PostgreSQL deployments.</p> <p>Related Articles: AKS Pod Connection to PostgreSQL using Workload Identity</p>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#installation","title":"Installation","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#windows","title":"Windows","text":"<pre><code># Using Chocolatey\nchoco install postgresql\n\n# Using installer from official website\n# Download from: https://www.postgresql.org/download/windows/\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#macos","title":"macOS","text":"<pre><code># Using Homebrew\nbrew install postgresql@16\n\n# Start PostgreSQL service\nbrew services start postgresql@16\n\n# Stop PostgreSQL service\nbrew services stop postgresql@16\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Install PostgreSQL\nsudo apt update\nsudo apt install postgresql postgresql-contrib\n\n# Start PostgreSQL service\nsudo systemctl start postgresql\n\n# Enable on boot\nsudo systemctl enable postgresql\n\n# Check status\nsudo systemctl status postgresql\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#using-docker","title":"Using Docker","text":"<pre><code># Pull PostgreSQL image\ndocker pull postgres:16\n\n# Run PostgreSQL container\ndocker run --name postgres-dev \\\n  -e POSTGRES_PASSWORD=mysecretpassword \\\n  -e POSTGRES_DB=mydb \\\n  -p 5432:5432 \\\n  -d postgres:16\n\n# Run with persistent volume\ndocker run --name postgres-dev \\\n  -e POSTGRES_PASSWORD=mysecretpassword \\\n  -e POSTGRES_DB=mydb \\\n  -v postgres-data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  -d postgres:16\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#connecting-to-postgresql","title":"Connecting to PostgreSQL","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#using-psql-command-line","title":"Using psql Command Line","text":"<pre><code># Connect to local database\npsql -U postgres\n\n# Connect to specific database\npsql -U username -d database_name\n\n# Connect to remote host\npsql -h hostname -U username -d database_name -p 5432\n\n# Connect with password prompt\npsql -h hostname -U username -d database_name -W\n\n# Connect using connection string\npsql \"postgresql://username:password@hostname:5432/database_name\"\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Set connection parameters\nexport PGHOST=localhost\nexport PGPORT=5432\nexport PGUSER=myuser\nexport PGPASSWORD=mypassword\nexport PGDATABASE=mydb\n\n# Connect using environment variables\npsql\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#azure-postgresql-flexible-server","title":"Azure PostgreSQL Flexible Server","text":"<pre><code># Connect to Azure PostgreSQL\npsql \"host=myserver.postgres.database.azure.com \\\n      port=5432 \\\n      dbname=mydb \\\n      user=myadmin \\\n      password=mypassword \\\n      sslmode=require\"\n\n# Using Azure AD authentication\npsql \"host=myserver.postgres.database.azure.com \\\n      port=5432 \\\n      dbname=mydb \\\n      user=myuser@myserver \\\n      sslmode=require\"\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#database-management","title":"Database Management","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#list-and-view-databases","title":"List and View Databases","text":"<pre><code>-- List all databases\n\\l\n\\list\n\n-- Connect to a database\n\\c database_name\n\\connect database_name\n\n-- Show current database\nSELECT current_database();\n\n-- Show database size\nSELECT pg_database.datname, \n       pg_size_pretty(pg_database_size(pg_database.datname)) AS size \nFROM pg_database;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#create-and-drop-databases","title":"Create and Drop Databases","text":"<pre><code>-- Create database\nCREATE DATABASE mydb;\n\n-- Create database with owner\nCREATE DATABASE mydb OWNER myuser;\n\n-- Create database with encoding\nCREATE DATABASE mydb \n  ENCODING 'UTF8' \n  LC_COLLATE 'en_US.UTF-8' \n  LC_CTYPE 'en_US.UTF-8';\n\n-- Drop database\nDROP DATABASE mydb;\n\n-- Drop database if exists\nDROP DATABASE IF EXISTS mydb;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#user-role-management","title":"User &amp; Role Management","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#create-and-manage-users","title":"Create and Manage Users","text":"<pre><code>-- Create user with password\nCREATE USER myuser WITH PASSWORD 'mypassword';\n\n-- Create user with additional privileges\nCREATE USER myuser WITH \n  PASSWORD 'mypassword'\n  CREATEDB\n  CREATEROLE;\n\n-- Alter user password\nALTER USER myuser WITH PASSWORD 'newpassword';\n\n-- Drop user\nDROP USER myuser;\n\n-- List all users\n\\du\n\n-- Grant privileges to user\nGRANT ALL PRIVILEGES ON DATABASE mydb TO myuser;\n\n-- Revoke privileges\nREVOKE ALL PRIVILEGES ON DATABASE mydb FROM myuser;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#role-management","title":"Role Management","text":"<pre><code>-- Create role\nCREATE ROLE readonly;\n\n-- Grant role to user\nGRANT readonly TO myuser;\n\n-- Create role with login\nCREATE ROLE myuser WITH LOGIN PASSWORD 'mypassword';\n\n-- Grant specific permissions to role\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly;\n\n-- Set default privileges for role\nALTER DEFAULT PRIVILEGES IN SCHEMA public\n  GRANT SELECT ON TABLES TO readonly;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#table-operations","title":"Table Operations","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#create-tables","title":"Create Tables","text":"<pre><code>-- Basic table creation\nCREATE TABLE employees (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    department VARCHAR(50),\n    salary DECIMAL(10, 2),\n    hire_date DATE DEFAULT CURRENT_DATE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create table with foreign key\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    total_amount DECIMAL(10, 2)\n);\n\n-- Create table from query\nCREATE TABLE archived_orders AS\nSELECT * FROM orders WHERE order_date &lt; '2024-01-01';\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#list-and-describe-tables","title":"List and Describe Tables","text":"<pre><code>-- List all tables in current database\n\\dt\n\n-- List tables in specific schema\n\\dt schema_name.*\n\n-- Describe table structure\n\\d table_name\n\n-- Show table with details\n\\d+ table_name\n\n-- Show table columns\nSELECT column_name, data_type, character_maximum_length\nFROM information_schema.columns\nWHERE table_name = 'employees';\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#alter-tables","title":"Alter Tables","text":"<pre><code>-- Add column\nALTER TABLE employees ADD COLUMN phone VARCHAR(20);\n\n-- Drop column\nALTER TABLE employees DROP COLUMN phone;\n\n-- Rename column\nALTER TABLE employees RENAME COLUMN name TO full_name;\n\n-- Change column type\nALTER TABLE employees ALTER COLUMN salary TYPE NUMERIC(12, 2);\n\n-- Add constraint\nALTER TABLE employees ADD CONSTRAINT salary_positive CHECK (salary &gt; 0);\n\n-- Rename table\nALTER TABLE employees RENAME TO staff;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#drop-tables","title":"Drop Tables","text":"<pre><code>-- Drop table\nDROP TABLE employees;\n\n-- Drop table if exists\nDROP TABLE IF EXISTS employees;\n\n-- Drop table and dependent objects\nDROP TABLE employees CASCADE;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#data-manipulation","title":"Data Manipulation","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#insert-data","title":"Insert Data","text":"<pre><code>-- Insert single row\nINSERT INTO employees (name, email, department, salary)\nVALUES ('John Doe', 'john@example.com', 'Engineering', 75000);\n\n-- Insert multiple rows\nINSERT INTO employees (name, email, department, salary)\nVALUES \n  ('Jane Smith', 'jane@example.com', 'Marketing', 65000),\n  ('Bob Johnson', 'bob@example.com', 'Sales', 70000);\n\n-- Insert with returning\nINSERT INTO employees (name, email)\nVALUES ('Alice Brown', 'alice@example.com')\nRETURNING id, created_at;\n\n-- Insert from select\nINSERT INTO archived_employees\nSELECT * FROM employees WHERE hire_date &lt; '2020-01-01';\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#select-data","title":"Select Data","text":"<pre><code>-- Select all columns\nSELECT * FROM employees;\n\n-- Select specific columns\nSELECT name, department, salary FROM employees;\n\n-- Select with WHERE clause\nSELECT * FROM employees WHERE department = 'Engineering';\n\n-- Select with multiple conditions\nSELECT * FROM employees \nWHERE department = 'Engineering' AND salary &gt; 70000;\n\n-- Select with LIKE\nSELECT * FROM employees WHERE name LIKE 'John%';\n\n-- Select with IN\nSELECT * FROM employees WHERE department IN ('Engineering', 'Sales');\n\n-- Select with ORDER BY\nSELECT * FROM employees ORDER BY salary DESC;\n\n-- Select with LIMIT\nSELECT * FROM employees ORDER BY salary DESC LIMIT 10;\n\n-- Select with aggregation\nSELECT department, COUNT(*), AVG(salary) as avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) &gt; 60000;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#update-data","title":"Update Data","text":"<pre><code>-- Update single row\nUPDATE employees \nSET salary = 80000 \nWHERE id = 1;\n\n-- Update multiple columns\nUPDATE employees \nSET salary = salary * 1.1, \n    department = 'Senior Engineering'\nWHERE department = 'Engineering' AND salary &gt; 75000;\n\n-- Update with returning\nUPDATE employees \nSET salary = 85000 \nWHERE id = 1\nRETURNING *;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#delete-data","title":"Delete Data","text":"<pre><code>-- Delete specific rows\nDELETE FROM employees WHERE id = 1;\n\n-- Delete with condition\nDELETE FROM employees WHERE hire_date &lt; '2020-01-01';\n\n-- Delete all rows (use with caution)\nDELETE FROM employees;\n\n-- Delete with returning\nDELETE FROM employees WHERE id = 1 RETURNING *;\n\n-- Truncate table (faster than DELETE)\nTRUNCATE TABLE employees;\n\n-- Truncate with cascade\nTRUNCATE TABLE employees CASCADE;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#indexes","title":"Indexes","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#create-and-manage-indexes","title":"Create and Manage Indexes","text":"<pre><code>-- Create index\nCREATE INDEX idx_employees_email ON employees(email);\n\n-- Create unique index\nCREATE UNIQUE INDEX idx_employees_email_unique ON employees(email);\n\n-- Create multi-column index\nCREATE INDEX idx_employees_dept_salary ON employees(department, salary);\n\n-- Create partial index\nCREATE INDEX idx_active_employees ON employees(email) \nWHERE active = true;\n\n-- List all indexes\n\\di\n\n-- Show indexes for specific table\nSELECT indexname, indexdef\nFROM pg_indexes\nWHERE tablename = 'employees';\n\n-- Drop index\nDROP INDEX idx_employees_email;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#query-optimization","title":"Query Optimization","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#explain-and-analyze","title":"EXPLAIN and ANALYZE","text":"<pre><code>-- Show query execution plan\nEXPLAIN SELECT * FROM employees WHERE department = 'Engineering';\n\n-- Show execution plan with costs\nEXPLAIN (COSTS ON) SELECT * FROM employees WHERE salary &gt; 70000;\n\n-- Analyze and execute query\nEXPLAIN ANALYZE SELECT * FROM employees WHERE department = 'Engineering';\n\n-- Detailed analysis\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE) \nSELECT * FROM employees \nWHERE department = 'Engineering';\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>-- Show running queries\nSELECT pid, usename, application_name, state, query\nFROM pg_stat_activity\nWHERE state = 'active';\n\n-- Show slow queries\nSELECT query, mean_exec_time, calls\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Show table statistics\nSELECT schemaname, tablename, \n       seq_scan, seq_tup_read, \n       idx_scan, idx_tup_fetch\nFROM pg_stat_user_tables;\n\n-- Show cache hit ratio\nSELECT \n  sum(heap_blks_read) as heap_read,\n  sum(heap_blks_hit) as heap_hit,\n  sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio\nFROM pg_statio_user_tables;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#backup-restore","title":"Backup &amp; Restore","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#using-pg_dump","title":"Using pg_dump","text":"<pre><code># Backup single database\npg_dump -U postgres mydb &gt; mydb_backup.sql\n\n# Backup with custom format (compressed)\npg_dump -U postgres -Fc mydb &gt; mydb_backup.dump\n\n# Backup specific tables\npg_dump -U postgres -t employees -t departments mydb &gt; tables_backup.sql\n\n# Backup to remote server\npg_dump -h remote-host -U postgres mydb &gt; mydb_backup.sql\n\n# Backup all databases\npg_dumpall -U postgres &gt; all_databases.sql\n\n# Backup only schema\npg_dump -U postgres --schema-only mydb &gt; schema_only.sql\n\n# Backup only data\npg_dump -U postgres --data-only mydb &gt; data_only.sql\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#using-pg_restore","title":"Using pg_restore","text":"<pre><code># Restore from custom format\npg_restore -U postgres -d mydb mydb_backup.dump\n\n# Restore with clean option (drop existing objects)\npg_restore -U postgres -d mydb --clean mydb_backup.dump\n\n# Restore specific table\npg_restore -U postgres -d mydb -t employees mydb_backup.dump\n\n# Restore to different database\npg_restore -U postgres -d newdb mydb_backup.dump\n\n# Restore from plain SQL\npsql -U postgres mydb &lt; mydb_backup.sql\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#azure-postgresql-backup","title":"Azure PostgreSQL Backup","text":"<pre><code># Backup Azure PostgreSQL database\npg_dump -h myserver.postgres.database.azure.com \\\n  -U myadmin@myserver \\\n  -d mydb \\\n  -Fc &gt; azure_backup.dump\n\n# Restore to Azure PostgreSQL\npg_restore -h myserver.postgres.database.azure.com \\\n  -U myadmin@myserver \\\n  -d mydb \\\n  --no-owner \\\n  --no-privileges \\\n  azure_backup.dump\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#common-use-cases","title":"Common Use Cases","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#working-with-json-data","title":"Working with JSON Data","text":"<pre><code>-- Create table with JSON column\nCREATE TABLE api_logs (\n    id SERIAL PRIMARY KEY,\n    request_data JSONB,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Insert JSON data\nINSERT INTO api_logs (request_data)\nVALUES ('{\"user_id\": 123, \"action\": \"login\", \"ip\": \"192.168.1.1\"}');\n\n-- Query JSON data\nSELECT request_data-&gt;&gt;'user_id' as user_id,\n       request_data-&gt;&gt;'action' as action\nFROM api_logs;\n\n-- Query nested JSON\nSELECT request_data-&gt;'user'-&gt;&gt;'name' as username\nFROM api_logs;\n\n-- Create index on JSON field\nCREATE INDEX idx_request_user_id ON api_logs ((request_data-&gt;&gt;'user_id'));\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#working-with-arrays","title":"Working with Arrays","text":"<pre><code>-- Create table with array column\nCREATE TABLE projects (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100),\n    tags TEXT[]\n);\n\n-- Insert array data\nINSERT INTO projects (name, tags)\nVALUES ('Project A', ARRAY['urgent', 'backend', 'api']);\n\n-- Query array data\nSELECT * FROM projects WHERE 'urgent' = ANY(tags);\n\n-- Array operations\nSELECT name, array_length(tags, 1) as tag_count\nFROM projects;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#transactions","title":"Transactions","text":"<pre><code>-- Begin transaction\nBEGIN;\n\n-- Perform operations\nINSERT INTO accounts (name, balance) VALUES ('Alice', 1000);\nUPDATE accounts SET balance = balance - 100 WHERE name = 'Bob';\n\n-- Commit transaction\nCOMMIT;\n\n-- Rollback transaction\nROLLBACK;\n\n-- Savepoint\nBEGIN;\nINSERT INTO logs (message) VALUES ('Step 1');\nSAVEPOINT step1;\nINSERT INTO logs (message) VALUES ('Step 2');\nROLLBACK TO step1;\nCOMMIT;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#connection-issues","title":"Connection Issues","text":"<pre><code>-- Check PostgreSQL is running\nsudo systemctl status postgresql\n\n-- Check listening ports\nsudo netstat -plnt | grep postgres\n\n-- Show connection limits\nSHOW max_connections;\n\n-- Show current connections\nSELECT count(*) FROM pg_stat_activity;\n\n-- Kill idle connections\nSELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE state = 'idle' AND state_change &lt; NOW() - INTERVAL '10 minutes';\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#permission-issues","title":"Permission Issues","text":"<pre><code>-- Check user permissions\n\\du username\n\n-- Grant all privileges\nGRANT ALL PRIVILEGES ON DATABASE mydb TO myuser;\n\n-- Grant table permissions\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO myuser;\n\n-- Grant sequence permissions\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO myuser;\n\n-- Reset ownership\nALTER DATABASE mydb OWNER TO newowner;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#performance-issues","title":"Performance Issues","text":"<pre><code>-- Find slow queries\nSELECT pid, now() - pg_stat_activity.query_start AS duration, query\nFROM pg_stat_activity\nWHERE state = 'active' AND now() - pg_stat_activity.query_start &gt; interval '5 minutes';\n\n-- Find missing indexes\nSELECT schemaname, tablename, attname, n_distinct, correlation\nFROM pg_stats\nWHERE schemaname NOT IN ('pg_catalog', 'information_schema')\nAND n_distinct &gt; 100\nORDER BY n_distinct DESC;\n\n-- Vacuum and analyze\nVACUUM ANALYZE employees;\n\n-- Full vacuum\nVACUUM FULL employees;\n\n-- Reindex table\nREINDEX TABLE employees;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#azure-postgresql-specific","title":"Azure PostgreSQL Specific","text":"<pre><code>-- Check server parameters\nSHOW ALL;\n\n-- Check Azure extensions\nSELECT * FROM pg_available_extensions ORDER BY name;\n\n-- Install extension\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Check replication lag (for read replicas)\nSELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS replication_lag_seconds;\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#psql-meta-commands","title":"psql Meta-Commands","text":"<pre><code>-- Help\n\\?\n\n-- List databases\n\\l\n\n-- List tables\n\\dt\n\n-- List schemas\n\\dn\n\n-- List functions\n\\df\n\n-- List views\n\\dv\n\n-- Describe table\n\\d table_name\n\n-- Execute SQL from file\n\\i filename.sql\n\n-- Output to file\n\\o output.txt\n\n-- Timing queries\n\\timing\n\n-- Quit psql\n\\q\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#environment-variables","title":"Environment Variables","text":"<pre><code># Connection settings\nexport PGHOST=localhost\nexport PGPORT=5432\nexport PGUSER=myuser\nexport PGPASSWORD=mypassword\nexport PGDATABASE=mydb\n\n# SSL settings\nexport PGSSLMODE=require\nexport PGSSLCERT=/path/to/cert.pem\nexport PGSSLKEY=/path/to/key.pem\n\n# Client encoding\nexport PGCLIENTENCODING=UTF8\n</code></pre>"},{"location":"developertools/cheatsheets/postgresql-cheat-sheet/#references","title":"References","text":"<ul> <li>PostgreSQL Official Documentation</li> <li>Azure Database for PostgreSQL Documentation</li> <li>AKS Pod Connection to PostgreSQL using Workload Identity</li> <li>PostgreSQL Tutorial</li> <li>pgAdmin - PostgreSQL Administration Tool</li> </ul> <p>Last Updated: December 30, 2025</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/","title":"Redis Cache Commands","text":"<p>Azure Cache for Redis is a fully managed, in-memory data store service provided by Microsoft Azure. It offers high-performance data caching capabilities, enabling applications to achieve low-latency access to frequently accessed data. Redis, being an open-source, in-memory data structure store, supports various commands for data manipulation and management. </p> <p>In this cheat sheet, we'll explore some commonly used Redis commands specifically for Azure Cache for Redis.</p> <p>Key Commands</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#set-key-value","title":"SET key value","text":"<p>Sets the value of a key.</p> <pre><code>SET mykey \"Hello\"\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#get-key","title":"GET key","text":"<p>Retrieves the value of a key.</p> <pre><code>GET mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#exists-key","title":"EXISTS key","text":"<p>Checks if a key exists.</p> <pre><code>EXISTS mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#del-key","title":"DEL key","text":"<p>Deletes one or more keys.</p> <pre><code>DEL mykey\n</code></pre> <p>String Commands</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#append-key-value","title":"APPEND key value","text":"<p>Appends a value to a key.</p> <pre><code>APPEND mykey \" World\"\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#strlen-key","title":"STRLEN key","text":"<p>Returns the length of the value stored in a key.</p> <pre><code>STRLEN mykey\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#incr-key","title":"INCR key","text":"<p>Increments the integer value of a key by one.</p> <pre><code>INCR mycounter\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#monitor","title":"MONITOR","text":"<p>The <code>MONITOR</code> command in Redis is used to stream all commands received by the Redis server in real-time. It's a helpful tool for debugging and monitoring purposes, allowing you to observe the activity on your Redis instance. Here's how you can use it:</p> <pre><code>MONITOR\n</code></pre> <p>output</p> <pre><code>1711242811.987417 [0 fd40:d75d:22:cae2:6b22:700:a41:ab2:40288] \"GET\" \"mykey\"\n1711242814.532343 [0 fd40:d75d:22:cae2:6b22:700:a41:849:42348] \"PING\"\n1711242814.532363 [0 fd40:d75d:22:cae2:6b22:700:a41:849:42346] \"INFO\" \"replication\"\n1711242815.257225 [0 fd40:d75d:22:cae2:6b22:700:a41:acb:52114] \"INFO\" \"replication\"\n1711242815.257260 [0 fd40:d75d:22:cae2:6b22:700:a41:acb:52128] \"PING\"\n1711242815.442303 [0 fd40:d75d:22:cae2:6b22:700:a41:840:36272] \"PING\"\n</code></pre> <p>Each line in the monitor output represents a command executed on the server, along with the timestamp, client information, and the command itself.</p> <p>To stop monitoring, simply close the connection or issue another command on the client.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#ping","title":"PING","text":"<p>the <code>PING</code> command to verify the connectivity status before executing other commands. If the server responds with a <code>PONG</code>, it indicates that the connection is alive and operational, allowing clients to proceed with executing Redis commands.</p> <pre><code>PING\n</code></pre> <p>output</p> <pre><code>PONG\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#keys","title":"KEYS","text":"<p>The <code>KEYS</code> command in Redis is used to retrieve all keys matching a specified pattern. When you issue the KEYS command with a pattern argument, Redis returns a list of keys that match the specified pattern. The * wildcard character matches zero or more characters in the key name, allowing for flexible pattern matching.</p> <pre><code>KEYS *\n</code></pre> <p>output <pre><code>1) \"key1\"\n2) \"key2\"\n3) \"another_key\"\n</code></pre></p> <pre><code>KEYS pattern\n\nKEYS user:*\n</code></pre>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#flushdb","title":"FLUSHDB","text":"<p>The <code>FLUSHDB</code> command in Redis is used to delete all keys from the currently selected database. It effectively removes all data stored in the Redis database associated with the current connection</p> <pre><code>FLUSHDB\nFLUSHDB ASYNC\n</code></pre> <p>Note</p> <p>The FLUSHDB command deletes all keys from the currently selected Redis database permanently. Use it with caution, as data loss is irreversible.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#flushall","title":"FLUSHALL","text":"<p>The <code>FLUSHALL</code> command in Redis is used to delete all keys from all databases in the Redis instance. It effectively removes all data stored across all databases associated with the Redis server.</p> <pre><code>FLUSHALL\nFLUSHALL ASYNC\n</code></pre> <p>Note</p> <p>The FLUSHALL command deletes all keys from all databases in the Redis instance permanently. Use it with extreme caution, as data loss is irreversible.</p>"},{"location":"developertools/cheatsheets/redis-cheat-sheet/#info","title":"INFO","text":"<p>The <code>INFO</code> command provides a wealth of information about the Redis server's state, making it a valuable tool for monitoring, troubleshooting, and performance tuning. By analyzing the output of the INFO command, administrators can gain insights into various aspects of the Redis server's operation and health.</p> <pre><code># Server\nredis_version:6.0.14\nredis_mode:standalone\nos:Windows\narch_bits:64\nmultiplexing_api:winsock_IOCP\nrun_id:2394a7426e43250f6e6412d256ad82d2989b844c\nuptime_in_seconds:2077180\nuptime_in_days:24\nhz:10\n\n# Clients\nconnected_clients:21\nmaxclients:7500\nclient_recent_max_input_buffer:8\nclient_recent_max_output_buffer:0\nclient_total_writes_outstanding:0\nclient_total_sent_bytes_outstanding:0\nblocked_clients:0\ntracking_clients:0\nclients_in_timeout_table:0\n\n# Memory\nused_memory:196544424\nused_memory_human:187.44M\nused_memory_rss:415309824\nused_memory_rss_human:396.07M\nused_memory_peak:906410480\nused_memory_peak_human:864.42M\nused_memory_peak_perc:21.68%\nused_memory_overhead:106090184\nused_memory_startup:691856\nused_memory_dataset:90454240\nused_memory_dataset_perc:46.18%\nused_memory_lua:32768\nmaxmemory:6100000000\nmaxmemory_reservation:638000000\nmaxfragmentationmemory_reservation:638000000\nmaxmemory_desired_reservation:638000000\nmaxfragmentationmemory_desired_reservation:638000000\nmaxmemory_human:5.68G\nmaxmemory_policy:volatile-lru\nmem_allocator:jemalloc-4.0.3\n\n# Stats\ntotal_connections_received:32639883\ntotal_commands_processed:154211013\ninstantaneous_ops_per_sec:21\nbytes_received_per_sec:6490\nbytes_sent_per_sec:547314\nbytes_received_per_sec_human:6.34K\nbytes_sent_per_sec_human:534.49K\nrejected_connections:0\nexpired_keys:7549179\nevicted_keys:0\nkeyspace_hits:58233911\nkeyspace_misses:26941507\npubsub_channels:2\npubsub_patterns:0\ntotal_oom_messages:0\n\n# Replication\nrole:master\n\n# CPU\nused_cpu_sys:6094.812500\nused_cpu_user:5064.234375\nused_cpu_avg_ms_per_sec:3\nserver_load:0.75\nevent_wait:37\nevent_no_wait:43\nevent_wait_count:46\nevent_no_wait_count:38\n\n# Cluster\ncluster_enabled:0\ncluster_myself_name:\n\n# Keyspace\ndb0:keys=31308,expires=26294,avg_ttl=26464306438\n</code></pre> <p>This cheat sheet provides a quick reference to some of the most commonly used Redis commands in Azure Cache for Redis environments. Make sure to refer to the official documentation for comprehensive details and additional commands.</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/","title":"Terraform Commands","text":""},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#introduction","title":"Introduction","text":"<p>This is a comprehensive cheat sheet of commonly used Terraform commands with examples.</p> <p>Terraform is the infrastructure as code tool from HashiCorp. It is a tool for building, changing, and managing infrastructure in a safe, repeatable way.</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#installing-terraform","title":"Installing Terraform","text":"<p>Use the following commands to install Terraform in Windows, MacOS and Linux environments.</p> <p><pre><code># MacOS (using Homebrew):\n# First, install the HashiCorp tap\nbrew tap hashicorp/tap\n# Now, install Terraform\nbrew install hashicorp/tap/terraform\n\n# Windows OS (using Choco)\nchoco install terraform\n</code></pre> For more information, refer to the official documentation:  - Install Terraform</p>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#upgrade-terraform","title":"Upgrade Terraform","text":"<p>Use the following commands to upgrade Terraform in Windows and macOS.</p> <pre><code># Verify the installation by running\nterraform --version\n\n# Update to the latest version of Terraform in macOS\nbrew update\nbrew upgrade hashicorp/tap/terraform\n\n# Update to the latest version of Terraform in Windows OS\nchoco upgrade terraform\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-help","title":"terraform help","text":"<p>Displays general help information about Terraform, including a list of available commands and options.</p> <pre><code>terraform -h\n# or\nterraform --help\n\n# Displays help information for a specific Terraform command\nterraform &lt;command&gt; --help\nterraform plan --help\nterraform workspace --help\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-init","title":"terraform init","text":"<p>Initializes a Terraform working directory by downloading and configuring providers.</p> <pre><code>terraform init\n\n# Forces Terraform to reconfigure the backend, even if it is already initialized\nterraform init -reconfigure\n\n# Specifies a backend configuration file to use during initialization\nterraform init -backend-config=\"access_key=$(az storage account keys list --resource-group \"rg-terraform-mgmt-poc\" --account-name \"sttfstatespoc01\" --query '[0].value' -o tsv)\"\n\n# If you don't want to hold a state lock during backend migration\nterraform init -lock=false\n\n# Use this to disable interactive prompts\nterraform init -input=false\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-plan","title":"terraform plan","text":"<p>Generates an execution plan, showing the changes that will be made without actually applying them.</p> <pre><code>terraform plan\n\n# Saves the generated plan to a file named tfplan for later use\nterraform plan -out=tfplan\n\n# Specifies a file containing variable values to be used during planning\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-apply","title":"terraform apply","text":"<p>Applies the changes required to reach the desired state defined in the Terraform configuration.</p> <pre><code>terraform apply\n\n# Automatically approves and applies the changes without requiring manual confirmation\nterraform apply -auto-approve\n\n# Use this command to refresh the state for manual changes done from portal directly\nterraform apply -refresh-only -var-file=\"./environments/dev-variables.tfvars\"\n\n# Applies the changes with planfilename\nterraform apply poc-plan\n\n# Do not hold a state lock during the Terraform apply operation\n# Use with caution if other engineers might run concurrent commands against the same workspace\nterraform apply -lock=false\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-destroy","title":"terraform destroy","text":"<p>Destroys the Terraform-managed infrastructure, terminating all resources defined in the Terraform configuration.</p> <pre><code>terraform destroy\n\n# Destroy the infrastructure without having to interactively type 'yes' to the plan\n# Useful in automation CI/CD pipelines\nterraform destroy --auto-approve\n\nterraform destroy -var-file=\"./environments/poc-variables.tfvars\"\n\n# Destroy an instance of a resource created with for_each\nterraform destroy -target=\"module.appgw.resource[\\\"key\\\"]\"\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-validate","title":"terraform validate","text":"<p>Checks if the Terraform configuration files are valid and properly formatted.</p> <pre><code>terraform validate\n\n# See errors and warnings in JSON format\nterraform validate -json\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-fmt","title":"terraform fmt","text":"<p>Automatically updates and formats the Terraform configuration files to follow the canonical format.</p> <pre><code>terraform fmt\n\n# Format files in subdirectories\nterraform fmt --recursive\n\n# Display differences between original configuration files and formatting changes\nterraform fmt --diff\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-state","title":"terraform state","text":"<p>Manages Terraform's state, allowing you to inspect, modify, and manage the state file.</p> <pre><code>terraform state list\n\n# Remove the specified instance from the state file\n# Useful when a resource has been manually deleted outside of Terraform\nterraform state rm 'azurerm_logic_app_workflow.logic[\"1\"]'\nterraform state rm 'azurerm_resource_group.rg'\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-output","title":"terraform output","text":"<p>Displays the outputs defined in the Terraform configuration after applying the changes.</p> <pre><code>terraform output\n\n# State file in JSON format\nterraform output -json\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-import","title":"terraform import","text":"<p>Imports existing infrastructure into Terraform state, allowing you to manage resources that were not created by Terraform.</p> <pre><code># Use this when the resource is created with for_each\nterraform import -var-file=\"./environments/dev-variables.tfvars\" 'azurerm_resource_group.rg[\"3\"]' /subscriptions/342334ec-8a2e-4b7d-a886-e772dc017316/resourceGroups/rg-sitecore-dev\n\nterraform import -var-file=\"./environments/dev-variables.tfvars\" 'azurerm_resource_group.rg' '/subscriptions/dsaf2343-8a2e-4b7d-a886-e772dc017316/resourceGroups/rg-demo'\n\nterraform import azurerm_resource_group.rg\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-workspace","title":"terraform workspace","text":"<p>Manages Terraform workspaces, allowing you to create, select, and delete different named workspaces.</p> <pre><code># Creates a new Terraform workspace\nterraform workspace new dev\n\n# Selects an existing Terraform workspace\nterraform workspace select prod\n\n# Lists all available Terraform workspaces\nterraform workspace list\n\n# Shows the currently selected Terraform workspace\nterraform workspace show\n\n# Deletes an existing Terraform workspace\nterraform workspace delete dev\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-show","title":"terraform show","text":"<p>Show the state file in a human-readable format.</p> <pre><code>terraform show\n\nterraform show &lt;path to statefile&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-providers","title":"terraform providers","text":"<p>Prints a tree of all available providers and their versions.</p> <pre><code>terraform providers\n\n# Locks the provider versions in the Terraform configuration to ensure reproducible builds\nterraform providers lock\n\nterraform providers init\nterraform providers migrate\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-force-unlock","title":"terraform force-unlock","text":"<p>Remove the lock with the specified lock ID from your workspace. Useful when a lock has become 'stuck', usually after an incomplete Terraform run.</p> <pre><code>terraform force-unlock &lt;lock_id&gt;\n</code></pre>"},{"location":"developertools/cheatsheets/terraform-cheat-sheet/#terraform-graph","title":"terraform graph","text":"<p>Produce a graph in DOT language showing the dependencies between objects in the state file.</p> <pre><code>terraform graph\n</code></pre>"},{"location":"developertools/software/mac/","title":"Download &amp; install Software in macOS","text":""},{"location":"developertools/software/mac/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive list of essential tools and software commonly needed by developers in the IT industry. If you are using macOS operating system, you can download and install the following software as per your requirements. It is recommended to use the <code>brew</code> tool for installation, but if you encounter any issues, you can also perform a direct manual install.</p> <p>Note</p> <p>You may need to restart your Mac or terminal after completing all installations.</p>"},{"location":"developertools/software/mac/#what-is-homebrew","title":"What is Homebrew?","text":"<p>Homebrew is a free and open-source software package management system that simplifies the installation of software on Apple's operating system, macOS, as well as Linux. </p>"},{"location":"developertools/software/mac/#install-homebrew","title":"Install Homebrew","text":"<p>This is the first software you may need to install before installing anything on a Mac. For more information, refer to this link: https://brew.sh/</p> <p>Homebrew is similar to Chocolatey in the Windows environment.</p> <p>homebrew (for Mac users) = choco (for Windows users)</p> <p>To use Homebrew, you will need to have a terminal window open and install Homebrew on your system. To install Homebrew, you can copy and paste the following command into the terminal:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Expected output:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/anjikeesari/.zprofile   \neval \"$(/opt/homebrew/bin/brew shellenv)\"  \n</code></pre> <p>Once the installation is finished, you can use the <code>brew</code> command to install, upgrade, and manage software packages using Homebrew.</p> <p>Verify the installation:</p> <pre><code>brew --version\n# or\nbrew doctor\n</code></pre> <p>Upgrade Homebrew:</p> <pre><code>brew upgrade\n</code></pre>"},{"location":"developertools/software/mac/#install-google-chrome","title":"Install Google Chrome","text":"<pre><code>brew install --cask google-chrome\n</code></pre> <p>Verify the installed version:</p> <pre><code>/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --version\n</code></pre> <p>Upgrade Google Chrome:</p> <pre><code>brew upgrade --cask google-chrome\n</code></pre>"},{"location":"developertools/software/mac/#install-iterm2","title":"Install iTerm2","text":"<p>iTerm2 is a popular terminal emulator for macOS. It provides a more advanced and feature-rich interface for working with the command line.</p> <pre><code>brew install --cask iterm2\n</code></pre> <p>Verify the installation:</p> <pre><code>/Applications/iTerm.app/Contents/MacOS/iTerm2 --version\n</code></pre> <p>Upgrade iTerm2:</p> <pre><code>brew upgrade --cask iterm2\n</code></pre>"},{"location":"developertools/software/mac/#install-powershell","title":"Install PowerShell","text":"<p>PowerShell is a command-line shell and scripting language developed by Microsoft. It allows you to automate tasks and manage your computer system efficiently.</p> <pre><code># install\nbrew install --cask powershell\n\n# upgrade\nbrew upgrade --cask powershell\n</code></pre>"},{"location":"developertools/software/mac/#install-vs-code","title":"Install VS Code","text":"<p>It is the most commonly used IDE, it is widely used for programming and supports various languages with extensive customization options.</p> <pre><code>brew install --cask visual-studio-code\n</code></pre> <p>Verify the installation:</p> <pre><code>code --version\n</code></pre> <p>Upgrade VS Code:</p> <pre><code>brew upgrade --cask visual-studio-code\n</code></pre>"},{"location":"developertools/software/mac/#vs-code-extensions","title":"VS Code Extensions","text":"<p>Install essential VS Code extensions using the command line:</p> <pre><code>code --install-extension ms-vscode.azurecli\ncode --install-extension ms-vscode.azure-account\ncode --install-extension ms-kubernetes-tools.vscode-aks-tools\ncode --install-extension ms-kubernetes-tools.vscode-kubernetes-tools\ncode --install-extension hashicorp.terraform\ncode --install-extension ms-dotnettools.csharp\ncode --install-extension ms-vscode-remote.remote-containers\ncode --install-extension ms-azuretools.vscode-docker\ncode --install-extension tim-koehler.vscode-helm\ncode --install-extension ms-vscode.powershell\n</code></pre> <p>List all installed extensions:</p> <pre><code>code --list-extensions\n</code></pre>"},{"location":"developertools/software/mac/#install-git","title":"Install git","text":"<p>Git is a version control system used for tracking changes in software projects. It allows multiple developers to collaborate on a project efficiently and helps manage different versions of the code.</p> <pre><code>brew install git\n</code></pre> <p>Verify the installation:</p> <pre><code>git --version\n</code></pre> <p>Output:</p> <pre><code>git version 2.43.0\n</code></pre> <p>Upgrade Git:</p> <pre><code>brew upgrade git\n</code></pre>"},{"location":"developertools/software/mac/#install-docker","title":"Install docker","text":"<p>Docker is a platform that simplifies the process of creating, deploying, and running applications using containers. </p> <pre><code>brew install --cask docker\n</code></pre> <p>Verify the installation:</p> <pre><code>docker --version\n</code></pre> <p>Output:</p> <pre><code>Docker version 20.10.21, build baeda1f\n</code></pre> <p>Upgrade Docker:</p> <pre><code>brew upgrade --cask docker\n</code></pre>"},{"location":"developertools/software/mac/#install-node","title":"Install node","text":"<p>Node.js is a JavaScript runtime that allows you to execute JavaScript code outside of a web browser.</p> <pre><code>brew install node\n</code></pre> <p>Verify the installation:</p> <pre><code>node --version  \nnpm --version\n</code></pre> <p>Upgrade Node.js:</p> <pre><code>brew upgrade node\n</code></pre>"},{"location":"developertools/software/mac/#install-dotnet","title":"Install dotnet","text":"<p>It provides a runtime environment and libraries for building and running different types of applications, especially those developed using the C# programming language. </p> <pre><code>brew install --cask dotnet\n</code></pre> <p>Verify the installation:</p> <pre><code>dotnet --version\n</code></pre> <p>Upgrade .NET:</p> <pre><code>brew upgrade --cask dotnet\n</code></pre>"},{"location":"developertools/software/mac/#install-python3","title":"Install python3","text":"<p>Python is a popular programming language known for its simplicity and versatility. Installing Python 3 refers to setting up the latest version of Python on your system for development purposes.</p> <pre><code>brew install python3\n</code></pre> <p>Verify the installation:</p> <pre><code>python3 --version\n</code></pre> <p>Upgrade Python 3:</p> <pre><code>brew upgrade python3\n</code></pre>"},{"location":"developertools/software/mac/#upgrade-pip","title":"Upgrade pip","text":"<pre><code>pip3 install --upgrade pip\n</code></pre>"},{"location":"developertools/software/mac/#install-mkdocs","title":"Install mkdocs","text":"<pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-material-extensions\n</code></pre>"},{"location":"developertools/software/mac/#install-azure-cli","title":"Install azure-cli","text":"<p>Azure CLI is a command-line interface for managing and interacting with Microsoft Azure cloud services. It provides a convenient way to automate and control your Azure resources.</p> <pre><code>brew install azure-cli\n</code></pre> <p>Verify the installation:</p> <pre><code>az --version\n</code></pre> <p>Upgrade Azure CLI:</p> <pre><code>brew upgrade azure-cli\n</code></pre>"},{"location":"developertools/software/mac/#install-kubectl","title":"Install kubectl","text":"<p>kubectl is a command-line tool used to interact with Kubernetes clusters. It enables you to deploy, manage, and monitor applications running on Kubernetes.</p> <p>For more information, please refer to the following link: - - https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/</p> <pre><code>brew install kubectl\n</code></pre> <p>Verify the installation:</p> <pre><code>kubectl version\n</code></pre> <p>Upgrade kubectl:</p> <pre><code>brew upgrade kubectl\n</code></pre>"},{"location":"developertools/software/mac/#install-lens","title":"Install Lens","text":"<p>Lens is a popular Kubernetes platform that serves as a robust and advanced development and management environment for Kubernetes clusters. It provides a comprehensive graphical user interface (GUI) that simplifies the management, monitoring, and interaction with Kubernetes resources and clusters.</p> <pre><code>brew install --cask lens\n</code></pre> <p>Verify the installation:</p> <pre><code>lens --version\n</code></pre> <p>Upgrade Lens:</p> <pre><code>brew upgrade --cask lens\n</code></pre>"},{"location":"developertools/software/mac/#install-terraform","title":"Install Terraform","text":"<p>Terraform is an infrastructure-as-code tool used for provisioning and managing cloud resources. It allows you to define your infrastructure in code and automates the deployment and management of resources across different cloud providers.</p> <pre><code>brew install terraform\n</code></pre> <p>Verify the installation:</p> <pre><code>terraform version\n</code></pre> <p>Upgrade Terraform:</p> <pre><code>brew upgrade terraform\n</code></pre>"},{"location":"developertools/software/mac/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. It allows you to define and install applications using charts, which are packages of pre-configured Kubernetes resources.</p> <pre><code>brew install helm\n</code></pre> <p>Verify the installation:</p> <pre><code>helm version\n</code></pre> <p>Upgrade Helm:</p> <pre><code>brew upgrade helm\n</code></pre>"},{"location":"developertools/software/mac/#install-argocd","title":"Install argocd","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.</p> <pre><code>brew install argocd\n</code></pre> <p>Verify the installation:</p> <pre><code>argocd version\n</code></pre> <p>Upgrade Argo CD:</p> <pre><code>brew upgrade argocd\n</code></pre>"},{"location":"developertools/software/mac/#install-pgadmin4","title":"Install pgadmin4","text":"<p>pgAdmin is a graphical administration and development platform for PostgreSQL, a popular open-source relational database management system. It provides a user-friendly interface for managing databases, running queries, and monitoring server activity.</p> <pre><code>brew install pgadmin4\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list pgadmin4\n</code></pre> <p>Upgrade pgAdmin 4:</p> <pre><code>brew upgrade pgadmin4\n</code></pre>"},{"location":"developertools/software/mac/#install-azure-data-studio","title":"Install Azure Data Studio","text":"<p>Data management tool that enables working with SQL Server, PostgerSQL, MySQL and more.</p> <pre><code>brew install --cask azure-data-studio\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list azure-data-studio\n</code></pre> <p>Upgrade Azure Data Studio:</p> <pre><code>brew upgrade --cask azure-data-studio\n</code></pre>"},{"location":"developertools/software/mac/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<p>Azure Storage Explorer is a standalone application provided by Microsoft that allows users to interact Azure storage account services and making it easier to work with Azure Blob Storage, Azure Queue Storage, Azure Table Storage, and Azure Cosmos DB.</p> <pre><code>brew install --cask microsoft-azure-storage-explorer\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list microsoft-azure-storage-explorer\n</code></pre> <p>Upgrade Azure Storage Explorer:</p> <pre><code>brew upgrade --cask microsoft-azure-storage-explorer\n</code></pre>"},{"location":"developertools/software/mac/#install-microsoft-remote-desktop","title":"Install Microsoft Remote Desktop","text":"<p>Microsoft Remote Desktop is a Microsoft application that allows users to remotely access and control Windows-based computers or virtual machines from other devices.</p> <pre><code>brew install --cask microsoft-remote-desktop\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list microsoft-remote-desktop\n</code></pre> <p>Upgrade Microsoft Remote Desktop:</p> <pre><code>brew upgrade --cask microsoft-remote-desktop\n</code></pre>"},{"location":"developertools/software/mac/#install-zoom","title":"Install zoom","text":"<p>Zoom is a video conferencing and online meeting platform that enables you to communicate with others through video, audio, and chat. It's widely used for remote work and virtual meetings.</p> <pre><code>brew install --cask zoom\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list zoom\n</code></pre> <p>Upgrade Zoom:</p> <pre><code>brew upgrade --cask zoom\n</code></pre>"},{"location":"developertools/software/mac/#install-teams","title":"Install Teams","text":"<p>Teams is a collaboration platform developed by Microsoft. It offers features such as chat, video meetings, file sharing, and integration with other Microsoft services to facilitate teamwork and communication.</p> <pre><code># install\nbrew install --cask microsoft-teams\n\n# uninstall teams, it will ask the Mac login password for any software uninstall.\nbrew uninstall microsoft-teams\n</code></pre>"},{"location":"developertools/software/mac/#install-whatsapp","title":"Install WhatsApp","text":"<p>Native desktop client for WhatsApp</p> <pre><code>brew install --cask whatsapp\n</code></pre>"},{"location":"developertools/software/mac/#install-net-7-sdk-for-mac","title":"Install .NET 7 SDK for Mac","text":"<p>The .NET SDK (Software Development Kit) is a set of tools and libraries provided by Microsoft for building applications using the .NET framework. Installing the .NET 7 SDK specifically refers to setting up the latest version of the .NET framework for Mac development.</p> <p>Use the following link for download and install it manually.</p> <p>https://dotnet.microsoft.com/en-us/download/dotnet/thank-you/sdk-7.0.100-macos-x64-installer?journey=vs-code</p>"},{"location":"developertools/software/mac/#install-github","title":"Install github","text":"<p>GitHub CLI (gh) is a command-line tool that brings GitHub functionality to your terminal, allowing you to manage pull requests, issues, repositories, and other GitHub features without leaving the command line.</p> <pre><code>brew install gh\n</code></pre> <p>Verify the installation:</p> <pre><code>gh --version\n</code></pre> <p>Upgrade GitHub CLI:</p> <pre><code>brew upgrade gh\n</code></pre>"},{"location":"developertools/software/mac/#install-jq","title":"Install jq","text":"<p>JQ is a lightweight and flexible command-line tool for processing JSON data. It provides various features to extract, manipulate, and transform JSON files efficiently.</p> <pre><code>brew install jq\n</code></pre> <p>Verify the installation:</p> <pre><code>jq --version\n</code></pre> <p>Upgrade JQ:</p> <pre><code>brew upgrade jq\n</code></pre>"},{"location":"developertools/software/mac/#install-postman","title":"Install postman","text":"<p>Postman is a popular API development and testing tool. It allows you to make HTTP requests, test APIs, and automate API workflows, making it easier to develop and debug APIs.</p> <pre><code>brew install --cask postman\n</code></pre> <p>Verify the installation:</p> <pre><code>brew list postman\n</code></pre> <p>Upgrade Postman:</p> <pre><code>brew upgrade --cask postman\n</code></pre>"},{"location":"developertools/software/mac/#install-lightshot","title":"Install Lightshot","text":"<p>Lightshot is a free program that offers a quick and easy way to capture a screen including basic editing tools. </p> <p>I don't see brew commands to install this tool but you can install it manually from here:</p> <p>Lightshot downloads</p> <p>&lt;!-- Here is the list of other software you may need during the development.</p> <pre><code>1. Zsh \n2. Homebrew - Mac\n2. choco - windows\n3. Chrome \n3. Alfred \n4. Iterm2 \n5. Git \n6. Pycharm \n5. Visual Studio Code \n6. Vim \n7. Extentions a. Prettier b. Bracket Pair Colorizer c. Live Share \n8. Slack \n9. Zoom \n10. Droplr \n11. Loom \n12. Virtual Desktop 13. Spectical\n``` --&gt;\n&lt;!-- \n## Reference\n\n- &lt;https://www.youtube.com/watch?v=0MiGnwPdNGE&gt; - How I setup the Terminal on my M1 Max MacBook Pro \n- https://www.robinwieruch.de/mac-setup-web-development/\n- https://www.robinwieruch.de/mac-setup-web-development/\n--&gt;\n\n## Troubleshooting\n\n### Common Installation Issues\n\n**Homebrew Permission Errors**\n\nIf you encounter permission errors during installation:\n\n```sh\n# Fix Homebrew permissions\nsudo chown -R $(whoami) /usr/local/bin /usr/local/lib /usr/local/share\n\n# For Apple Silicon Macs\nsudo chown -R $(whoami) /opt/homebrew\n</code></pre> <p>Command Not Found After Installation</p> <p>If installed tools are not recognized:</p> <pre><code># Add Homebrew to PATH (for Apple Silicon)\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\n\n# For Intel Macs\necho 'eval \"$(/usr/local/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\n\n# Restart terminal or source the profile\nsource ~/.zprofile\n</code></pre> <p>Xcode Command Line Tools Issues</p> <p>Homebrew requires Xcode Command Line Tools:</p> <pre><code># Install Xcode Command Line Tools\nxcode-select --install\n\n# Verify installation\nxcode-select -p\n</code></pre> <p>Version Conflicts</p> <p>To check installed versions and resolve conflicts:</p> <pre><code># List all installed packages\nbrew list\n\n# Check for outdated packages\nbrew outdated\n\n# Force reinstall a package\nbrew reinstall &lt;package&gt;\n</code></pre>"},{"location":"developertools/software/mac/#additional-resources","title":"Additional Resources","text":"<ul> <li>Homebrew Documentation</li> <li>Homebrew FAQ</li> <li>Network Tools</li> <li>Kubectl Cheat Sheet</li> <li>Docker Cheat Sheet</li> <li>Helm Cheat Sheet</li> <li>Terraform Cheat Sheet</li> <li>Git Cheat Sheet</li> </ul>"},{"location":"developertools/software/mac/#for-intel-macs","title":"For Intel Macs","text":"<p>echo 'eval \"$(/usr/local/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile</p>"},{"location":"developertools/software/mac/#restart-terminal-or-source-the-profile","title":"Restart terminal or source the profile","text":"<p>source ~/.zprofile <pre><code>**Xcode Command Line Tools Issues**\n\nHomebrew requires Xcode Command Line Tools:\n\n```sh\n# Install Xcode Command Line Tools\nxcode-select --install\n\n# Verify installation\nxcode-select -p\n</code></pre></p> <p>Version Conflicts</p> <p>To check installed versions and resolve conflicts:</p> <pre><code># List all installed packages\nbrew list\n\n# Check for outdated packages\nbrew outdated\n\n# Force reinstall a package\nbrew reinstall &lt;package&gt;\n</code></pre>"},{"location":"developertools/software/mac/#quick-command-reference-windows-vs-macos","title":"Quick Command Reference: Windows vs macOS","text":"Task Windows (Chocolatey) macOS (Homebrew) Install package <code>choco install &lt;package&gt;</code> <code>brew install &lt;package&gt;</code> Install GUI app <code>choco install &lt;package&gt;</code> <code>brew install --cask &lt;package&gt;</code> Update package manager <code>choco upgrade chocolatey</code> <code>brew update</code> Upgrade package <code>choco upgrade &lt;package&gt; -y</code> <code>brew upgrade &lt;package&gt;</code> Upgrade all <code>choco upgrade all -y</code> <code>brew upgrade</code> List installed <code>choco list --local-only</code> <code>brew list</code> Search packages <code>choco search &lt;package&gt;</code> <code>brew search &lt;package&gt;</code> Uninstall package <code>choco uninstall &lt;package&gt;</code> <code>brew uninstall &lt;package&gt;</code> Get package info <code>choco info &lt;package&gt;</code> <code>brew info &lt;package&gt;</code>"},{"location":"developertools/software/mac/#additional-resources_1","title":"Additional Resources","text":"<ul> <li>Homebrew Documentation</li> <li>Homebrew FAQ</li> <li>Network Tools</li> </ul>"},{"location":"developertools/software/windows/","title":"Download &amp; install Software in Windows OS","text":""},{"location":"developertools/software/windows/#introduction","title":"Introduction","text":"<p>In this article, I am going to present a comprehensive list of essential tools and software commonly needed by developers in the IT industry. If you are using Windows operating system, you can download and install the following software as per your requirements. It is recommended to use the choco tool for installation, but if you encounter any issues, you can also perform a direct install.</p> <p>Note</p> <p>Restart your computer when prompted or needed and also restart your terminal for every install before validating it.</p>"},{"location":"developertools/software/windows/#install-chocolatey","title":"Install Chocolatey","text":"<p>We will be using Chocolatey commands to install all the required software and developer tools.</p> <p>What is Chocolatey?</p> <p>Chocolatey is a package manager for Windows that enables you to install, upgrade, and manage software packages from the command line.</p> <p>It is strongly recommended to use choco commands for searching and installing the required software instead of attempting manual installations using the provided links.</p> <p>To use Chocolatey, you will need to have Windows PowerShell installed on your system. You can then install Chocolatey using a PowerShell command, and use it to install and manage packages from the command line.</p> <p>Install Chocolatey</p> <p>This is the first step you need to take before installing anything on the Windows OS.</p> <p>To install Chocolatey on your Windows system, open a terminal window (such as PowerShell or Command Prompt) and run as Administrator with the following command:</p> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> <p>Reference - https://docs.chocolatey.org/en-us/choco/setup</p> <p>Verify the version installed <pre><code>choco --version\n</code></pre></p> <p>Upgrading Chocolatey <pre><code>choco upgrade chocolatey\n</code></pre></p>"},{"location":"developertools/software/windows/#install-vs-code","title":"Install VS Code","text":"<p>It is the most commonly used IDE, It is widely used for programming and supports various languages with extensive customization options.</p> <p>Open Command Prompt (cmd) or PowerShell as Administrator and run:</p> <pre><code>choco install vscode\n</code></pre> <p>Verify the installation</p> <pre><code>code --help\n\n# or\n\ncode --version\n\n\n# Check for the latest version (available via choco)\n\nchoco info vscode\n</code></pre> <p>Upgrade VS Code to the latest version</p> <pre><code>choco upgrade vscode -y\n</code></pre> <p>Note:    \"Yes to all prompts\" or \"Automatically accept all confirmations.\"</p>"},{"location":"developertools/software/windows/#vs-code-extensions","title":"VS Code Extensions","text":"<p>After install VS Code, install the following extensions in VS Code as per the need.</p> <p>You can install directly using the VS Code CLI:</p> <pre><code>code --install-extension ms-vscode.azurecli\n# or\ncode --install-extension ms-vscode.azurecli --force\n\ncode --install-extension ms-vscode.azure-account\ncode --install-extension ms-kubernetes-tools.vscode-aks-tools\ncode --install-extension ms-kubernetes-tools.vscode-kubernetes-tools\ncode --install-extension hashicorp.terraform\ncode --install-extension ms-dotnettools.csharp\ncode --install-extension microsoft.k8s-bridge-to-kubernetes\ncode --install-extension ms-vscode-remote.remote-containers\ncode --install-extension ms-vscode-remote.vscode-remote-extensionpack\ncode --install-extension ms-azuretools.vscode-docker\ncode --install-extension formulahendry.dotnet\ncode --install-extension tht13.helm-intellisense\ncode --install-extension tim-koehler.vscode-helm\ncode --install-extension ms-vscode.mssql\ncode --install-extension ms-vscode.powershell\ncode --install-extension mongodb.mongodb-vscode\n</code></pre> <p>List All Installed Extensions</p> <pre><code>code --list-extensions\n</code></pre>"},{"location":"developertools/software/windows/#install-git","title":"Install Git","text":"<p>Git is a version control system used for tracking changes in software projects. It allows multiple developers to collaborate on a project efficiently and helps manage different versions of the code.</p> <pre><code>choco install git -y\n</code></pre> <p>Verify the installation:</p> <pre><code>git --version\n</code></pre> <p>Output:</p> <pre><code>git version 2.43.0.windows.1\n</code></pre> <p>Upgrade Git to the Latest Version</p> <pre><code>choco upgrade git -y\n</code></pre>"},{"location":"developertools/software/windows/#install-chrome","title":"Install Chrome","text":"<p>Chrome is a popular web browser developed by Google. It provides a fast and secure browsing experience and supports various web technologies.</p> <pre><code>choco install googlechrome  -y\n</code></pre> <p>Upgrade Google Chrome</p> <pre><code>choco upgrade googlechrome -y\n</code></pre>"},{"location":"developertools/software/windows/#install-node-js","title":"Install Node JS","text":"<p>Node.js is a JavaScript runtime that allows you to execute JavaScript code outside of a web browser.</p> <pre><code>choco install nodejs -y\n</code></pre> <pre><code>node --version\n# or\nnode -v\nnpm -v\n</code></pre> <p>Upgrade Node.js</p> <pre><code>choco upgrade nodejs -y\n</code></pre>"},{"location":"developertools/software/windows/#install-docker","title":"Install Docker","text":"<p>Docker is a platform that simplifies the process of creating, deploying, and running applications using containers.</p> <pre><code>choco install docker-desktop -y\n</code></pre> <pre><code>docker --version\ndocker compose version\n</code></pre> <p>Upgrade Docker Desktop</p> <pre><code>choco upgrade docker-desktop -y\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-cli","title":"Install Azure CLI","text":"<p>Azure CLI is a command-line interface for managing and interacting with Microsoft Azure cloud services. It provides a convenient way to automate and control your Azure resources.</p> <pre><code>choco install azure-cli -y\n</code></pre> <p>Verify the installation:</p> <pre><code>az --version\n</code></pre> <p>Output:</p> <pre><code>azure-cli                         2.54.0\ncore                              2.54.0\ntelemetry                          1.1.0\n</code></pre> <p>Upgrade Azure CLI</p> <pre><code>choco upgrade azure-cli -y\n</code></pre> <p>Useful Commands</p> <pre><code>az account list --output table\naz account show\naz --help\n</code></pre>"},{"location":"developertools/software/windows/#install-terraform","title":"Install Terraform","text":"<p>Terraform is an infrastructure-as-code tool used for provisioning and managing cloud resources. It allows you to define your infrastructure in code and automates the deployment and management of resources across different cloud providers.</p> <pre><code>choco install terraform -y\n</code></pre> <pre><code>terraform --version\n</code></pre> <p>Upgrade Terraform</p> <pre><code>choco upgrade terraform -y\n</code></pre> <pre><code>terraform -help\n</code></pre>"},{"location":"developertools/software/windows/#install-kubectl","title":"Install kubectl","text":"<p>kubectl is a command-line tool used to interact with Kubernetes clusters. It enables you to deploy, manage, and monitor applications running on Kubernetes.</p> <p><pre><code>choco install kubernetes-cli\n</code></pre> For more information, please refer to the following link: - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/</p> <pre><code># Test to ensure the version you installed is up-to-date:\nkubectl version\nkubectl version --client\n\n# use this for detailed view of version:\nkubectl version --client --output=yaml\n\nor \nkubectl version --output=json\n\n#Verify kubectl configuration\nkubectl cluster-info\n</code></pre> <p>Upgrade kubectl</p> <pre><code>choco upgrade kubernetes-cli -y\n</code></pre>"},{"location":"developertools/software/windows/#install-lens","title":"Install Lens","text":"<p>Lens is a popular Kubernetes platform that serves as a robust and advanced development and management environment for Kubernetes clusters. It provides a comprehensive graphical user interface (GUI) that simplifies the management, monitoring, and interaction with Kubernetes resources and clusters.</p> <pre><code>choco install lens\n</code></pre> <pre><code>lens --version\n</code></pre> <p>Upgrade Lens</p> <pre><code>choco upgrade lens -y\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-kubelogin","title":"Install azure kubelogin","text":"<p>Azure Kubelogin is a tool that enables seamless authentication and access to Azure Kubernetes Service (AKS) clusters using your Azure credentials.</p> <pre><code>choco install azure-kubelogin -y\n</code></pre> <p>Verify the installation:</p> <pre><code>kubelogin --version\n</code></pre> <p>Upgrade Azure Kubelogin:</p> <pre><code>choco upgrade azure-kubelogin -y\n</code></pre>"},{"location":"developertools/software/windows/#install-helm","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes that simplifies the deployment and management of applications on Kubernetes clusters. It allows you to define and install applications using charts, which are packages of pre-configured Kubernetes resources.</p> <pre><code>choco install kubernetes-helm -y\n</code></pre> <pre><code>helm version\n</code></pre> <pre><code>choco upgrade kubernetes-helm -y\n</code></pre>"},{"location":"developertools/software/windows/#install-pgadmin4","title":"Install pgadmin4","text":"<p>pgAdmin is a graphical administration and development platform for PostgreSQL, a popular open-source relational database management system. It provides a user-friendly interface for managing databases, running queries, and monitoring server activity.</p> <pre><code>choco install pgadmin4 -y\n</code></pre> <p>Upgrade pgAdmin 4</p> <pre><code>choco upgrade pgadmin4 -y\n</code></pre>"},{"location":"developertools/software/windows/#install-postgresql","title":"Install PostgreSQL","text":"<p>To install PostgreSQL, you can use Chocolatey by running the following command from the command line or PowerShell:</p> <pre><code>choco install postgresql -y\n</code></pre> <p>Alternatively, you can choose to use the graphical installation wizard for PostgreSQL on Windows.</p> <p>Downloading PostgreSQL</p> <p>Follow the installation wizard instructions to complete the PostgreSQL installation on your Windows system.</p> <p>Note</p> <p>Depending on your requirements, you may want to uncheck the option for <code>Stack Builder</code> during the installation process.</p> <p>psql - Command-line tools</p> <p>By default, the installer does not modify the system path. If you wish to use command-line tools like <code>psql</code>, you will need to manually add PostgreSQL to your system's  path after installation.</p> <p>Update Environment Variables</p> <p>To update the system path and include PostgreSQL in it, add the following directory to your PATH variable:</p> <p><pre><code>C:\\Program Files\\PostgreSQL\\16\\bin\n</code></pre> Make sure to replace 16 with your specific PostgreSQL version if it differs.</p> <p>Note</p> <p>System restart is needed after the installation</p> <p>verify the psql installation</p> <pre><code>psql --version\n\n#output\npsql (PostgreSQL) 16.1\n</code></pre>"},{"location":"developertools/software/windows/#install-argocd","title":"Install argocd","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.</p> <pre><code>choco install argocd -y\n</code></pre> <pre><code>argocd version\n\n# output\nargocd: v2.4.7+81630e6\n  BuildDate: 2022-07-18T21:49:23Z\n  GitCommit: 81630e6d5075ac53ac60457b51343c2a09a666f4\n  GitTreeState: clean\n  GoVersion: go1.18.3\n  Compiler: gc\n  Platform: windows/amd64\nargocd-server: v2.5.2+148d8da\n</code></pre> <p>Upgrade Argo CD</p> <pre><code>choco upgrade argocd -y\n</code></pre>"},{"location":"developertools/software/windows/#install-sql-server","title":"Install SQL server","text":"<p>SQL Server is a relational database management system developed by Microsoft. It allows you to store, manage, and retrieve structured data efficiently.</p> <pre><code>choco install sql-server-management-studio\n</code></pre>"},{"location":"developertools/software/windows/#install-azure-data-studio","title":"Install Azure Data Studio","text":"<p>Azure Data Studio is a cross-platform database tool for data professionals using the Microsoft family of on-premises and cloud data platforms on Windows, MacOS, and Linux.</p> <pre><code>choco install azure-data-studio\n</code></pre>"},{"location":"developertools/software/windows/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<p>Azure Storage Explorer is a standalone application provided by Microsoft that allows users to interact Azure storage account services and making it easier to work with Azure Blob Storage, Azure Queue Storage, Azure Table Storage, and Azure Cosmos DB.</p> <pre><code>choco install microsoftazurestorageexplorer\n</code></pre>"},{"location":"developertools/software/windows/#install-rdcman","title":"Install RDCMan","text":"<p>RDCMan, short for Remote Desktop Connection Manager, is a free Microsoft Windows utility used to manage multiple remote desktop connections from a single application. It is particularly useful to connect and manage multiple remote servers, workstations, or virtual machines.</p> <p>Key Features</p> <ul> <li>Group RDP sessions for easy organization    </li> <li>Save credentials per group or server    </li> <li>Automatically reconnect dropped sessions    </li> <li>Session-wide actions like log off, disconnect, or reconnect    </li> <li>Custom display resolution and settings for individual servers</li> </ul> <p>Option 1: Install via Chocolatey </p> <pre><code>choco install remote-desktop-connection-manager\n\n# To upgrade:\nchoco upgrade remote-desktop-connection-manager\n</code></pre> <p>Option 2: Manual Installation</p> <p>Download RDCMan from the official Microsoft site or trusted repository: *   Microsoft RDCMan Download Page</p>"},{"location":"developertools/software/windows/#mremoteng","title":"mRemoteNG","text":"<p>mRemoteNG (multi-Remote Next Generation) is an open-source. It supports multiple remote connection protocols, making it a powerful alternative to RDCMan with broader capabilities.   </p> <p>Key Features</p> <ul> <li>Tabbed interface for managing multiple sessions    </li> <li>Support for a wide range of protocols - RDP, VNC, SSH, Telnet, ICA, HTTP/HTTPS, RLogin, Raw Socket   </li> <li>Credential inheritance and secure storage    </li> <li>Connection tree and grouping for organization    </li> <li>Import/Export of configuration files</li> </ul> <p>Option 1: Install via Chocolatey (Recommended)</p> <p><pre><code>choco install mremoteng\n\n# To upgrade\nchoco upgrade mremoteng\n</code></pre> Option 2: Manual Installation</p> <p>Download Installer from the official site:     *   https://mremoteng.org/download</p>"},{"location":"developertools/software/windows/#install-dotnet","title":"Install dotnet","text":"<p>Microsoft .NET 8.0 Runtime 8.0.10</p> <pre><code>choco install dotnet-8.0-runtime\n</code></pre> <p>Microsoft .NET 8.0 SDK 8.0.403</p> <pre><code>choco install dotnet-8.0-sdk\n# verify the installation\ndotnet --version\n</code></pre>"},{"location":"developertools/software/windows/#install-python","title":"Install Python","text":"<p>Python is a popular programming language known for its simplicity and versatility. It is widely used for web development, data analysis, and scripting tasks.</p> <pre><code>choco install python\n</code></pre> <p>Verify the installation:</p> <pre><code>python --version\n</code></pre> <p>Upgrade Python:</p> <pre><code>choco upgrade python -y\n</code></pre>"},{"location":"developertools/software/windows/#verify-pip-installation","title":"Verify Pip installation","text":"<pre><code>pip --version\n</code></pre> <p>Output:</p> <pre><code>pip 23.3.1 from C:\\Python312\\lib\\site-packages\\pip (python 3.12)\n</code></pre>"},{"location":"developertools/software/windows/#install-wsl","title":"Install WSL","text":"<p>Windows Subsystem for Linux (WSL) is a feature of Windows that allows developers to run a Linux environment without the need for a separate virtual machine or dual booting. </p> <pre><code>choco install wsl\n</code></pre> <pre><code>wsl --help\n# or\nwsl --list --verbose\n</code></pre> <p>Update WSL</p> <pre><code>wsl --update\n</code></pre>"},{"location":"developertools/software/windows/#install-ubunto","title":"Install Ubunto","text":"<p>Ubuntu on Windows via WSL (Windows Subsystem for Linux) is incredibly useful \u2014 it gives you the power of Linux without leaving Windows.</p> <p>install Ubuntu</p> <pre><code>wsl --install -d Ubuntu\n</code></pre> <p>Check WSL Version</p> <pre><code>wsl --list --verbose\n</code></pre>"},{"location":"developertools/software/windows/#install-ansible","title":"Install Ansible","text":"<p>Ansible is an open-source automation tool used for configuring systems, deploying software, and orchestrating IT infrastructure. It uses simple, human-readable YAML files (called playbooks) and requires no agents on managed machines.</p> <p>install Ansible easily on Ubuntu (on WSL or native Linux)</p> <pre><code># Step 1: Update your package list\nsudo apt update\n# Step 2: Install required dependencies\nsudo apt install -y software-properties-common\n# Step 3: Add the official Ansible PPA (Personal Package Archive)\nsudo add-apt-repository --yes --update ppa:ansible/ansible\n# Step 4: Install Ansible\nsudo apt install -y ansible\n# Step 5: Verify the installation\nansible --version\n</code></pre> <p>Test Ansible with a Simple Command</p> <pre><code>ansible localhost -m ping\n\n# If it's working, you\u2019ll see:\nlocalhost | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"developertools/software/windows/#install-jq","title":"Install JQ","text":"<p>JQ is a lightweight and flexible command-line tool for processing JSON data. It provides various features to extract, manipulate, and transform JSON files efficiently.</p> <pre><code>choco install jq\n</code></pre> <p>Verify the installation:</p> <pre><code>jq --version\n</code></pre> <p>Upgrade JQ:</p> <pre><code>choco upgrade jq -y\n</code></pre>"},{"location":"developertools/software/windows/#install-mongodb-compass","title":"Install MongoDB Compass","text":"<p>Install MongoDB Compass using Chocolatey <pre><code>choco install mongodb-compass -y\n</code></pre> Check MongoDB Version</p> <pre><code>mongod --version\n</code></pre> <pre><code>choco upgrade mongodb-compass -y\n</code></pre>"},{"location":"developertools/software/windows/#install-postman","title":"Install Postman","text":"<p>Postman is a popular API development and testing tool. It allows you to make HTTP requests, test APIs, and automate API workflows, making it easier to develop and debug APIs.</p> <pre><code>choco install postman -y\n</code></pre> <p>Upgrade Postman <pre><code>choco upgrade postman -y\n</code></pre></p>"},{"location":"developertools/software/windows/#install-notepad","title":"Install Notepad++","text":"<pre><code>choco install notepadplusplus -y\n</code></pre> <p>Verify Installation:</p> <pre><code>notepad++\n</code></pre> <p>Upgrade Notepad++:</p> <pre><code>choco upgrade notepadplusplus -y\n</code></pre>"},{"location":"developertools/software/windows/#windows-terminal","title":"Windows Terminal","text":"<p>Allows us to access multiple command-line tools and shells in one customizable interface. It is an open-source project developed and maintained by Microsoft.</p> <pre><code>choco install microsoft-windows-terminal -y\n</code></pre> <p>Upgrade Windows Terminal:</p> <pre><code>choco upgrade microsoft-windows-terminal -y\n</code></pre>"},{"location":"developertools/software/windows/#install-lightshot","title":"Install Lightshot","text":"<p>Lightshot is a free program that offers a quick and easy way to capture a screen including basic editing tools. </p> <p>I don't see brew commands to install this tool but you can install it manually from here:</p> <p>Lightshot downloads</p>"},{"location":"developertools/software/windows/#manual-install","title":"Manual Install","text":"<p>If you would prefer to install manually then here is the same list of software you can download and Install them manually</p> <ul> <li>Visual studio code (recommended) - https://code.visualstudio.com/download/</li> <li>Visual studio (optional) - https://visualstudio.microsoft.com/downloads/</li> <li>SQL Server Management Studio (optional) - https://www.microsoft.com/en-us/sql-server/sql-server-downloads</li> <li>Notepad++ - https://notepad-plus-plus.org/downloads/</li> <li>Google Chrome \u2013 search in google for later version of Google Chrome and download &amp; install it</li> <li>Node JS - https://nodejs.org/en/download/</li> <li>Git - https://git-scm.com/download/win</li> <li>Docker desktop - https://docs.docker.com/desktop/install/windows-install/</li> </ul>"},{"location":"developertools/software/windows/#additional-software","title":"Additional Software","text":"<p>You may need these additional software to perform daily activities.</p> <ul> <li>Zoom - https://zoom.us/download</li> <li>Teams - https://www.microsoft.com/en-us/microsoft-teams/download-app</li> <li>WhatsApp - https://www.whatsapp.com/download</li> <li>SQL Search - https://www.red-gate.com/products/sql-development/sql-search/</li> <li>JSON viewer online - https://codebeautify.org/jsonviewer</li> <li>regexr validator - https://regexr.com/</li> <li>SAML Tracer Browser extension</li> <li>WSL - https://learn.microsoft.com/en-us/windows/wsl/install</li> <li>Azure storage-explorer</li> <li>RDC - https://learn.microsoft.com/en-us/sysinternals/downloads/rdcman</li> <li>base64encode - https://www.base64encode.org/</li> <li>LightShot (prntscr) - https://app.prntscr.com/en/</li> </ul>"},{"location":"developertools/software/windows/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developertools/software/windows/#common-installation-issues","title":"Common Installation Issues","text":"<p>Chocolatey Permission Errors</p> <p>If you encounter permission errors, ensure you're running PowerShell or Command Prompt as Administrator:</p> <ol> <li>Right-click PowerShell/CMD</li> <li>Select \"Run as Administrator\"</li> <li>Run the installation command again</li> </ol> <p>Path/Environment Variable Issues</p> <p>If installed tools are not recognized:</p> <pre><code># Refresh environment variables in current session\nrefreshenv\n\n# Or restart your terminal\n</code></pre> <p>Version Conflicts</p> <p>To check installed versions and resolve conflicts:</p> <pre><code># List all installed packages\nchoco list --local-only\n\n# Force reinstall a package\nchoco install &lt;package&gt; --force -y\n</code></pre> <p>Network/Proxy Issues</p> <p>If downloads fail due to network restrictions:</p> <pre><code># Set proxy for Chocolatey\nchoco config set proxy &lt;proxy-url&gt;\nchoco config set proxyUser &lt;username&gt;\nchoco config set proxyPassword &lt;password&gt;\n</code></pre>"},{"location":"developertools/software/windows/#additional-resources","title":"Additional Resources","text":"<ul> <li>Chocolatey Documentation</li> <li>Troubleshooting Guide</li> <li>Network Tools</li> <li>Kubectl Cheat Sheet</li> <li>Docker Cheat Sheet</li> <li>Helm Cheat Sheet</li> <li>Terraform Cheat Sheet</li> </ul>"},{"location":"developertools/software/workstation/","title":"Developer Workstation Configuration","text":"<p>Building an effective development environment is crucial for productivity, performance, and long-term comfort. This guide provides practical recommendations for assembling a modern developer workstation, whether you're setting up a new machine, upgrading an existing one, or planning remote work infrastructure.</p>"},{"location":"developertools/software/workstation/#hardware-recommendations","title":"Hardware Recommendations","text":""},{"location":"developertools/software/workstation/#processor-cpu","title":"Processor (CPU)","text":"<p>Modern development workloads\u2014compiling code, running containers, virtual machines, and AI models\u2014demand multi-core performance.</p> <p>Recommended Options: - Intel: Core i7-14700K or i9-14900K (14<sup>th</sup> Gen) for maximum performance - AMD: Ryzen 7 7700X or Ryzen 9 7950X for excellent multi-threaded workloads - Apple: M3 Pro or M3 Max for Mac users (exceptional power efficiency)</p> <p>Minimum: 8 cores / 16 threads Recommended: 12+ cores / 24+ threads for containerized development, AI/ML workloads</p>"},{"location":"developertools/software/workstation/#memory-ram","title":"Memory (RAM)","text":"<p>Insufficient RAM is the most common bottleneck for developers running IDEs, browsers, databases, and Docker containers simultaneously.</p> <p>Minimum: 16GB DDR4/DDR5 Recommended: 32GB DDR5 (sweet spot for most developers) Advanced: 64GB+ for AI/ML, large dataset processing, or extensive virtualization</p> <p>Tips: - Prioritize capacity over speed for development workloads - Ensure motherboard supports future RAM upgrades - DDR5 is now the standard for new builds (2025)</p>"},{"location":"developertools/software/workstation/#storage","title":"Storage","text":"<p>Primary Drive (OS + Applications) - NVMe SSD (PCIe 4.0 or 5.0): 500GB minimum, 1TB recommended - Recommended brands: Samsung 990 Pro, WD Black SN850X, Crucial T700 - Speed target: 5,000+ MB/s read for instant IDE loading and fast compilation</p> <p>Secondary Drive (Projects + Data) - NVMe SSD: 1TB-2TB for active projects and Docker images - SATA SSD: Budget-friendly option for older project archives - HDD (Optional): 4TB+ for long-term storage and backups</p> <p>Pro Tip: Keep OS + dev tools on primary drive, active projects on secondary NVMe</p>"},{"location":"developertools/software/workstation/#display-configuration","title":"Display Configuration","text":"<p>Modern development benefits significantly from multi-monitor setups for code, documentation, debugging, and communication.</p> <p>Recommended Setup: - Dual 27\" 4K monitors (3840\u00d72160) for optimal code density and multitasking - Ultrawide 34\"+ (3440\u00d71440) as single-monitor alternative - Minimum resolution: 1920\u00d71080 (1080p) - avoid lower resolutions</p> <p>Display Features to Consider: - IPS or VA panels for accurate colors and wide viewing angles - Matte/anti-glare finish to reduce eye strain - Adjustable stands (height, tilt, pivot) for ergonomic positioning - USB-C with power delivery for single-cable laptop connection - High refresh rate (120Hz+) is nice but optional for coding</p> <p>Budget-Friendly: 2\u00d7 24\" 1080p monitors (~\\(200-300 total)   **Premium:** 2\u00d7 27\" 4K IPS monitors (~\\)800-1200 total)</p>"},{"location":"developertools/software/workstation/#input-devices","title":"Input Devices","text":"<p>Keyboard</p> <p>Developers type millions of keystrokes\u2014invest in comfort and quality.</p> <p>Recommended: - Mechanical keyboards with tactile feedback (Cherry MX Brown, Gateron, or Keychron switches) - Ergonomic split keyboards (Kinesis Advantage, Ergodox, ZSA Moonlander) to prevent RSI - Wireless with long battery life for desk flexibility</p> <p>Popular choices: Keychron Q series, Das Keyboard, Microsoft Ergonomic Keyboard</p> <p>Mouse</p> <ul> <li>Ergonomic design to prevent wrist strain (vertical or contoured)</li> <li>Wireless with Bluetooth + 2.4GHz for flexibility</li> <li>Adjustable DPI (1600-3200 DPI range is ideal)</li> </ul> <p>Alternatives: Trackball (Logitech MX Ergo), Trackpad (Apple Magic Trackpad)</p>"},{"location":"developertools/software/workstation/#workstation-types","title":"Workstation Types","text":""},{"location":"developertools/software/workstation/#desktop-workstation-maximum-performance","title":"Desktop Workstation (Maximum Performance)","text":"<p>Best for: Performance-critical work, upgradability, cost efficiency</p> <p>Advantages: - Best price-to-performance ratio - Upgradeable components (RAM, storage, GPU) - Better cooling for sustained performance - More ports and expansion options</p> <p>Sample Build (Mid-Range - $1,500-2,000): - CPU: AMD Ryzen 7 7700X or Intel i7-14700K - RAM: 32GB DDR5 - Storage: 1TB NVMe SSD + 2TB SATA SSD - Motherboard: B650 (AMD) or Z790 (Intel) - PSU: 650W 80+ Gold modular - Case: Fractal Design Meshify, NZXT H510</p>"},{"location":"developertools/software/workstation/#laptop-workstation-portability-power","title":"Laptop Workstation (Portability + Power)","text":"<p>Best for: Hybrid work, travel, flexibility</p> <p>Recommended Specs (2025): - CPU: Intel Core i7/i9 (13<sup>th</sup>/14<sup>th</sup> Gen) or AMD Ryzen 7/9 (7000 series) or Apple M3 Pro/Max - RAM: 32GB minimum (64GB for AI/ML work) - Storage: 1TB NVMe SSD minimum - Display: 15-16\" with high resolution (2560\u00d71600 or better) - Ports: Thunderbolt 4/USB4 for docking station support</p> <p>Top Picks: - Windows: Dell XPS 15/17, Lenovo ThinkPad P1, HP ZBook Studio - macOS: MacBook Pro 16\" (M3 Pro/Max with 32-64GB RAM) - Linux-friendly: System76 Oryx Pro, Framework Laptop (upgradeable design)</p>"},{"location":"developertools/software/workstation/#software-development-environment","title":"Software &amp; Development Environment","text":""},{"location":"developertools/software/workstation/#operating-system","title":"Operating System","text":"<p>Choose based on your tech stack and team standards:</p> <ul> <li>Windows 11: .NET, Azure, enterprise development; use WSL2 for Linux tooling</li> <li>macOS: iOS/Mac development (required), excellent Unix environment, M-series performance</li> <li>Linux: Server-side development, open-source projects, maximum customization (Ubuntu, Fedora, Arch)</li> </ul>"},{"location":"developertools/software/workstation/#essential-development-tools","title":"Essential Development Tools","text":"<ul> <li>IDE/Editor: VS Code, Visual Studio 2022, JetBrains tools (Rider, IntelliJ, PyCharm)</li> <li>Version Control: Git with GitHub CLI or Azure DevOps</li> <li>Containerization: Docker Desktop, Podman</li> <li>Package Managers: Homebrew (macOS), Chocolatey (Windows), apt/dnf (Linux)</li> <li>Terminal: Windows Terminal, iTerm2, Warp</li> <li>Shell: PowerShell 7+, Zsh with Oh-My-Zsh, Fish</li> </ul> <p>See: Windows Setup Guide | macOS Setup Guide</p>"},{"location":"developertools/software/workstation/#next-steps","title":"Next Steps","text":"<p>Continue your development environment setup with these comprehensive guides and resources:</p> <ul> <li>Windows Developer Setup - Detailed Windows environment configuration</li> <li>macOS Developer Setup - Complete macOS setup guide</li> <li>Network Tools - Essential networking utilities</li> <li>Cheat Sheets - Quick reference guides</li> </ul>"},{"location":"developertools/tools/network-tools/","title":"Useful Tools","text":""},{"location":"developertools/tools/network-tools/#network-troubleshooting","title":"Network troubleshooting","text":"<p>Here are some useful tools and links that can be helpful for troubleshooting network-related issues, monitoring network performance, and analyzing network traffic. </p> Tool/Resource Description Ping Ping is a command-line tool that sends ICMP Echo Request packets to test network connectivity. Traceroute Traceroute is used to trace the route packets take through the internet to a destination host. Wireshark Wireshark is a powerful network protocol analyzer that can capture and inspect network traffic. Netstat Netstat is a command-line tool that displays network connections, routing tables, and interface statistics. Nslookup Nslookup is a command-line tool for querying DNS (Domain Name System) to resolve domain names to IP addresses. Nmap Nmap is a versatile network scanning tool that can discover hosts and services on a network. TCPDump TCPDump is a command-line packet analyzer that captures and displays network traffic in real-time. ipconfig (Windows) / ifconfig (Linux) Display and configure network interfaces on Windows (ipconfig) and Linux (ifconfig). Netcat Netcat, also known as the \"Swiss Army Knife\" of networking, is a versatile networking utility. Fiddler Fiddler is a web debugging proxy that can capture and inspect HTTP/HTTPS traffic between a client and server. GRC ShieldsUP! ShieldsUP! is an online tool for testing the security of your firewall by probing for open ports. MTR (My TraceRoute) MTR combines the functionality of ping and traceroute to provide detailed network path analysis. Speedtest.net Speedtest.net allows you to test your internet connection's speed and latency to various servers. WhatIsMyIP.com WhatIsMyIP.com provides information about your public IP address and geolocation."},{"location":"developertools/tools/network-tools/#dns-troubleshooting","title":"DNS troubleshooting","text":"<p>Here are some useful tools and links for DNS (Domain Name System) troubleshooting:</p> Tool/Resource Description Nslookup Nslookup is a command-line tool for querying DNS to resolve domain names to IP addresses and vice versa. Dig Dig is a command-line tool for querying DNS servers to retrieve DNS records and information. Nslookup Online An online Nslookup tool that provides DNS lookup results for domain names and IP addresses. MXToolBox MXToolBox provides a collection of DNS and network diagnostic tools, including DNS lookup and DNS health checks. DNSQuery.org DNSQuery.org offers online DNS lookup and DNS resolution services for domain names and IP addresses. DNS Checker DNS Checker allows you to check DNS records, DNS propagation, and DNS speed for a domain name. WhatIsMyDNS WhatIsMyDNS provides information about the DNS servers that resolve a specific domain name. IntoDNS IntoDNS checks the health and configuration of your DNS servers and provides a detailed report. DnsViz DnsViz is a tool for visualizing the DNS resolution process and displaying DNSSEC-related information. Google Public DNS Information about using Google Public DNS servers, which can help with DNS resolution issues. Cloudflare DNS Cloudflare offers a fast and privacy-focused DNS resolver that can be used to troubleshoot DNS issues. Quad9 DNS Quad9 provides a secure and privacy-aware DNS service that blocks malicious domains. OpenDNS OpenDNS offers DNS resolution with security features and content filtering capabilities."},{"location":"developertools/tools/network-tools/#other-tools","title":"Other Tools","text":"Tool/Resource Description Password Generator Random Password Generator base64encode Encode to Base64 format JSON Web Tokens JSON Web Tokens ipchicken identifies the IP address of your system"},{"location":"developertools/tools/network-tools/#azure","title":"Azure","text":"Tool/Resource Description Azure Resource Explorer Azure Resource Explorer"},{"location":"personal/breakfast-recipes/","title":"Breakfast Recipes","text":""},{"location":"personal/breakfast-recipes/#upma-recipe","title":"Upma Recipe","text":"<p>Upma! Basic and popular South Indian Breakfast Recipe which is loved by everyone.</p> <p>Ingredients</p> <ul> <li>2 cup Rava | suji or semolina flour</li> <li>\u00bd tsp Asafoetida</li> <li>5 piece Beans</li> <li>1 Carrot, medium</li> <li>4 Chili pepper, Green</li> <li>2 tbsp Coriander</li> <li>2 tsp Ginger</li> <li>2 Onion, medium</li> <li>1 tsp Lemon juice</li> <li>2 Salt,- 1 tsp Turmeric, Powder</li> <li>5 tbsp Vegetable oil</li> <li>1 cup Milk</li> <li>10 piece Cashews, - 1 tsp Split Urad dal</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/breakfast-recipes/#poha-recipe","title":"Poha Recipe","text":"<p>Poha is a most popular breakfast or snack recipe made with flattened rice (poha), onions, roasted peanuts, and a few spices.</p> <p>Ingredients</p> <ul> <li>2 tbsp Coriander, leaves</li> <li>2 Green chilies</li> <li>1 Onion ((about \u00be cup), medium</li> <li>2 tbsp Peanuts</li> <li>1 Potato, medium</li> <li>1 tbsp Lemon juice</li> <li>\u00bd tsp Mustard seeds</li> <li>1 Salt</li> <li>\u00bd tsp Sugar</li> <li>\u00bd tsp Turmeric</li> <li>2 tbsp Oil</li> <li>1 sprig Curry, leaves</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/breakfast-recipes/#uttapam-recipe","title":"Uttapam Recipe","text":"<p>vegetable uttapam recipe | veg uttapam | mixed veggie uttapa</p> <p>Ingredients</p> <ul> <li>1 Carrot</li> <li>1 Chilli</li> <li>2 tbsp Coriander</li> <li>1 inch Ginger</li> <li>1 Onion</li> <li>2 cup Idli rice</li> <li>1 cup Poha / aval / flattened rice, thin</li> <li>\u00bd tsp Methi / fenugreek</li> <li>2 \u00bc tsp Salt</li> <li>1 Oil</li> <li>3 Curry, leaves</li> <li>1 Water (for soaking &amp; grinding)</li> <li>\u00bd capsicum (finely chopped)</li> <li>1 To mato (finely chopped)</li> <li>\u00bd cup urad dal</li> </ul> <p>Reference: - Pintrest:</p>"},{"location":"personal/breakfast-recipes/#mooli-paratha","title":"Mooli Paratha","text":"<p>Make delicious and healthy stuffed mooli parathas with this easy recipe. They go well with yogurt, pickle or a chutney. </p> <p>Ingredients</p> <ul> <li>3 tbsp Coriander, fine leaves</li> <li>\u00bd inch Ginger</li> <li>1 Green chilies</li> <li>4 cups Radish</li> <li>\u00bd tsp Garam masala</li> <li>\u2153 tsp Kashmiri red chili powder</li> <li>\u00bd tsp Salt</li> <li>\u215b tsp Turmeric</li> <li>2 cups Wheat flour</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/dinner-recipes/","title":"Dinner Recipes","text":""},{"location":"personal/dinner-recipes/#dosa-recipe","title":"Dosa Recipe","text":"<p>Dosa is a traditional South Indian crepe/pancake made from fermented rice and dal batter. </p> <p>Ingredients</p> <ul> <li>3 cups Idli rice</li> <li>1 \u00bd tsp Fenugreek seeds</li> <li>2 tsp Salt</li> <li>1 Water</li> <li>\u00be Cup Poha</li> <li>1 Cup Urad dal</li> </ul> <p>Reference: - Pintrest:</p>"},{"location":"personal/dinner-recipes/#idli-recipe","title":"Idli Recipe","text":"<p>Ingredients</p> <ul> <li>2 cup Idli rice or sona masuri rice</li> <li>1 cup Poha / aval / avalakki / flattened rice, thick</li> <li>1 tsp Salt</li> <li>1 Oil</li> <li>1 Water</li> <li>1 cup Urad dal</li> </ul> <p>Reference: - indianhealthyrecipes:</p>"},{"location":"personal/lunch-recipes/","title":"Lunch Recipes","text":""},{"location":"personal/lunch-recipes/#hyderabad-bawarchi-style-mutton-dum-biryani","title":"Hyderabad Bawarchi Style Mutton Dum Biryani","text":"<p>The Special Hyderabadi Mutton Dum Biryani </p> <p>Ingredients</p> <ul> <li>Marinate meat</li> <li>1 Chillies, Green</li> <li>1 Coriander, Powder</li> <li>2 Coriander small bunch, Green</li> <li>1 Juice of a lemon</li> <li>1 Mint - small bunch</li> <li>3 Onions, Medium Size</li> <li>1 Onions, Fried Half</li> <li>1 Curd</li> <li>1 Ginger garlic paste</li> <li>1 Basmati rice</li> <li>1 Cardamoms, Black</li> <li>2 Cinnamon</li> <li>2 Clove</li> <li>2 Garam masala</li> <li>2 Salt</li> <li>1 Turmeric</li> <li>1 Oil</li> <li>1 Cumin, Roasted Powder</li> <li>2 Cumin, Black</li> <li>2 Ghee</li> <li>2 Saffron milk</li> <li>2 Biryani, leaves</li> <li>Mirchi</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#hyderabad-chicken-dum-biryani","title":"Hyderabad Chicken Dum Biryani","text":"<p>The Special Hyderabadi Mutton Dum Biryani </p> <p>Ingredients</p> <ul> <li>1 Chicken</li> <li>1 Chilli, Red Powder</li> <li>1 Chillies, Green</li> <li>1 Coriander, Roasted Powder</li> <li>2 Coriander, Green</li> <li>2 Lemons, Juice </li> <li>1 Little green coriander</li> <li>1 Mint, leaves</li> <li>1 Oil from fried onions</li> <li>1 Oil used to fry onions</li> <li>2 Onions, Big fried</li> <li>1 Onions, Fried</li> <li>1 Curd</li> <li>2 Ginger garlic spread</li> <li>1 Maida flour paste</li> <li>1 Basmati rice</li> <li>2 Cardamom</li> <li>1 Cardamom, Powder</li> <li>2 Cinnamon</li> <li>2 Clove</li> <li>2 Garam masala</li> <li>2 Mace</li> <li>1 Saffron</li> <li>3 Salt</li> <li>1 Star anise</li> <li>2 Cumin, Black</li> <li>1 Cumin, Roasted Powder</li> <li>2 Ghee</li> <li>1 Milk cream</li> <li>1 Biryani, Leaf</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#chicken-curry","title":"Chicken Curry","text":"<p>The Special Hyderabadi Chicken curry recipe </p> <p>Ingredients</p> <ul> <li>500 g Chicken, bone-in</li> <li>1 tsp Coriander, powder</li> <li>3 Onions, medium size</li> <li>3 Tomato puree, fresh medium size</li> <li>1 tbsp Ginger-garlic paste or adrak lahsun paste</li> <li>2 cup Water adjust as per gravy need</li> <li>1 tsp Black pepper, powder</li> <li>2 tsp Fenugreek leaves or kasuri methi., dry</li> <li>2 tsp Garam masala, powder</li> <li>2 tsp Red chili powder</li> <li>1 \u00bd tsp Salt adjust as per need</li> <li>\u00bd tsp Turmeric or haldi, powder</li> <li>2 tsp Lemon juice or vinegar</li> <li>5 tbsp Vegetable oil</li> <li>1 tbsp Cumin seed</li> <li>8 Water soaked cashew nut .optional, hot</li> <li>1 Bay leaf or tej patta</li> <li>4 5 green cardamon or elaichi</li> <li>2 inch Cinnamon or dalchini</li> <li>4 5 cloves or laungh</li> <li>2 tbsp Finely chopped coriander leaves or dhania patta</li> <li>3 Vertically sliced green chilli</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/lunch-recipes/#chicken-65","title":"Chicken 65","text":"<p>The Special Chicken 65 </p> <p>Ingredients</p> <ul> <li>1 \u00be lbs Chicken thighs</li> <li>2 Green chilis</li> <li>2 \u00bd tbsp Ginger garlic spread</li> <li>\u00bc tsp Black pepper</li> <li>2 \u00bd tbsp Corn starch</li> <li>\u00bc tsp Garam masala</li> <li>2 \u2153 tbsp Kashmiri chili powder</li> <li>3 tbsp Rice flour</li> <li>\u00bd tsp Salt</li> <li>\u215b tsp Turmeric</li> <li>1 Neutral oil</li> <li>1 tbsp Olive oil</li> <li>\u00bd tsp Cumin, powder</li> <li>2 sprig Curry, leaves</li> <li>\u00bc cup Dahi</li> </ul> <p>Reference: - Pintrest</p>"},{"location":"personal/yosemite-clouds-rest/","title":"Yosemite clouds rest","text":"<ul> <li>Hike: Clouds Rest (Tenaya Lake / Sunrise Lakes route)</li> <li>Round-trip: ~14.5 miles </li> <li>Elevation gain: ~1,775 ft </li> <li>Summit elevation: ~9,926 ft</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#timeline","title":"Timeline","text":"<ul> <li>04:00 \u2014 Final group check at Yosemite Valley (pack, restrooms, load vehicles).</li> <li>04:30 \u2014 Depart Yosemite Valley.</li> <li>05:30 \u2013 06:00 \u2014 Arrive Sunrise Lakes / Tenaya Lake (Tioga Road) trailhead; final kit check, restroom.</li> <li>06:00 \u2013 06:30 \u2014 Start hike.</li> <li>~10:30 \u2013 11:30 \u2014 Target summit arrival. Lunch / photos (30\u201345 min).</li> <li>11:30 \u2013 15:00 \u2014 Descend to trailhead.</li> <li>~15:30 \u2014 Back to Yosemite Valley or continue onward plans.</li> </ul> <p>Note</p> <p>Driving time Valley \u2192 Tenaya Lake \u2248 ~1 hour depending on traffic and Tioga Road conditions. Confirm Tioga Road seasonal status before leaving.</p>"},{"location":"personal/yosemite-clouds-rest/#route-summary-permits","title":"Route summary &amp; permits","text":"<ul> <li>Trailhead: Sunrise Lakes / Tenaya Lake on Tioga Road.</li> <li>Route: Out-and-back via Sunrise Lakes \u2192 switchbacks \u2192 Clouds Rest ridge \u2192 summit \u2192 return. Ridge has exposed granite and expansive views (Half Dome, Tenaya Canyon).</li> <li>Permits: No wilderness/day-hike permit required for a Clouds Rest day hike. (Overnight/backpacking requires permit.)</li> <li>Roads/season: Tioga Road is seasonal; check current NPS/Yosemite conditions for closures or restrictions before departure.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#personal-checklist","title":"Personal checklist","text":"<ul> <li> Daypack (20\u201330 L)</li> <li> Water: 2.5\u20133 L (minimum 2 L; 3 L recommended at high elevation)</li> <li> Lunch + extra snacks (energy bars, trail mix, nuts)</li> <li> Lightweight lunch bag / ziplock for trash (pack out all trash)</li> <li> Sturdy hiking boots / trail shoes (broken in)</li> <li> Trekking poles (recommended for steep sections)</li> <li> Hat, sunglasses, sunscreen (SPF 30+)</li> <li> Layers: moisture-wicking base, insulating mid-layer, wind/rain shell</li> <li> Headlamp (with fresh batteries)</li> <li> Personal first-aid items (blister care, personal meds)</li> <li> ID, cash / credit card</li> <li> Phone with offline maps preloaded + power bank</li> <li> Lightweight emergency blanket or bivy + whistle</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#group-shared-checklist-distribute-among-group","title":"Group / shared checklist (distribute among group)","text":"<ul> <li> Navigation: printed map or route printout + one device with GPX (designate navigator)</li> <li> Comprehensive first-aid kit \u2014 designate responsible person</li> <li> Emergency shelter / extra emergency blanket(s)</li> <li> Multi-tool + small repair kit (duct tape, patch, needle/thread)</li> <li> Extra 1\u20132 L water (group spare) &amp; basic water-treatment method (filter/drops) \u2014 optional</li> <li> Spare headlamp or spare batteries</li> <li> Trash bag for packing out group waste</li> <li> Bear-aware food storage plan for vehicles (follow NPS rules)</li> <li> Small roll of medical tape, blister supplies</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#safety-reminders-read-aloud-before-starting","title":"Safety reminders (read aloud before starting)","text":"<ul> <li>Start early to avoid afternoon thunderstorms; high country storms develop quickly.</li> <li>Summit altitude is ~9,900 ft \u2014 pace for altitude, hydrate, and rest as needed.</li> <li>Wear traction-capable shoes; granite can be slippery when wet.</li> <li>Cell service is spotty \u2014 do not rely on cell for navigation or rescue. Carry offline maps and tell someone your plan.</li> <li>If anyone shows severe altitude sickness signs (confusion, severe shortness of breath, fainting), descend immediately and seek help.</li> <li>Pack out all trash; follow Leave No Trace principles.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#pre-trip-checks-4824-hours-before","title":"Pre-trip checks (48\u201324 hours before)","text":"<ul> <li> Check Yosemite Current Conditions / Tioga Road status and park alerts.</li> <li> Check weather forecast for Tenaya Lake / Tuolumne Meadows (watch for thunderstorms).</li> <li> Confirm vehicles have fuel and emergency supplies (water, jumper cables, spare tire).</li> <li> Confirm group contact person who is not on the hike and share itinerary (route, start time, vehicle plate).</li> <li> Print or download map/GPS route and verify trailhead coordinates.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#emergency-park-contacts-use-in-an-emergency","title":"Emergency &amp; park contacts (use in an emergency)","text":"<ul> <li>Emergency: 911 (park emergency response).</li> <li>Non-emergency / park dispatch: consult Yosemite NPS current conditions page or your park materials for the local non-emergency number. (Cell service may be limited; plan accordingly.)</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#vehicle-parking-notes","title":"Vehicle &amp; parking notes","text":"<ul> <li>Parking at Sunrise Lakes trailhead is limited; arrive early. If lot is full, find legal roadside parking on Tioga Road and obey all posted signs.</li> <li>If you plan point-to-point logistics, arrange vehicle shuttles in advance. For an out-and-back hike from Sunrise Lakes, a single vehicle is sufficient.</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#optional-group-items-if-wanted","title":"Optional group items (if wanted)","text":"<ul> <li>Lightweight camera / binoculars</li> <li>Small pot / stove for warm drink at summit (consider weight and fire rules)</li> <li>Extra warm layer for summit photo stop</li> </ul>"},{"location":"personal/yosemite-clouds-rest/#printable-instruction","title":"Printable instruction","text":"<p>To print: select this checklist text, paste into any text editor or document app, format as needed, then print on US Letter or A4. If you want, I can reformat this to a single-column PDF optimized for printing (US Letter or A4) and post it here again.</p> <p>Have a safe hike \u2014 if you want I can now:</p> <ol> <li>Reformat to A4 or US Letter single-page PDF for printing (render here).</li> <li>Shorten to a \u00bd-page quick checklist.</li> <li>Add a vehicle packing checklist.</li> </ol> <p>Tell me which and I\u2019ll render it immediately in the chat.</p>"},{"location":"research/","title":"Research &amp; Publications","text":""},{"location":"research/#overview","title":"Overview","text":"<p>Cloud architecture researcher with published peer-reviewed papers in algorithmic trading, distributed systems, and cloud-native messaging architectures. Active research focus on AI/ML integration, high-throughput systems, and next-generation cloud platforms.</p> <p>Research Philosophy: Bridging the gap between theoretical models and practical implementation by conducting research in production environments and sharing actionable insights for enterprise architects and engineers.</p>"},{"location":"research/#published-scholarly-articles","title":"Published Scholarly Articles","text":""},{"location":"research/#integrating-ai-agent-frameworks-with-existing-microservices-an-adapter-based-architecture-for-enterprises-wip","title":"Integrating AI Agent Frameworks with Existing Microservices: An Adapter-Based Architecture for Enterprises (WIP)","text":"<p>Status: Work in Progress - Internal Review Expected Publication: 2025</p> <p>Research Contribution: Novel architectural framework for integrating AI agent systems with existing microservices ecosystems, enabling enterprises to adopt intelligent automation through evolutionary rather than revolutionary modernization. Addresses critical challenges in authentication propagation, distributed tracing, security compliance, and performance optimization while supporting multiple AI agent frameworks (LangChain, Semantic Kernel, Agent Framework).</p> <p>Key Innovations: - Hybrid architecture for AI-microservices integration with standardized adapter patterns - Validated design patterns for service wrapping and agent tool development - Production implementation in financial services sector with 40% reduction in API orchestration complexity - Comprehensive security evaluation confirming SOC 2 and PCI DSS compliance</p> <p>Keywords: AI agents, microservices architecture, enterprise integration, large language models, API gateway, distributed systems, RESTful services, authentication propagation, tool calling, Model Context Protocol, LangChain, Semantic Kernel, financial services, software architecture patterns</p> <p>Read on Site</p>"},{"location":"research/#ai-driven-algorithmic-trading-integrating-machine-learning-hybrid-technical-indicators-and-risk-management-for-momentum-strategies","title":"AI-Driven Algorithmic Trading: Integrating Machine Learning, Hybrid Technical Indicators, and Risk Management for Momentum Strategies","text":"<p>Journal: International Journal of Engineering and Computer Science (IJECS) Volume: Vol. 14 No. 11 (2025) | Published: November 24, 2025 DOI: https://doi.org/10.18535/ijecs.v14i11.5335</p> <p>Research Contribution: Novel integration of supervised machine learning with hybrid technical indicators (VWAP, MACD, RSI, Bollinger Bands) and dynamic risk management for momentum trading. Demonstrates 14% higher returns and 7% lower drawdown compared to conventional rule-based approaches, with Sharpe ratio improvement from 0.8 to 1.7.</p> <p>Key Innovations: - Sub-second latency trading system with 67% accuracy (vs. 54% baseline) - Momentum-specific signal fusion with adaptive weighting - Integrated real-time risk management with momentum-aware controls - Comprehensive data validation across multiple market data providers</p> <p>Keywords: Algorithmic trading, machine learning, momentum trading, technical indicators, risk management, hybrid signals, real-time systems, VWAP, artificial intelligence, quantitative finance</p> <p>Read on Site | Read Full Paper | Download PDF | View DOI</p>"},{"location":"research/#high-throughput-cloud-native-messaging-architectures-design-and-performance-analysis-of-pubsub-microservices-with-kubernetes-and-azure-event-hub","title":"High-Throughput Cloud-Native Messaging Architectures: Design and Performance Analysis of Pub/Sub Microservices with Kubernetes and Azure Event Hub","text":"<p>Journal: International Journal of Advance Research in Computer Science and Management Studies (IJARCSMS) Volume: Vol. 13 Issue 11 | Published: 2025 DOI: https://doi.org/10.61161/ijarcsms.v13i11.1</p> <p>Research Contribution: Comprehensive empirical performance analysis of high-throughput cloud-native messaging architectures using Azure Kubernetes Service (AKS) and Azure Event Hub within a publish-subscribe model. Demonstrates how cloud-native designs effectively handle large-scale data ingestion and real-time event streaming with minimal latency while maintaining 99.9%+ reliability.</p> <p>Key Findings: - Systematic evaluation of scalability, throughput, and latency under varying load conditions - Operational efficiency through Kubernetes auto-scaling and partition-based distribution - Best practices for integrating observability, security, and automation - Reference architecture for mission-critical enterprise applications</p> <p>Keywords: Cloud-Native Architecture, Microservices, Kubernetes, Azure Event Hub, Messaging Architecture, Pub/Sub, Event-Driven Systems, Distributed Systems, High-Throughput Processing</p> <p>Read on Site | Read Full Paper | Download PDF | View DOI</p>"},{"location":"research/#research-interests","title":"Research Interests","text":""},{"location":"research/#primary-research-areas","title":"Primary Research Areas","text":"<ul> <li>Cloud-native architecture patterns and distributed systems design</li> <li>Algorithmic trading systems with AI/ML integration</li> <li>Microservices design and event-driven architectures</li> <li>High-throughput messaging and streaming platforms</li> <li>Real-time data processing at enterprise scale</li> </ul>"},{"location":"research/#emerging-research-focus","title":"Emerging Research Focus","text":"<ul> <li>AI agents for cloud operations and automation</li> <li>Performance optimization of Kubernetes-based systems</li> <li>Integration patterns for Azure OpenAI Service</li> <li>Next-generation platform capabilities (WebAssembly, serverless architectures)</li> </ul>"},{"location":"research/#active-research-projects","title":"Active Research Projects","text":""},{"location":"research/#current-investigations","title":"Current Investigations","text":"<ul> <li>AI-Powered Cloud Infrastructure Automation: Exploring intelligent agents for infrastructure management and optimization</li> <li>High-Performance Event Streaming: Advanced patterns for real-time data processing in financial services</li> <li>Cloud-Native Security: Zero-trust architecture implementations in Kubernetes environments</li> </ul>"},{"location":"research/#research-impact","title":"Research Impact","text":""},{"location":"research/#academic-contributions","title":"Academic Contributions","text":"<ul> <li>Published in peer-reviewed international journals</li> <li>DOI-indexed publications for citation and verification</li> <li>Bridging gap between theoretical models and practical implementation</li> <li>Providing actionable insights for cloud architects and engineers</li> </ul>"},{"location":"research/#industry-applications","title":"Industry Applications","text":"<ul> <li>Real-world performance analysis from production environments</li> <li>Best practices for enterprise-scale deployments</li> <li>Reference architectures for mission-critical systems</li> <li>Compliance-aware designs for regulated industries (financial services)</li> </ul>"},{"location":"research/#future-research-directions","title":"Future Research Directions","text":"<ul> <li>Advanced AI/ML Integration: Next-generation machine learning models for cloud optimization</li> <li>Edge Computing Patterns: Hybrid cloud-edge architectures for real-time processing</li> <li>Sustainable Cloud Computing: Energy-efficient architecture patterns for large-scale systems</li> <li>Financial Technology Innovation: Advanced trading system architectures with regulatory compliance</li> </ul>"},{"location":"research/#collaboration-opportunities","title":"Collaboration Opportunities","text":"<p>Interested in collaborating on research in cloud architecture, distributed systems, or algorithmic trading? </p> <p>Contact Me | LinkedIn</p>"},{"location":"research/publications/ai-agent-microservices-integration/","title":"Integrating AI Agent Frameworks with Existing Microservices: An Adapter-Based Architecture for Enterprises","text":""},{"location":"research/publications/ai-agent-microservices-integration/#authors","title":"Authors","text":"<p>Anji Keesari Cloud Architect  San Ramon, California, USA Email: anjkeesari@gmail.com</p>"},{"location":"research/publications/ai-agent-microservices-integration/#abstract","title":"Abstract","text":"<p>Enterprises face a critical challenge integrating AI agent technologies into established microservices architectures without disrupting operations. Organizations in regulated industries have invested substantially in microservices infrastructure that cannot be easily replaced. This paper presents an architectural framework enabling AI agent integration with existing microservices ecosystems through evolutionary rather than revolutionary modernization.</p> <p>This paper proposes a hybrid three-layer architecture introducing an agent orchestration layer that interacts with microservices through standardized adapter patterns and API gateway integration. The framework addresses enterprise requirements including authentication propagation, distributed tracing, security compliance, and performance optimization while supporting multiple agent platforms and maintaining backward compatibility with existing services.</p> <p>Unlike prior work that focuses primarily on agent runtimes or LLM-to-tool interfaces, this paper targets brownfield enterprise integration: integrating agents into existing microservices while preserving established security, observability, and operational controls.</p> <p>Key contributions include the three-layer integration architecture, a reusable pattern catalog with implementation guidance, and an anonymized, production-inspired case study (TradeSignal, a market-data-driven \u201ctrading research assistant\u201d integrating an agent layer with a microservices backend) with an evaluation methodology.</p> <p>Index Terms\u2014AI agents, microservices architecture, enterprise integration, large language models, API gateway, distributed systems, RESTful services, authentication propagation, tool calling, Model Context Protocol, LangChain, Semantic Kernel, financial services, software architecture patterns.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#1-introduction","title":"1. Introduction","text":""},{"location":"research/publications/ai-agent-microservices-integration/#11-background-and-motivation","title":"1.1 Background and Motivation","text":"<p>Microservices architectures have become the standard for enterprise distributed systems, with organizations investing substantially in these infrastructures to power critical business operations. Concurrently, large language models (LLMs) and AI agent frameworks offer unprecedented opportunities for intelligent automation. AI agents\u2014autonomous systems capable of reasoning, planning, and executing complex tasks through tool use\u2014can interpret natural language instructions, make contextual decisions, and orchestrate multiple services to accomplish high-level objectives.</p> <p>However, enterprises face a critical challenge: existing microservices infrastructures represent substantial investments that cannot be abandoned, yet AI agent frameworks are designed for greenfield development. This disconnect creates barriers to AI adoption, especially in regulated industries where system stability, security, and compliance are paramount. Organizations need practical approaches enabling evolutionary modernization rather than complete system replacement.</p> <p>Regulated industries exemplify this challenge. Financial services platforms, healthcare systems, and government infrastructures rely on mature microservices ecosystems handling sensitive data under strict compliance requirements (SOC 2, PCI DSS, HIPAA). Introducing AI agents requires architectural planning ensuring security, reliability, and regulatory compliance.</p> <p>This paper addresses these challenges through a hybrid architectural framework enabling evolutionary integration of AI agents with existing microservices.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#12-problem-statement","title":"1.2 Problem Statement","text":"<p>Integrating AI agent frameworks with existing microservices architectures presents significant challenges that current research and industry practices have not adequately addressed:</p> <p>Architectural Incompatibility: Traditional microservices communicate through synchronous REST calls or message queues with well-defined contracts. AI agents operate through dynamic tool calling, where the runtime determines function invocation based on natural language reasoning. This fundamental difference creates integration challenges that standard API gateways cannot fully resolve.</p> <p>Security and Compliance: Enterprise API ecosystems implement sophisticated security models (OAuth 2.0, JSON Web Tokens (JWTs), role-based access control (RBAC), and attribute-based access control (ABAC)). AI agents must respect these boundaries while orchestrating calls across multiple services with different authentication requirements. Current frameworks provide limited guidance on enterprise security integration, authentication propagation, and audit logging required for regulatory compliance.</p> <p>Performance Optimization: REST APIs require low-latency responses with strict service-level agreements (SLAs) (e.g., sub-100 ms for critical operations in latency-sensitive domains). AI agents add computational overhead from LLM inference, tool selection, and multi-step reasoning. Organizations need architectural patterns minimizing latency impact while maintaining intelligence benefits, including strategies for caching, request batching, and hybrid execution models.</p> <p>Observability and Schema Management: Traditional distributed tracing tools are designed for request-response patterns, not complex agent decision trees with multiple tool invocations. Additionally, bridging strongly-typed microservice schemas (OpenAPI, Protocol Buffers) with natural language agent interactions requires systematic approaches to schema translation, validation, and error handling.</p> <p>These interconnected challenges require a comprehensive architectural framework addressing them holistically while maintaining compatibility with existing systems.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#13-research-objectives","title":"1.3 Research Objectives","text":"<p>This research develops a practical architectural framework for integrating AI agents with existing microservices in enterprise environments. Specific objectives include: (1) designing a comprehensive framework supporting multiple agent platforms while preserving security, performance, and compliance; (2) identifying reusable design patterns with concrete implementation guidance; (3) developing optimization strategies for latency, security, and observability; and (4) providing systematic implementation guidance enabling organizations to adopt AI agents while maintaining compatibility with existing infrastructure.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#14-contributions","title":"1.4 Contributions","text":"<p>This paper contributes: (1) a three-layer, adapter-based architectural framework enabling non-invasive AI agent integration with existing microservices; (2) seven reusable design patterns with implementation guidance and trade-off analysis addressing authentication propagation, tool registry management, caching strategies, and hybrid orchestration; (3) a systematic implementation approach covering framework selection, automated tool generation from OpenAPI specifications, enterprise security integration, and observability integration; (4) architectural principles supporting evolutionary adoption while preserving existing security models and compliance constraints; and (5) an anonymized, production-inspired case study (TradeSignal, a market-data-driven \u201ctrading research assistant\u201d integrating an agent layer with a microservices backend) plus evaluation methodology to validate feasibility and quantify latency/cost/correctness trade-offs.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#15-scope-and-methodology","title":"1.5 Scope and Methodology","text":"<p>This paper is an architecture and design-pattern contribution grounded in enterprise microservices constraints. We assume existing services expose well-defined APIs (e.g., REST, gRPC) with established authentication and authorization controls, and we focus on integration mechanisms that do not require microservice modification. The scope is the agent-to-microservice integration boundary (tool modeling, adapters/facades, authentication propagation, schema translation, and observability). We do not propose new learning algorithms or LLM architectures; instead, we provide a framework-agnostic integration approach intended to be implemented with multiple agent platforms.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#2-related-work","title":"2. Related Work","text":"<p>This paper builds upon and extends research in three interconnected areas: microservices architecture patterns, AI agent frameworks, and enterprise AI integration strategies. It reviews key contributions in each area and identifies gaps that our framework addresses.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#21-microservices-architecture-and-api-management","title":"2.1 Microservices Architecture and API Management","text":"<p>Foundational work by Newman [1] and Richardson [2] established microservices design principles and pattern languages, focusing primarily on greenfield system design rather than integration with new capabilities. API gateways [3,4] and service meshes (Istio [5], Linkerd [6]) provide sophisticated infrastructure for service communication but are designed for traditional request-response patterns with predefined endpoints. They lack semantic understanding of dynamic, reasoning-based invocation required by AI agents.</p> <p>OpenAPI specifications [7] enable automated API documentation and client generation but assume static, programmatic consumption. This paper extends these concepts by automatically generating agent tool definitions from OpenAPI specifications, bridging structured API contracts with natural language agent interfaces\u2014a gap not addressed by existing API management solutions.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#22-ai-agent-frameworks-and-orchestration","title":"2.2 AI Agent Frameworks and Orchestration","text":"<p>Modern agent frameworks including LangChain [8], AutoGen [9], Semantic Kernel [10], and LangGraph [12] provide abstractions for tool orchestration and multi-step reasoning. LangChain pioneered agent-tool interaction patterns but assumes simple integration scenarios. AutoGen advances multi-agent coordination but targets research scenarios. Microsoft's enterprise frameworks (e.g., Semantic Kernel) emphasize production-readiness but often focus on building new applications rather than integrating with existing architectures.</p> <p>Recent advances in LLM function calling [13,14] improved tool invocation reliability, while standardization efforts like Model Context Protocol (MCP) aim to provide unified tool interfaces. Emerging platforms (Azure AI Foundry) integrate multiple frameworks with enterprise governance. However, these developments address the agent-LLM interface rather than the architectural challenges of integrating agents with production microservices\u2014specifically authentication propagation, distributed tracing across agent reasoning steps, schema bridging between natural language and typed APIs, and compliance enforcement in regulated environments.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#23-enterprise-ai-integration","title":"2.3 Enterprise AI Integration","text":"<p>MLOps literature [15] addresses model deployment and monitoring but focuses on traditional ML rather than autonomous agents. Enterprise AI governance frameworks [16,17] provide responsible AI guidance but are orthogonal to architectural integration challenges. API security research [18] examines protecting ML endpoints from attacks, not enabling agents as authenticated API clients. Industry case studies [19,20] document chatbot implementations with custom point-to-point integrations rather than systematic architectural frameworks.</p> <p>Earlier work on semantic web services and AI planning [21,22] attempted automated service orchestration through formal ontologies but proved too complex for adoption. Modern LLM-based agents make intelligent orchestration practical through natural language understanding but introduce new challenges this paper addresses.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#24-research-gaps-and-our-contribution","title":"2.4 Research Gaps and Our Contribution","text":"<p>Critical gaps remain despite progress in related areas:</p> <p>Brownfield Integration: Existing frameworks assume greenfield development, with minimal guidance for integrating agents with established microservices in regulated industries.</p> <p>Enterprise Security: Current frameworks lack patterns for authentication propagation, authorization enforcement, and audit logging when agents access protected APIs. Security models for human users don't translate directly to autonomous agents.</p> <p>Performance &amp; Observability: Limited research addresses end-to-end latency optimization when agents orchestrate multiple services under enterprise SLAs, or capturing agent reasoning in distributed traces for troubleshooting and compliance.</p> <p>Schema Bridging: The semantic gap between strongly-typed API schemas and natural language agent interactions lacks systematic solutions for translation, validation, and type safety.</p> <p>Cost Optimization: Research on optimizing LLM token costs, implementing intelligent caching, and determining when to use agent vs. traditional orchestration is nascent.</p> <p>Validated Patterns: No comprehensive catalog of design patterns exists for enterprise agent-microservices integration, forcing organizations to discover solutions independently.</p> <p>This paper addresses these gaps through an architectural framework providing integrated solutions spanning architecture, security, performance, and reusable patterns for enterprise adoption.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#3-architectural-framework","title":"3. Architectural Framework","text":"<p>This section presents a hybrid, three-layer architectural framework for integrating AI agent systems with existing microservices. The framework addresses the challenges in Section 1.2 by separating concerns across layers while preserving existing service boundaries.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#31-overview-and-design-principles","title":"3.1 Overview and Design Principles","text":"<p>Our framework introduces a three-layer architecture between client applications and existing microservices (Fig. 1):</p> <p>Layer 1: Agent Orchestration Layer manages agent lifecycle, reasoning, and tool selection. Framework-agnostic, supporting LangChain, Semantic Kernel, and Microsoft Agent Framework through unified abstraction.</p> <p>Layer 2: Integration and Adapter Layer bridges natural language interactions with structured APIs through Tool Registry, Service Adapters, Authentication Bridge, and Schema Transformation components, handling security and data conversions.</p> <p>Layer 3: Existing Microservices Layer represents unchanged enterprise infrastructure, preserving investments while exposing capabilities through standard APIs (REST, gRPC, GraphQL).</p> <p>Cross-Cutting Concerns including security, observability, and performance optimization span all layers with distributed tracing, audit logging, and monitoring providing end-to-end visibility.</p> <p>Key principles: Non-Invasiveness (no microservice modifications), Framework Agnosticism (preventing vendor lock-in), Security by Design (enforcement at every layer), Evolutionary Adoption (incremental integration), and Performance Optimization (caching, batching, hybrid execution).</p> <p>Typical flow: user request \u2192 agent selects tools \u2192 integration layer enforces authn/authz and schema validation \u2192 microservices execute \u2192 integration transforms \u2192 agent synthesizes.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#32-architecture-diagrams","title":"3.2 Architecture Diagrams","text":"<p>Figs. 1\u20133 summarize the system overview, request flow, and a reference deployment model.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#321-system-overview-architecture","title":"3.2.1 System Overview Architecture","text":"<p>Fig. 1. Three-Layer Hybrid Architecture with Cross-Cutting Concerns</p> <p></p> <p>Description: This diagram illustrates the three-layer architecture with clear separation of concerns and integrated cross-cutting capabilities. Client applications interact with the Agent Orchestration Layer, which manages agent lifecycle and reasoning. The Integration Layer bridges agents and microservices through facades, authentication, and schema translation. Existing microservices remain unchanged in Layer 3. Cross-cutting concerns\u2014security, observability, and performance\u2014apply across all layers and are shown separately for readability. The observability tools shown are representative options; in practice, organizations typically adopt either a cloud-native stack (e.g., Azure Monitor/Application Insights) or an open-source stack (e.g., Prometheus/Grafana/Jaeger), with OpenTelemetry for instrumentation.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#322-component-interaction-sequence","title":"3.2.2 Component Interaction Sequence","text":"<p>Fig. 2. Agent Request Processing Flow</p> <p></p> <p>Steps: 1. User submits natural language query 2. Agent queries Tool Registry for available tools 3. Registry returns filtered tool list based on permissions 4. Agent reasons about query and selects appropriate tools (LLM inference) 5. Agent invokes selected tool through Integration Layer 6-7. Integration Layer retrieves authentication token via Auth Bridge 8. Integration Layer calls microservice with auth token 9. Microservice returns response 10. Integration Layer transforms response to agent-friendly format 11. Agent synthesizes final response (LLM inference) 12. Final response returned to user</p>"},{"location":"research/publications/ai-agent-microservices-integration/#323-deployment-architecture","title":"3.2.3 Deployment Architecture","text":"<p>Fig. 3. Single-Region Azure Deployment (Reference)</p> <p></p> <p>Components:</p> <ul> <li>Agent runtime and tools: Agent orchestrator, tool registry, MCP client, MCP tool servers (facades/adapters), and retrieval components (RAG)</li> <li>Core runtime (single region): Azure Front Door, Application Gateway/Ingress, AKS (agent runtime and microservices), Entra ID, Key Vault, ACR, Azure OpenAI (chat + embeddings), Redis, Cosmos DB, and Azure Monitor/Application Insights</li> <li>Delivery and GitOps: Source repo, CI pipeline, build/security scans (SAST/SCA/IaC/secrets + image scan), SBOM/signing, Helm charts, and Argo CD</li> <li>Platform extensions: API Management, service mesh, Azure AI Foundry (Agent Service), Service Bus, Azure Storage, and vector index/search</li> <li>Security posture and policy: Azure Policy / OPA Gatekeeper, Defender for Cloud/Containers, and Sentinel (SIEM)</li> </ul> <p>Note: Not every deployment requires all components shown; the diagram is a reference configuration.</p> <p>Data Flow: 1. Code, IaC, and Helm charts are versioned in a source repository; CI builds containers and runs security checks (SAST/SCA/IaC/secrets, image scanning) and produces signed artifacts/SBOMs (as configured) 2. Images are pushed to ACR; Argo CD syncs Helm releases/manifests from Git into AKS (GitOps) 3. Admission policies (Azure Policy/Gatekeeper) can enforce guardrails at deploy time; Defender for Cloud/Containers provides posture and runtime visibility; Sentinel can centralize security events 4. Users connect to the regional endpoint via Azure Front Door 5. Ingress routes HTTPS traffic to the agent runtime in AKS 6. Agent runtime authenticates/authorizes via Entra ID and uses Azure OpenAI (chat + embeddings) for planning, tool selection, and synthesis 7. Tool discovery and invocation are performed via MCP (tool registry + MCP tool servers), with API Mgmt applying policies where used 8. Retrieval-augmented generation (RAG) queries a vector index/search service; Redis caches frequent retrieval/tool responses 9. Async tasks publish/consume messages via Service Bus (when workflows are long-running) 10. Session state and orchestration metadata are persisted in Cosmos DB; large artifacts can be stored in Azure Storage 11. Secrets and keys are retrieved from Key Vault; images are pulled from ACR to AKS 12. Telemetry flows to Azure Monitor/Application Insights for unified observability; security logs/events can be centralized in Sentinel</p> <p>Figs. 1\u20133 provide a compact reference for the components, interactions, and deployment considerations described in Sections 3.1\u20133.6.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#33-agent-orchestration-layer","title":"3.3 Agent Orchestration Layer","text":"<p>This layer comprises three components:</p> <p>Agent Runtime Environment hosts agent instances and orchestrates reasoning and multi-step workflows (e.g., Reason + Act (ReAct)).</p> <p>Tool Registry and Discovery catalogs tools and metadata (often auto-generated from OpenAPI), supporting permission-filtered discovery and versioning.</p> <p>State Management stores conversation/session state with context pruning and audit trails for troubleshooting and compliance.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#34-integration-and-adapter-layer","title":"3.4 Integration and Adapter Layer","text":"<p>This layer bridges agents and microservices through four key patterns:</p> <p>Service Facade Pattern wraps complex APIs into simplified, agent-optimized tools and normalizes errors and retries.</p> <p>Authentication Bridge Pattern propagates user and service principal identities (JWT/OAuth 2.0), handles refresh, and enforces authorization (RBAC/ABAC) at tool boundaries.</p> <p>Schema Translation and Validation generates tool schemas from contracts (e.g., OpenAPI), validates inputs, normalizes outputs, and manages schema evolution.</p> <p>Caching and Performance Optimization applies caching and batching to reduce repeated calls and stabilize latency.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#35-existing-microservices-and-cross-cutting-concerns","title":"3.5 Existing Microservices and Cross-Cutting Concerns","text":"<p>Existing Microservices Layer remains unchanged, exposing standard APIs (REST, gRPC, GraphQL) and contracts (e.g., OpenAPI). Schema drift is absorbed by the Integration Layer rather than requiring microservice changes.</p> <p>Cross-Cutting Concerns span all layers: Security, Observability, Performance, and Compliance.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#36-data-flow-and-orchestration-patterns","title":"3.6 Data Flow and Orchestration Patterns","text":"<p>The framework supports four orchestration patterns:</p> <p>Synchronous Request-Response for interactive queries: request \u2192 tool calls \u2192 synthesis \u2192 response.</p> <p>Asynchronous Processing for long-running workflows: return job ID, execute in background, publish status, and notify on completion.</p> <p>Hybrid Orchestration routes simple deterministic operations to non-agent paths and reserves agents for multi-step reasoning (based on complexity, latency, cost, and ambiguity).</p> <p>Event-Driven Integration triggers agent actions from microservice events when proactive workflows are needed.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#4-implementation-approach","title":"4. Implementation Approach","text":"<p>This section provides practical guidance for implementing the framework in Section 3. While the architecture is platform-agnostic, we include concrete examples informed by the TradeSignal case study (Section 6).</p>"},{"location":"research/publications/ai-agent-microservices-integration/#41-agent-framework-selection","title":"4.1 Agent Framework Selection","text":"<p>Framework selection impacts development velocity, operational complexity, and maintainability. This section evaluates frameworks against enterprise requirements.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#411-evaluation-criteria","title":"4.1.1 Evaluation Criteria","text":"<p>Organizations should assess agent frameworks based on:</p> <p>Production Readiness: Error handling, logging, telemetry, monitoring, and operational troubleshooting.</p> <p>Enterprise Integration: Authentication/authorization integration (OAuth 2.0, JWT, service principals) and compatibility with enterprise controls.</p> <p>Performance Characteristics: Latency, resource usage, caching, streaming, and controls to reduce redundant LLM calls.</p> <p>Multi-Agent Support: Support for specialized agents, delegation, coordination, and result aggregation.</p> <p>Extensibility: Tool/plugin APIs, customization hooks, and compatibility with domain-specific workflows.</p> <p>Ecosystem and Community: Documentation quality, examples, and third-party integrations.</p> <p>Licensing and Cost: Commercial licensing constraints and total cost of ownership.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#412-framework-comparison","title":"4.1.2 Framework Comparison","text":"<p>LangChain (Python, JavaScript): Large ecosystem and flexible composition; often best for prototyping and highly customized builds.</p> <p>Microsoft Semantic Kernel (.NET, Python, Java): Enterprise-oriented with strong .NET support and production features.</p> <p>Microsoft Agent Framework (Python, .NET): Multi-agent support with task routing and observability; a fit for complex workflows.</p> <p>AutoGen (Python): Research-oriented multi-agent conversations; typically best for experimentation.</p> <p>LangGraph (Python): Stateful graph workflows and human-in-the-loop patterns for explicit control flow.</p> <p>Azure AI Foundry (Multi-language): Managed platform with integrated governance, security, and deployment primitives.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#413-recommendation","title":"4.1.3 Recommendation","text":"<p>For enterprise production deployments, Semantic Kernel, Microsoft Agent Framework, and managed platforms such as Azure AI Foundry are often good fits due to production readiness and governance features. For Python-first teams needing maximum flexibility, LangChain with LangGraph provides a mature alternative.</p> <p>Our TradeSignal trading research assistant case study (Section 6) uses Microsoft Agent Framework due to its multi-agent capabilities and enterprise integration. Framework selection should align with organizational strategy and constraints; the architectural patterns in this paper remain platform-agnostic.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#42-tool-development-and-registration","title":"4.2 Tool Development and Registration","text":"<p>Converting microservices into agent-consumable tools is central to the integration approach.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#421-automated-tool-generation-from-openapi","title":"4.2.1 Automated Tool Generation from OpenAPI","text":"<p>Most modern microservices expose OpenAPI/Swagger specifications. Tool generation can be automated from these contracts:</p> <p>Specification Parsing: Extract operations, parameter/response schemas, and authentication requirements from OpenAPI 3.x.</p> <p>Tool Schema Generation: Convert operations into tool definitions with: - Name: Concise, descriptive identifier (e.g., <code>get_stock_quote</code>, <code>run_backtest</code>) - Description: LLM-oriented intent and usage guidance - Parameters: Typed inputs (required vs. optional) - Authentication: Required authentication method and scopes</p> <p>LLM-Optimized Descriptions: OpenAPI summaries are often too technical; enrich them with user-oriented intent:</p> <pre><code>Original: \"GET /api/marketdata/{symbol}/quote\"\nEnhanced: \"Get the latest quote (price, change, volume). Use when user asks for current price. Requires a valid symbol.\"\n</code></pre> <p>Parameter Simplification: Complex APIs with many optional parameters are simplified into multiple focused tools. For example, a comprehensive search API might be exposed as separate tools for common search patterns (search by name, search by ID, search by date range).</p>"},{"location":"research/publications/ai-agent-microservices-integration/#422-custom-tool-implementation","title":"4.2.2 Custom Tool Implementation","text":"<p>Not all integrations can be fully automated. Custom tool implementation handles:</p> <p>Complex Business Logic: Orchestrating multiple calls and enforcing business rules behind a simplified tool.</p> <p>Data Aggregation: Combining sources, performing calculations, and returning structured summaries.</p> <p>Error Handling and Validation: Retries, circuit breakers, and domain-specific validation.</p> <p>Performance Optimization: Prefetching, caching, and summarization to reduce tool calls and stabilize response times.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#423-tool-organization-and-protocol-standards","title":"4.2.3 Tool Organization and Protocol Standards","text":"<p>Efficient discovery benefits from domain categories (market-data, research, analytics/backtesting), capability tags (read-only, mutating, sensitive), and lightweight usage notes.</p> <p>Standards like Model Context Protocol (MCP) can improve portability. Regardless of protocol choice, enterprise requirements persist (token validation, authorization, audit logging, secure transformation).</p>"},{"location":"research/publications/ai-agent-microservices-integration/#43-security-implementation","title":"4.3 Security Implementation","text":"<p>This section summarizes enterprise security controls applied across the layers.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#431-threat-model-and-trust-boundaries","title":"4.3.1 Threat Model and Trust Boundaries","text":"<p>Assume user input is untrusted even when the runtime environment is controlled. Key threats include prompt injection leading to unsafe tool use, confused-deputy authorization failures, token leakage via telemetry, and data exfiltration through overly broad tool responses. Trust boundaries exist at (i) client \u2192 agent, (ii) agent \u2192 integration/tools, and (iii) integration \u2192 microservices. Mitigations include permission-filtered tool discovery, schema validation, explicit authorization checks for each tool call, and data minimization/redaction in logs and traces.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#432-authentication-architecture","title":"4.3.2 Authentication Architecture","text":"<p>User Authentication Flow (example identity provider: Microsoft Entra ID or Okta): 1. User authenticates to frontend application (OAuth 2.0, OpenID Connect (OIDC)) 2. Frontend receives JWT access token 3. Agent requests include the bearer token 4. Authentication Bridge validates token and extracts user identity 5. Token is propagated to all downstream microservice calls</p> <p>Service Principal Authentication: For scheduled or background agent tasks, service principals (managed identities in Azure, service accounts in Kubernetes) provide authentication without user context. These principals have restricted permissions limited to necessary operations.</p> <p>Token Management: - Cache tokens with expiry awareness and refresh when applicable - Keep tokens in memory only; never log or persist - Invalidate tokens on logout/session termination</p>"},{"location":"research/publications/ai-agent-microservices-integration/#433-authorization-enforcement","title":"4.3.3 Authorization Enforcement","text":"<p>Role-Based Access Control (RBAC): Roles determine which tools are visible and invocable; the Tool Registry filters tools accordingly.</p> <p>Attribute-Based Access Control (ABAC): Enforce fine-grained checks (user, resource, environment) at the Integration Layer before invoking downstream services.</p> <p>Least Privilege: Use minimal scopes/permissions; separate identities for workflows when needed.</p> <p>Authorization Logging: Log allow/deny decisions with sufficient context for audit and incident response.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#434-data-privacy-and-compliance","title":"4.3.4 Data Privacy and Compliance","text":"<p>PII Handling: Personally Identifiable Information (PII) is handled according to regulatory requirements: - Do not log PII in plain text - Encrypt conversation state at rest - Audit access to sensitive records - Enforce retention and deletion policies</p> <p>Data Residency: Ensure processing/storage satisfy regional constraints (e.g., GDPR, CCPA); consider region-specific deployments.</p> <p>Encryption: - All data in transit uses TLS 1.3 - Sensitive data at rest is encrypted using managed keys (Azure Key Vault, AWS KMS) - Conversation state in shared storage is encrypted</p> <p>Compliance Validation: The framework includes compliance validation tools that verify: - All tools enforce authentication - Authorization checks occur before service invocation - Audit logs capture required information - Data handling meets retention and encryption requirements</p>"},{"location":"research/publications/ai-agent-microservices-integration/#44-observability-and-monitoring","title":"4.4 Observability and Monitoring","text":"<p>Observability supports troubleshooting, performance optimization, and compliance verification.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#441-distributed-tracing","title":"4.4.1 Distributed Tracing","text":"<p>OpenTelemetry enables standardized distributed tracing across layers. Spans cover agent execution, tool invocations, microservice calls, and model operations, with correlation IDs and selected attributes (e.g., tool name, token usage, and request identifiers).</p> <p>Traces can be visualized in common backends (open-source or cloud-native) to identify bottlenecks.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#442-structured-logging","title":"4.4.2 Structured Logging","text":"<p>JSON logs with consistent fields (timestamp, level, component, operation, identifiers, status) enable aggregation and analysis. Suggested levels: DEBUG (reasoning breadcrumbs), INFO (operations), WARN (retries/degradation), ERROR (failures/violations).</p> <p>Centralize logs in a searchable backend. Redact/minimize sensitive fields by default.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#443-performance-metrics-and-alerting","title":"4.4.3 Performance Metrics and Alerting","text":"<p>Key metrics include end-to-end latency, tool-call latency, model time/token usage, cache hit rate, error rates, and concurrency. Dashboards support monitoring and capacity planning.</p> <p>Configure alerts for elevated error rates, p95 latency regressions, authentication failures, and resource exhaustion.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#444-agent-decision-logging","title":"4.4.4 Agent Decision Logging","text":"<p>Decision logs capture: user query, tools considered/selected (with brief rationale), invocation parameters, results, and synthesis outcome. Audit trails record who/what/when/why and accessed resources; retention follows applicable regulatory requirements with secure archival.</p> <p>Section 4 outlined implementation considerations; Section 5 catalogs reusable patterns that address common integration challenges.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#5-design-patterns-and-best-practices","title":"5. Design Patterns and Best Practices","text":"<p>This section catalogs reusable patterns and best practices for agent\u2013microservice integration. Each pattern includes intent, motivation, implementation guidance, and trade-offs.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#51-architectural-patterns","title":"5.1 Architectural Patterns","text":""},{"location":"research/publications/ai-agent-microservices-integration/#511-service-facade-pattern","title":"5.1.1 Service Facade Pattern","text":"<p>Intent: Simplify complex microservice APIs into agent-optimized interfaces that reduce cognitive load on LLMs and minimize token usage.</p> <p>Motivation: Microservice APIs can be parameter-heavy and response-dense; agents perform better with smaller, intent-oriented tools.</p> <p>Implementation: Wrap complex APIs into simpler tools with fewer required parameters and concise, structured outputs. The facade can set defaults, select providers, and summarize responses.</p> <p>Benefits: - Reduced token usage and faster responses - Higher tool selection accuracy - Encapsulated business logic</p> <p>Trade-offs: - Additional abstraction and maintenance overhead - Multiple facades may be needed for different use cases</p> <p>When to Use: For complex APIs with &gt;5 parameters or when response data needs significant filtering or transformation.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#512-tool-registry-pattern","title":"5.1.2 Tool Registry Pattern","text":"<p>Intent: Centralize tool definitions, metadata, and access control to enable dynamic tool discovery and permission-based filtering.</p> <p>Motivation: Hardcoded tool lists do not scale across teams, versions, and permission models.</p> <p>Implementation: - Store tool definitions and metadata (including required permissions) - Query by user context and return only authorized tools - Support versioning and controlled updates (e.g., A/B tests)</p> <p>Benefits: - Centralized governance and consistent access control - Easier tool lifecycle management (add/remove/update/version)</p> <p>Trade-offs: - Additional dependency; mitigate with caching and redundancy - Requires disciplined configuration management</p> <p>When to Use: Recommended for deployments with multiple tools (&gt;5) or varying user permissions.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#513-authentication-bridge-pattern","title":"5.1.3 Authentication Bridge Pattern","text":"<p>Intent: Propagate user authentication context through agent workflows to downstream microservices while maintaining security boundaries.</p> <p>Motivation: Agents must call protected APIs on behalf of users without handling credentials unsafely.</p> <p>Implementation: - Capture and validate the user token at ingress - Propagate identity to downstream calls via the integration layer - Use separate service principals/managed identities for background tasks</p> <p>Benefits: - Preserves existing security models and audit trails - Reduces credential exposure to agent logic</p> <p>Trade-offs: - Requires token lifecycle handling (expiry/refresh) - More complex propagation across multi-service workflows</p> <p>When to Use: Recommended when downstream systems enforce authentication (typical in enterprise environments).</p>"},{"location":"research/publications/ai-agent-microservices-integration/#514-hybrid-orchestration-pattern","title":"5.1.4 Hybrid Orchestration Pattern","text":"<p>Intent: Intelligently route requests between agent-based reasoning and traditional API orchestration based on complexity, performance requirements, and cost.</p> <p>Motivation: Route deterministic operations to fast paths and reserve agents for ambiguity and multi-step reasoning.</p> <p>Implementation: - Classify requests by ambiguity, reasoning depth, latency targets, and cost sensitivity - Simple requests \u2192 direct API calls - Complex requests \u2192 agent reasoning - Borderline requests \u2192 restricted tool set or shorter prompts</p> <p>Decision Logic: Use simple rules (and telemetry feedback) to tune routing over time.</p> <p>Benefits: - Reduced cost and latency for simple operations - Flexibility to adapt routing based on load/cost</p> <p>Trade-offs: - Additional complexity and risk of misclassification - Requires monitoring to tune rules</p> <p>When to Use: When cost optimization is important or when significant portion of requests are simple/deterministic.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#515-circuit-breaker-pattern-for-llm-providers","title":"5.1.5 Circuit Breaker Pattern for LLM Providers","text":"<p>Intent: Prevent cascading failures when LLM providers experience outages or degraded performance.</p> <p>Motivation: Rate limits, outages, and latency spikes can cascade into retries and degraded user experience.</p> <p>Implementation: - Monitor LLM provider health (success rate, latency, error types) - States: CLOSED (normal), OPEN (failing, stop requests), HALF-OPEN (testing recovery) - When failure threshold exceeded (e.g., 50% errors over 1 minute), open circuit - While open, immediately fail-fast or route to fallback provider - After timeout (e.g., 30 seconds), enter half-open state - If test requests succeed, close circuit; otherwise reopen</p> <p>Benefits: - Prevents cascading failures and resource exhaustion - Enables fail-fast and fallback routing</p> <p>Trade-offs: - Requires careful threshold tuning and adds integration complexity</p> <p>When to Use: For production systems with strict SLAs or when using multiple LLM providers.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#516-saga-pattern-for-multi-step-workflows","title":"5.1.6 Saga Pattern for Multi-Step Workflows","text":"<p>Intent: Manage distributed transactions across multiple microservices with compensating actions for rollback on failure.</p> <p>Motivation: Multi-step workflows with writes can partially fail and require compensating actions to restore consistency.</p> <p>Implementation: - Break workflow into steps with explicit success criteria - Each step has corresponding compensating action (rollback) - Agent tracks workflow state (steps completed, pending, failed) - On failure, execute compensating actions in reverse order - Final state: either all steps completed or all rolled back</p> <p>Order/Alert Workflow Example: Define compensations (delete, revert derived artifacts, cancellation notices) and execute them in reverse order on failure.</p> <p>Benefits: - Clear recovery path for distributed write workflows - Explicit modeling of complex workflows</p> <p>Trade-offs: - Compensation design is non-trivial; actions must be idempotent - Not all operations have perfect rollback semantics</p> <p>When to Use: For workflows with multiple write operations that must maintain consistency.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#517-agent-specialization-pattern","title":"5.1.7 Agent Specialization Pattern","text":"<p>Intent: Create specialized agents for specific domains rather than single general-purpose agent, improving accuracy and performance.</p> <p>Motivation: As tool count grows, a single general agent can struggle with selection accuracy; specialization narrows the decision space.</p> <p>Implementation: - Define agent personas aligned with user roles or task domains   - Portfolio Analysis Agent (read-only, analysis tools)   - Trading Agent (market data, portfolio views, alerting)   - Strategy Agent (backtesting, risk metrics) - Each agent has curated tool set relevant to domain - Router agent directs user requests to appropriate specialist agent - Specialists can delegate to other specialists when needed</p> <p>Benefits: - Higher tool selection accuracy and faster reasoning - Easier per-domain tuning</p> <p>Trade-offs: - Requires accurate routing and more operational overhead</p> <p>When to Use: When total tool count exceeds 25-30 or when distinct user roles have different tool requirements.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#52-implementation-best-practices","title":"5.2 Implementation Best Practices","text":""},{"location":"research/publications/ai-agent-microservices-integration/#521-gradual-rollout-and-validation","title":"5.2.1 Gradual Rollout and Validation","text":"<p>Start with Read-Only Tools: Begin integration with read-only, non-critical tools to validate architecture with minimal risk. Build confidence before introducing write operations.</p> <p>Shadow Mode Testing: Run agents in parallel with existing systems, comparing agent outputs to traditional methods without exposing to users. Identify discrepancies and refine before production release.</p> <p>Phased Rollout: Start with internal users, expand to a small pilot, then gradually increase exposure. Monitor quality, latency, cost, and incident rates at each stage before expanding.</p> <p>Human-in-the-Loop: For high-stakes operations, require human approval before executing agent actions. Gradually reduce oversight as confidence builds.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#522-prompt-engineering-and-tool-descriptions","title":"5.2.2 Prompt Engineering and Tool Descriptions","text":"<p>Optimize Tool Descriptions for LLMs: - Use clear, action-oriented names and intent-oriented descriptions - Include minimal prerequisites/dependencies and one short usage example - Keep descriptions concise and consistent across tools</p> <p>System Prompts: - Define role/scope and behavioral guidelines - Include safety constraints (verify inputs before mutations; do not guess) - Provide domain context and required terminology</p> <p>Few-Shot Examples: Add a small number of high-quality examples when tool misuse is common.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#523-intelligent-caching-strategies","title":"5.2.3 Intelligent Caching Strategies","text":"<p>Multi-Level Caching: - L1 - Tool Definitions: Cache tool schemas in agent runtime (rarely change) - L2 - LLM Responses: Cache identical prompts (works for common queries) - L3 - Microservice Responses: Cache read-only API responses with appropriate time-to-live (TTL) values</p> <p>Cache Key Design: Include user identity, query parameters, and timestamps in cache keys to prevent unauthorized access to cached data.</p> <p>Selective Caching: Only cache deterministic, stable data. Do not cache: - User-specific sensitive data beyond session lifetime - Real-time data (live market prices) - Intermediate workflow state</p> <p>Cache Invalidation: Implement time-to-live (TTL)-based expiration and explicit invalidation for write operations affecting cached data.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#524-error-handling-and-recovery","title":"5.2.4 Error Handling and Recovery","text":"<p>Graceful Degradation: Provide a user-friendly failure message, suggest an alternative action, and avoid exposing internal error details.</p> <p>Automatic Retries: Implement exponential backoff for transient failures (network issues, rate limits).</p> <p>Error Context: Log tool name, sanitized parameters, user/session identifiers, and trace IDs for debugging.</p> <p>Fallback Flows: For critical operations, maintain a non-agent path (e.g., traditional UI).</p>"},{"location":"research/publications/ai-agent-microservices-integration/#525-performance-optimization","title":"5.2.5 Performance Optimization","text":"<p>Parallel Tool Invocation: When agent needs multiple independent tools, invoke them in parallel rather than sequentially.</p> <p>Response Streaming: Stream LLM responses to users for perceived performance improvement, especially for longer responses.</p> <p>Prompt Optimization: Minimize prompt tokens: - Keep only relevant conversation history - Summarize long tool responses before re-injecting - Use smaller models for simple tasks</p> <p>Connection Pooling: Reuse HTTP connections to microservices to reduce connection overhead.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#53-common-pitfalls-and-solutions","title":"5.3 Common Pitfalls and Solutions","text":""},{"location":"research/publications/ai-agent-microservices-integration/#531-token-context-window-limitations","title":"5.3.1 Token Context Window Limitations","text":"<p>Problem: LLMs have finite context windows (model-dependent). Long conversations or large tool responses can exhaust available context.</p> <p>Solutions: - Summarize and prune: Periodically summarize history and retain only task-relevant turns - Filter tool outputs: Return only relevant fields (avoid full payloads) - Monitor context use: Track context growth and apply backpressure when approaching limits</p>"},{"location":"research/publications/ai-agent-microservices-integration/#532-hallucination-and-accuracy-issues","title":"5.3.2 Hallucination and Accuracy Issues","text":"<p>Problem: LLMs may generate plausible but incorrect information, especially for numbers, dates, or specific facts.</p> <p>Solutions: - Ground and verify: Require tool-backed facts and add explicit verification steps for critical actions - Constrain outputs: Prefer structured outputs (e.g., function calling/JSON mode) - Escalate risk: Require human approval for high-stakes decisions</p>"},{"location":"research/publications/ai-agent-microservices-integration/#533-cost-management","title":"5.3.3 Cost Management","text":"<p>Problem: LLM API costs can escalate quickly with high usage or inefficient prompting.</p> <p>Solutions: - Monitor and cap: Track token usage and enforce budgets/rate limits - Route by complexity: Use cheaper models for simple tasks and larger models for complex reasoning - Reduce calls: Cache common queries and avoid redundant LLM/tool invocations</p>"},{"location":"research/publications/ai-agent-microservices-integration/#534-security-and-compliance","title":"5.3.4 Security and Compliance","text":"<p>Problem: Agents may inadvertently expose sensitive data or enable unauthorized operations.</p> <p>Solutions: - Least privilege and redaction: Minimize permissions and redact sensitive fields in logs/caches - Authorize every call: Enforce authorization at each tool invocation, not only at ingress - Audit and review: Maintain audit trails and conduct regular security testing</p>"},{"location":"research/publications/ai-agent-microservices-integration/#535-tool-selection-accuracy","title":"5.3.5 Tool Selection Accuracy","text":"<p>Problem: Agents select wrong tools or misuse tools, especially as tool count increases.</p> <p>Solutions: - Reduce ambiguity: Use clear naming, categories, and concise, intent-oriented tool descriptions - Constrain choices: Use specialization (or restricted tool sets) as tool count grows - Validate inputs: Validate and normalize parameters before invocation</p>"},{"location":"research/publications/ai-agent-microservices-integration/#536-latency-and-user-experience","title":"5.3.6 Latency and User Experience","text":"<p>Problem: Multi-step agent reasoning introduces latency (often seconds), which may feel slow for users accustomed to instant responses.</p> <p>Solutions: - Set expectations and show progress: Provide progress indicators and stream partial results when feasible - Route and defer: Use hybrid routing for fast paths and async execution for long-running workflows</p> <p>Section 6 presents an anonymized case study and evaluation approach; Section 7 summarizes challenges and limitations.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#6-case-study-and-evaluation","title":"6. Case Study and Evaluation","text":"<p>This section summarizes an implementation case study used to validate the feasibility of the proposed architecture and patterns. We base the discussion on TradeSignal, a production-inspired, market-data-driven \u201ctrading research assistant\u201d system that integrates an AI agent layer with a microservices-based backend. To keep the case study reusable and appropriate for publication, we anonymize external provider identities and endpoints, and focus on architectural decisions and measurable system properties rather than vendor-specific integrations.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#61-case-study-context-anonymized","title":"6.1 Case Study Context (Anonymized)","text":"<p>The system provides a conversational interface for market research and analysis. Users submit natural language queries (e.g., \u201cGet the latest quote for a symbol\u201d, \u201cSummarize recent news\u201d, \u201cRun a simple backtest\u201d), and the agent orchestrates calls to internal services and external data providers.</p> <p>External dependencies are abstracted behind adapters and described generically:</p> <ul> <li>Broker API: OAuth 2.0-protected trading/brokerage integration (token acquisition, refresh, and scoped access).</li> <li>Market Data Provider A/B: Quote and chart retrieval APIs used with fallback routing.</li> <li>News/Sentiment Provider: News aggregation and sentiment inputs.</li> </ul>"},{"location":"research/publications/ai-agent-microservices-integration/#62-mapping-to-the-proposed-architecture","title":"6.2 Mapping to the Proposed Architecture","text":"<p>The implementation aligns with the three-layer framework (Section 3):</p> <ul> <li>Agent orchestration layer: A single-agent deployment can handle basic tool calling; a multi-agent specialization approach decomposes responsibilities into (i) real-time market data retrieval, (ii) computational strategy analysis/backtesting, (iii) news/research synthesis, and (iv) a coordinator agent for routing and aggregation.</li> <li>Integration and adapter layer: A service layer mediates tool invocations, applies schema validation/translation, enforces authorization checks, and implements caching. Provider-specific logic is isolated into adapters, enabling provider swapping without changing agent prompts or tool schemas.</li> <li>Existing services and data layer: The backend persists user and analysis artifacts in a relational store and uses an in-memory cache for latency-sensitive reads.</li> </ul> <p>In TradeSignal, this mapping corresponds to (i) a managed agent runtime (cloud-hosted) for orchestration, (ii) a microservice-facing API layer that hosts adapters/facades and caching, and (iii) standard backing services for persistence and low-latency reads. Provider anonymization is handled at the adapter boundary so prompts, tool schemas, and business logic remain stable when providers change.</p> <p>Cross-cutting concerns (Section 3) were incorporated as follows:</p> <ul> <li>Security: OAuth 2.0 token handling for external calls; enterprise identity integration for platform authentication; and least-privilege authorization checks at tool boundaries (not only at API ingress).</li> <li>Performance: Short time-to-live (TTL) caching for high-frequency reads (quotes/movers), request batching where possible, and model selection strategies (smaller models for retrieval/summarization; larger models for multi-step quantitative reasoning).</li> <li>Operations: Containerized deployment with local orchestration and a Kubernetes (Helm) path for cloud deployment; health probes for readiness/liveness.</li> </ul>"},{"location":"research/publications/ai-agent-microservices-integration/#63-evaluation-methodology","title":"6.3 Evaluation Methodology","text":"<p>The primary goal of this evaluation is not to claim universal benchmark numbers, but to demonstrate (i) architectural feasibility in a realistic microservices setting and (ii) measurable trade-offs across latency, cost, and correctness under representative workloads.</p> <p>We recommend reporting results under two workload classes:</p> <ol> <li>Retrieval-oriented queries (single or few tool calls): quote lookups, top movers, simple news summaries.</li> <li>Analysis-oriented queries (multi-step reasoning and/or CPU-bound tasks): backtests, portfolio/risk analysis, multi-signal comparisons.</li> </ol> <p>Suggested metrics (report medians and tail latency):</p> <ul> <li>End-to-end latency (\\(p50\\), \\(p95\\)) per workload class</li> <li>Tool-call latency by tool category (internal service vs. external provider adapter)</li> <li>Cache hit rate and impact on latency for retrieval workloads</li> <li>Cost per query (token-based LLM cost + compute cost for CPU-bound analysis)</li> <li>Correctness proxies: schema validation pass rate; tool selection accuracy on a labeled query set; and regression tests for deterministic analytics</li> </ul> <p>Evaluation Setup (Recommended Reporting Format):</p> Workload Class Representative Queries Primary Components Exercised Metrics to Report Measurement Notes Retrieval-oriented Quote lookup, top movers, short news summary Agent runtime, tool registry, adapter layer, cache, external provider adapter \\(p50/p95\\) latency, tool-call latency, cache hit rate, token usage Report cold-cache vs. warm-cache; include provider fallback rate as an operational indicator Analysis-oriented Backtest, portfolio/risk analysis, multi-signal comparison Agent runtime, orchestrator/specialists, deterministic compute service, persistence \\(p50/p95\\) latency, compute time vs. LLM time split, cost per query, error rate Separate deterministic compute runtime from LLM inference; include timeout/retry policy configuration <p>For both workload classes, collect measurements using end-to-end request tracing (e.g., OpenTelemetry spans) and structured logs with correlation IDs. Report the workload generator characteristics (request rate, concurrency, duration) and deployment conditions (region placement, instance sizes, and model selection policy) to keep results comparable.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#64-evaluation-notes-and-threats-to-validity","title":"6.4 Evaluation Notes and Threats to Validity","text":"<p>To keep claims defensible for journal review, the following should be stated explicitly when reporting results:</p> <ul> <li>Results are deployment-dependent (model choice, region placement, network conditions, and provider rate limits materially affect latency and throughput).</li> <li>Caching can inflate throughput for retrieval workloads; report both cold-cache and warm-cache runs.</li> <li>For analysis workloads, separate LLM inference time from deterministic compute time (e.g., backtest engine runtime).</li> <li>Provider anonymization can reduce reproducibility; mitigate by reporting adapter behavior and interface contracts (schemas) rather than vendor-specific endpoints.</li> </ul>"},{"location":"research/publications/ai-agent-microservices-integration/#7-challenges-and-limitations","title":"7. Challenges and Limitations","text":"<p>This section summarizes key challenges, limitations, and directions for future work.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#71-technical-challenges","title":"7.1 Technical Challenges","text":""},{"location":"research/publications/ai-agent-microservices-integration/#711-llm-latency-and-cost-trade-offs","title":"7.1.1 LLM Latency and Cost Trade-offs","text":"<p>Challenge: LLM inference adds latency and cost relative to direct API calls. Multi-step reasoning can feel slow for interactive workflows, and larger models can improve accuracy at higher cost.</p> <p>Current Mitigation: Hybrid orchestration, caching, response streaming, and model selection based on task complexity help but cannot eliminate the fundamental latency floor.</p> <p>Future Direction: Faster inference, distillation, and local deployment may reduce latency; speculative execution can pre-fetch likely data to hide round-trips.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#712-context-window-management","title":"7.1.2 Context Window Management","text":"<p>Challenge: Multi-step conversations can exhaust context windows as history, tool schemas, and tool responses accumulate, forcing summarization or restarts.</p> <p>Current Mitigation: Summarization, selective retention, and response filtering reduce context use but may omit details.</p> <p>Future Direction: Larger-context models and external memory (e.g., vector databases, knowledge graphs) can offload long-term context.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#713-tool-selection-accuracy","title":"7.1.3 Tool Selection Accuracy","text":"<p>Challenge: Tool selection remains imperfect, especially for ambiguous requests or overlapping tools.</p> <p>Current Mitigation: User confirmation for high-stakes operations, fallback to human assistance, and continuous refinement of tool descriptions.</p> <p>Future Direction: Fine-tuning on organization-specific tool usage and reinforcement learning from human feedback (RLHF) can improve selection accuracy.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#714-multi-service-transaction-consistency","title":"7.1.4 Multi-Service Transaction Consistency","text":"<p>Challenge: Consistency across multi-service write workflows is complex, and compensating actions may be imperfect (e.g., reversing trades after market movement).</p> <p>Current Mitigation: Saga pattern with compensating transactions, human approval for high-value operations, and explicit transaction boundaries.</p> <p>Future Direction: Stronger coordination (e.g., two-phase commit) may improve consistency at a performance and availability cost.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#715-real-time-data-requirements","title":"7.1.5 Real-Time Data Requirements","text":"<p>Challenge: For rapidly changing domains, data can become stale between reasoning and execution.</p> <p>Current Mitigation: Refresh data at execution time, not reasoning time. Display timestamps indicating freshness.</p> <p>Future Direction: Event-driven architectures and streaming tool responses can reduce staleness.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#72-organizational-challenges","title":"7.2 Organizational Challenges","text":""},{"location":"research/publications/ai-agent-microservices-integration/#721-skills-gap-and-training","title":"7.2.1 Skills Gap and Training","text":"<p>Challenge: Many teams lack experience with LLM-based systems (prompting, context management, and hallucination mitigation), slowing delivery.</p> <p>Recommendations: Provide training, seed the team with specialists early, and document internal best practices.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#722-change-management-and-user-adoption","title":"7.2.2 Change Management and User Adoption","text":"<p>Challenge: Users may resist conversational interfaces, and reliability concerns can slow adoption in critical domains.</p> <p>Recommendations: Roll out gradually, measure user outcomes, and offer hybrid interfaces alongside conversational entry points.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#723-governance-and-accountability","title":"7.2.3 Governance and Accountability","text":"<p>Challenge: Accountability for agent errors can be unclear across model, framework, tool, and user responsibilities.</p> <p>Recommendations: Define governance and escalation paths, maintain audit trails, and keep humans in the loop for critical actions.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#73-limitations-and-future-research-directions","title":"7.3 Limitations and Future Research Directions","text":""},{"location":"research/publications/ai-agent-microservices-integration/#731-framework-generalizability","title":"7.3.1 Framework Generalizability","text":"<p>Limitation: Broader validation across industries and organizational contexts would strengthen generalizability.</p> <p>Future Research: Cross-domain case studies and comparative evaluations could identify industry-specific adaptations.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#732-advanced-reasoning-capabilities","title":"7.3.2 Advanced Reasoning Capabilities","text":"<p>Limitation: Agents excel at orchestration and synthesis but still struggle with deep domain reasoning and mathematical optimization.</p> <p>Future Research: Hybrid designs combining LLMs with specialized models (e.g., optimization engines and predictors) could expand capabilities.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#733-long-running-autonomous-workflows","title":"7.3.3 Long-Running Autonomous Workflows","text":"<p>Limitation: Long-running autonomous agents (hours/days) remain challenging due to error accumulation and context drift.</p> <p>Future Research: Persistent agents with checkpoint/resume, external memory, and multi-day context management are promising directions.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#734-multi-modal-capabilities","title":"7.3.4 Multi-Modal Capabilities","text":"<p>Limitation: The framework primarily targets text interactions; many enterprise artifacts require visual understanding.</p> <p>Future Research: Integration with vision-capable multi-modal models and specialized chart/diagram generation tools could enable richer interactions.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#735-explainability-and-transparency","title":"7.3.5 Explainability and Transparency","text":"<p>Limitation: While decisions can be logged, explaining why an agent made a choice remains challenging due to model opacity.</p> <p>Future Research: Explainable AI techniques adapted to LLM-based agents, counterfactual analysis, and causality-based explanations could improve transparency.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#736-adversarial-robustness","title":"7.3.6 Adversarial Robustness","text":"<p>Limitation: Beyond basic prompt injection, sophisticated adversarial attacks remain a concern.</p> <p>Future Research: Red team exercises with advanced attackers, formal verification methods for agent behaviors, and adversarial training could harden agents.</p> <p>These limitations set expectations for adoption and motivate future work.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#8-conclusion","title":"8. Conclusion","text":""},{"location":"research/publications/ai-agent-microservices-integration/#81-summary-of-contributions","title":"8.1 Summary of Contributions","text":"<p>This paper presented an architectural framework for integrating AI agent systems with existing enterprise microservices while preserving established service boundaries and investments.</p> <p>Our key contributions include:</p> <p>1. Architectural Framework: A layered, hybrid architecture comprising the Agent Orchestration Layer, Integration and Adapter Layer, and cross-cutting security and observability frameworks. The architecture enables integration between AI agents and microservices while maintaining security boundaries, performance requirements, and compliance standards. Unlike prior work focusing on greenfield agent development, our framework explicitly addresses brownfield integration with existing systems.</p> <p>2. Design Pattern Catalog: Seven reusable design patterns\u2014Service Facade, Tool Registry, Authentication Bridge, Hybrid Orchestration, Circuit Breaker for LLM Providers, Saga for Multi-Step Workflows, and Agent Specialization\u2014provide proven solutions to common integration challenges. Each pattern includes implementation guidance, trade-off analysis, and applicability criteria.</p> <p>3. Implementation Guidance: Systematic guidance on agent framework selection, automated tool generation from OpenAPI specifications, enterprise security integration, and comprehensive observability patterns. Detailed best practices and common pitfalls with solutions enable organizations to accelerate implementation.</p> <p>4. Non-Invasive Integration Principles: The framework preserves existing microservices unchanged, enabling evolutionary adoption without disrupting operations. This de-risks AI integration for enterprises with substantial infrastructure investments.</p> <p>Together, these contributions address gaps in brownfield integration patterns, security propagation, observability, schema bridging between natural language and typed APIs, and reusable design guidance.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#82-practical-implications","title":"8.2 Practical Implications","text":"<p>Practical implications for enterprise adoption include:</p> <p>Accelerated AI Adoption: Organizations can adopt AI agent technologies without prohibitive cost and risk of wholesale system replacement. Evolutionary integration enables gradual rollout, building confidence before large-scale commitment.</p> <p>Reduced Implementation Risk: Validated patterns and practices reduce trial-and-error experimentation. Organizations can leverage proven approaches, avoiding common pitfalls and accelerating time-to-value.</p> <p>Democratized AI Capabilities: The framework enables organizations without deep AI expertise to deploy agent-based automation. Systematic tool generation from OpenAPI specifications, standardized security integration, and comprehensive observability lower the expertise barrier.</p> <p>Enterprise-Grade Quality: By addressing security, compliance, performance, and observability up front, the framework supports production deployments, including regulated environments.</p> <p>Foundation for Innovation: New capabilities (multi-modal inputs, advanced reasoning, domain-specific models) can be incorporated within the same integration boundaries.</p> <p>Industry Applicability: While technology-agnostic, the patterns generalize across domains with similar integration constraints.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#83-future-work","title":"8.3 Future Work","text":"<p>Promising future directions include:</p> <p>Multi-Modal Integration: Support for documents, charts, and images to expand use cases beyond text.</p> <p>Federated Agent Architectures: Specialized agents collaborating across organizational and security boundaries.</p> <p>Continuous Learning: Learning from usage patterns and feedback (including RLHF) to improve tool selection and orchestration.</p> <p>Domain-Specific Frameworks: Industry-specific variants that encode compliance constraints and common workflow patterns.</p> <p>Advanced Reasoning Integration: Combining agents with specialized models (optimization, prediction, causal inference) for decision support.</p> <p>Edge and Hybrid Deployments: Patterns for on-premises/air-gapped deployments and cloud-edge trade-offs.</p> <p>Cross-Organizational Agent Collaboration: B2B collaboration with explicit trust, data sharing, and security boundaries.</p> <p>Unified Agent Platforms: How platform-managed agent stacks can adopt these patterns while preserving portability.</p> <p>Empirical Validation: Cross-domain evaluations to strengthen generalizability claims.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#84-closing-remarks","title":"8.4 Closing Remarks","text":"<p>Integrating AI agents with existing microservices provides a pragmatic path to modernization: evolutionary adoption can deliver automation benefits while preserving stability and prior investments.</p> <p>As AI capabilities evolve, the core principles\u2014layered integration, security by design, evolutionary adoption, and production-grade operations\u2014remain stable.</p> <p>The architecture diagrams, patterns, and implementation guidance provide a concrete starting point for practitioners and a basis for further research.</p> <p>Rather than replacing microservices, the goal is to combine reliable services with agent-driven orchestration where it adds value.</p>"},{"location":"research/publications/ai-agent-microservices-integration/#references","title":"References","text":"<p>[1] S. Newman, Building Microservices: Designing Fine-Grained Systems, 2<sup>nd</sup> ed. O'Reilly Media, 2021.</p> <p>[2] C. Richardson, Microservices Patterns: With Examples in Java. Manning Publications, 2018.</p> <p>[3] R. T. Fielding, \"Architectural styles and the design of network-based software architectures,\" Ph.D. dissertation, University of California, Irvine, 2000.</p> <p>[4] M. Amundsen, RESTful Web API Patterns and Practices. O'Reilly Media, 2022.</p> <p>[5] \"Istio: Service Mesh,\" Istio Authors, 2023. [Online]. Available: https://istio.io. [Accessed: Jan. 2026].</p> <p>[6] \"Linkerd: Ultralight Service Mesh,\" Linkerd Authors, 2023. [Online]. Available: https://linkerd.io. [Accessed: Jan. 2026].</p> <p>[7] \"OpenAPI Specification v3.1.0,\" OpenAPI Initiative, 2021. [Online]. Available: https://spec.openapis.org/oas/v3.1.0. [Accessed: Jan. 2026].</p> <p>[8] H. Chase, \"LangChain: Building applications with LLMs through composability,\" GitHub repository, 2022. [Online]. Available: https://github.com/langchain-ai/langchain. [Accessed: Jan. 2026].</p> <p>[9] Q. Wu et al., \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,\" arXiv preprint arXiv:2308.08155, 2023.</p> <p>[10] Microsoft, \"Semantic Kernel: Integrate cutting-edge LLM technology quickly and easily into your apps,\" GitHub repository, 2023. [Online]. Available: https://github.com/microsoft/semantic-kernel. [Accessed: Jan. 2026].</p> <p>[11] Microsoft, \"Azure AI Foundry: Agent Service,\" Microsoft Azure Documentation, 2024. [Online]. Available: https://learn.microsoft.com/azure/ai-studio/. [Accessed: Jan. 2026].</p> <p>[12] LangGraph Contributors, \"LangGraph: Build stateful, multi-actor applications with LLMs,\" GitHub repository, 2024. [Online]. Available: https://github.com/langchain-ai/langgraph. [Accessed: Jan. 2026].</p> <p>[13] OpenAI, \"Function calling and other API updates,\" OpenAI Blog, Jun. 2023. [Online]. Available: https://openai.com/index/function-calling-and-other-api-updates/. [Accessed: Jan. 2026].</p> <p>[14] J. Schick et al., \"Toolformer: Language Models Can Teach Themselves to Use Tools,\" arXiv preprint arXiv:2302.04761, 2023.</p> <p>[15] D. Sculley et al., \"Hidden Technical Debt in Machine Learning Systems,\" in Proc. 28<sup>th</sup> Int. Conf. Neural Information Processing Systems (NIPS), 2015, pp. 2503-2511.</p> <p>[16] R. Bellamy et al., \"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias,\" IBM Journal of Research and Development, vol. 63, no. \u2158, pp. 4:1-4:15, 2019.</p> <p>[17] B. Goodman and S. Flaxman, \"European Union Regulations on Algorithmic Decision-Making and a 'Right to Explanation',\" AI Magazine, vol. 38, no. 3, pp. 50-57, 2017.</p> <p>[18] N. Carlini and D. Wagner, \"Towards Evaluating the Robustness of Neural Networks,\" in Proc. IEEE Symposium on Security and Privacy (SP), 2017, pp. 39-57.</p> <p>[19] A. Xu, Z. Liu, Y. Guo, V. Sinha, and R. Akkiraju, \"A New Chatbot for Customer Service on Social Media,\" in Proc. CHI Conference on Human Factors in Computing Systems, 2017, pp. 3506-3510.</p> <p>[20] J. Hill, W. R. Ford, and I. G. Farreras, \"Real conversations with artificial intelligence: A comparison between human\u2013human online conversations and human\u2013chatbot conversations,\" Computers in Human Behavior, vol. 49, pp. 245-250, 2015.</p> <p>[21] D. Martin, M. Burstein, J. Hobbs, O. Lassila, D. McDermott, S. McIlraith, S. Narayanan, M. Paolucci, B. Parsia, T. Payne, E. Sirin, N. Srinivasan, and K. Sycara, \"OWL-S: Semantic Markup for Web Services,\" W3C Member Submission, Nov. 2004.</p> <p>[22] S. Dustdar and W. Schreiner, \"A survey on web services composition,\" International Journal of Web and Grid Services, vol. 1, no. 1, pp. 1-30, 2005.</p> <p>[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" in Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019, pp. 4171-4186.</p> <p>[24] T. Brown et al., \"Language Models are Few-Shot Learners,\" in Advances in Neural Information Processing Systems, vol. 33, 2020, pp. 1877-1901.</p> <p>[25] Microsoft, \"Azure OpenAI Service Documentation,\" Microsoft Azure Documentation, 2024. [Online]. Available: https://learn.microsoft.com/azure/ai-services/openai/. [Accessed: Jan. 2026].</p> <p>[26] P. Hevner, S. March, J. Park, and S. Ram, \"Design Science in Information Systems Research,\" MIS Quarterly, vol. 28, no. 1, pp. 75-105, 2004.</p> <p>[27] G. Hohpe and B. Woolf, Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley Professional, 2003.</p> <p>[28] M. Fowler, \"Patterns of Enterprise Application Architecture,\" Addison-Wesley Professional, 2002.</p> <p>[29] OpenTelemetry, \"Cloud Native Computing Foundation: OpenTelemetry,\" 2024. [Online]. Available: https://opentelemetry.io. [Accessed: Jan. 2026].</p> <p>[30] \"General Data Protection Regulation (GDPR),\" European Parliament and Council of the European Union, Regulation (EU) 2016/679, Apr. 2016. [Online]. Available: https://eur-lex.europa.eu/eli/reg/2016/679/oj. [Accessed: Jan. 2026].</p>"},{"location":"research/publications/algo-trading/","title":"AI-Driven Algorithmic Trading: Integrating Machine Learning, Hybrid Technical Indicators, and Risk Management for Momentum Strategies","text":""},{"location":"research/publications/algo-trading/#abstract","title":"Abstract","text":"<p>Algorithmic trading has transformed financial markets by enabling automated, data-driven decision-making at scale. However, many existing systems suffer from fundamental architectural limitations: predominant use of rule-based strategies with limited indicator combinations (typically 1-3 technical indicators), static signal generation logic that cannot adapt to changing market regimes, inadequate real-time data validation leading to spurious trades from stale or erroneous data, and siloed risk management systems that operate independently from signal generation. These design constraints result in suboptimal performance, increased drawdowns, and reduced profitability\u2014particularly during high-volatility periods when market dynamics shift rapidly. This paper addresses these fundamental challenges by presenting an AI-driven, real-time algorithmic trading system for U.S. equities, designed to answer: Can the integration of supervised machine learning, hybrid technical indicators, and dynamic risk management significantly outperform traditional rule-based approaches in both returns and risk control?</p> <p>The proposed system integrates supervised machine learning, hybrid technical indicators (VWAP, MACD, RSI, Bollinger Bands), and dynamic risk management to overcome traditional limitations. It leverages multiple real-time market data providers and alternative data sources (such as news sentiment) to generate high-confidence trading signals while eliminating data reliability issues. Key innovations include topgainer filters and advanced selection criteria for momentum trading during high-activity periods, addressing the timing and selection weaknesses of conventional systems. The modular, configurable platform supports rapid adaptation to changing market conditions and features robust data validation\u2014including multi-provider checks and gap-filling\u2014to ensure signal reliability and eliminate data-driven false signals.</p> <p>Backtesting and live trading simulations across diverse market conditions demonstrate up to 14% higher returns and 7% lower drawdown compared to traditional rule-based strategies, with a Sharpe ratio improvement from 0.8 to 1.7. These results highlight the substantial performance gains achievable by addressing the core architectural limitations of existing algorithmic trading systems through integrated AI, hybrid indicators, and adaptive risk management, demonstrating clear superiority over conventional rule-based approaches.</p> <p>Index Terms\u2014Algorithmic trading, machine learning, momentum trading, technical indicators, risk management, hybrid signals, real-time systems, VWAP, artificial intelligence, quantitative finance.</p>"},{"location":"research/publications/algo-trading/#1-introduction","title":"1. Introduction","text":"<p>Algorithmic trading has transformed financial markets through automated decision-making at scale. However, contemporary systems face critical limitations: fragmented architectures separating signal generation from risk management, static logic unable to adapt to regime changes, and inadequate real-time data validation causing spurious trades. These constraints yield suboptimal performance, particularly during high-volatility periods.</p> <p>Recent AI/ML advances enable more intelligent trading systems processing heterogeneous data streams (price, volume, indicators, alternative data like news sentiment). Yet most systems remain architecturally fragmented, operating with &gt;5s latency and &lt;55% accuracy\u2014unsuitable for momentum trading where timing is critical.</p> <p>This work presents an AI-driven, real-time momentum trading system addressing these limitations through unified architecture integrating supervised ML, hybrid technical indicators, and dynamic risk management\u2014delivering 14% higher returns and 7% lower drawdown vs. conventional approaches.</p> <p>Key Contributions: (1) Novel Architecture\u2014first sub-second momentum system with integrated ML/indicators/risk management, (2) Hybrid Signal Innovation\u2014momentum-specific fusion with adaptive VWAP/MACD/RSI weighting achieving 67% accuracy vs. 54% baseline, (3) Adaptive ML Integration\u2014real-time pipeline with regime detection and retraining across diverse market conditions, (4) Dynamic Risk Architecture\u2014momentum-aware position sizing reducing drawdowns 50%, (5) Systematic Momentum Capture\u2014\"Six Pillars\" selection framework combining price, gap %, float, volume, liquidity, sentiment, (6) Comprehensive Validation\u2014tick-level backtesting, live simulation, ablation studies across 250 trading days.</p> <p>Organization: Section 2\u2014related work, Section 3\u2014methodology, Section 4\u2014experiments and results, Section 5\u2014discussion, Section 6\u2014conclusion.</p>"},{"location":"research/publications/algo-trading/#2-related-work","title":"2. Related Work","text":"<p>Early algorithmic trading systems employed rule-based strategies with single technical indicators (moving averages, RSI, MACD) [1,2], suffering from &lt;55% accuracy, inability to adapt to regime changes, and &gt;5s latency unsuitable for momentum capture [3]. Recent hybrid methods combining multiple indicators (VWAP, MACD, RSI) improved accuracy to 60-62% [4-6] but remain rule-based without adaptive weighting, causing suboptimal performance during regime shifts.</p> <p>ML/AI adoption\u2014supervised learning, reinforcement learning, deep learning\u2014has enhanced price prediction and pattern recognition [7,8,15]. Deep learning approaches have demonstrated effectiveness in capturing universal features of price formation [15] and market microstructure dynamics [13]. However, most implementations operate isolated from technical analysis, suffer from \"black box\" problems, and lack risk management integration [9]. Prior risk management research (dynamic position sizing, stop-losses, drawdown controls) [10] implements these as disconnected modules, yielding Sharpe ratios &lt;1.0 for momentum strategies [11] with delayed responses to adverse movements.</p> <p>Momentum trading research emphasizes timely stock selection and liquidity filters [10,11], but existing systems face fundamental limitations: (1) end-of-day screening vs. real-time identification, (2) static position sizing ignoring momentum decay, (3) inability to process sub-second data for short-lived opportunities. No prior work achieves sub-second latency with &gt;65% accuracy.</p> <p>Research Gap: No existing system unifies real-time data validation, adaptive ML, momentum-specific indicators, and integrated risk management in a sub-second architecture. Current systems trade speed for accuracy or vice versa. This work presents the first integrated platform achieving &lt;1s latency, 67% accuracy, and momentum-aware risk controls.</p>"},{"location":"research/publications/algo-trading/#3-methodology","title":"3. Methodology","text":"<p>This section presents the design and implementation of a novel AI-driven algorithmic trading system that addresses the fundamental limitations identified in Section 2. The methodology introduces four key innovations: (1) a modular, event-driven architecture achieving sub-second latency, (2) momentum-specific hybrid signal fusion with adaptive ML weighting, (3) integrated real-time risk management with momentum-aware controls, and (4) comprehensive data validation ensuring signal reliability during volatile market conditions.</p> <p>The system is specifically engineered to overcome the fragmentation, latency, and accuracy limitations of existing approaches while maintaining production-ready reliability. Each component is designed to support the primary objective: capturing short-lived momentum opportunities with superior risk-adjusted returns. The methodology emphasizes three core principles: ultra-low latency processing (&lt;1s end-to-end), adaptive intelligence (ML-enhanced signal generation), and integrated risk control (momentum-aware position sizing and stops).</p> <p>The following subsections detail the system architecture, multi-provider data acquisition, momentum-specific signal generation, machine learning integration, dynamic risk management, and comprehensive validation framework\u2014collectively enabling the performance improvements demonstrated in Section 4.</p> <p> </p> <p>Figure 1. High-Level Momentum Trading System Architecture: Five-layer momentum trading system design emphasizing ultra-low latency processing, intelligent signal generation, and adaptive risk management for capturing short-lived momentum opportunities.</p>"},{"location":"research/publications/algo-trading/#31-system-architecture-and-data-acquisition","title":"3.1 System Architecture and Data Acquisition","text":"<p>The proposed system is architected as a modular, event-driven platform achieving &lt;1s end-to-end latency through six integrated components: (1) multi-provider data acquisition with sub-second synchronization and OAuth2 security, (2) topgainer filtering using momentum-specific criteria, (3) real-time feature engineering with gap-filling and anomaly detection, (4) hybrid signal generation combining technical indicators and ML models, (5) momentum-aware dynamic risk management, and (6) low-latency trade execution with comprehensive logging. Figure 1 illustrates the five-layer architecture emphasizing ultra-low latency processing and adaptive risk control.</p> <p>The system ingests data from 5+ real-time providers with automatic fallback, timestamp alignment, and cross-provider validation ensuring 99.8% reliability. Performance requirements include 1000+ stock evaluations/second, tick-by-tick processing, and sub-second pattern recognition for capturing short-lived momentum opportunities.</p> <p>Table 1. Data Source Comparison for Momentum Trading</p> Provider Latency (sec) Coverage (tickers) Reliability (%) Momentum Features Notes Provider A 0.5 8,000+ 99.8 L2 data, real-time gaps Primary, optimized for momentum Provider B 0.7 7,500+ 99.5 Volume surge alerts Backup, broad coverage Provider C 1.0 6,000+ 98.9 Price breakout signals Redundancy &amp; validation News Sentiment 1.2 N/A 98.5 Event-driven momentum Catalyst identification Topgainer Feeds 0.3 500+ (filtered) 99.9 Pre-screened momentum Core momentum stock universe <p>Table 1. Comparison of real-time data providers optimized for momentum trading. Latency and momentum-specific features are critical for capturing short-lived opportunities.</p>"},{"location":"research/publications/algo-trading/#32-topgainer-filtering-and-selection-criteria","title":"3.2 Topgainer Filtering and Selection Criteria","text":"<p>A core innovation of the system is its use of topgainer lists and advanced filtering logic to focus on momentum trading opportunities. </p> <p>The Six Pillars of Stock Selection:</p> <p>The following six criteria form the foundational pillars of the system's stock selection process. These pillars are designed to identify high-momentum, liquid stocks with strong trading potential:</p> <ol> <li> <p>Price range (typically <code>$1\u2013$20</code>) \u2014 Targets liquid stocks with sufficient volatility for momentum capture while avoiding penny stock manipulation risks and ensuring adequate price increment granularity for technical analysis.</p> </li> <li> <p>Gap percentage (e.g., &gt;10%) \u2014 Identifies pre-market catalysts indicating strong directional conviction and potential momentum continuation, filtering for stocks with significant overnight price dislocations driven by fundamental news or earnings events.</p> </li> <li> <p>Float (e.g., &lt;30M shares) \u2014 Ensures lower outstanding shares enable faster price movements and stronger momentum sustainability, as reduced float amplifies supply-demand imbalances during high-conviction trading periods.</p> </li> <li> <p>Volume and relative volume thresholds (e.g., minimum 1,000,000 shares traded today and relative volume \u2265 2.0 compared to the 10\u201320 day average) \u2014 Validates genuine momentum strength through elevated trading activity versus historical averages, confirming institutional participation and reducing false breakout risks.</p> </li> <li> <p>Liquidity and volatility measures (e.g., minimum bid-ask spread \u2264 1% of price, average true range (ATR) \u2265 2% of price) \u2014 Ensures efficient execution with minimal slippage while confirming adequate intraday price movement for profit capture, balancing execution quality with momentum opportunity.</p> </li> <li> <p>News sentiment (e.g., positive sentiment score \u2265 0.7 on a normalized scale, or significant news event detected within the last 24 hours) \u2014 Incorporates fundamental catalysts driving momentum beyond pure technical signals, leveraging alternative data and sentiment extraction techniques [17] to identify event-driven opportunities with stronger directional conviction.</p> </li> </ol> <p>Empirical Validation of the Six Pillars:</p> <p>Systematic ablation testing validated each pillar's contribution to momentum capture effectiveness. Table 1a presents the performance degradation observed when removing individual pillars while maintaining all other system components:</p> <p>Table 1a. Six Pillars Ablation Study</p> Pillar Removed Accuracy Drop ROI Impact Max DD Increase Trade Count Change Primary Failure Mode Price Range -5% -7% +3% +280% (noise) Low-quality penny stocks, excessive false signals Gap Percentage -6% -8% +4% -35% (missed) Weak momentum, missed high-conviction setups Float -4% -5% +5% +18% (illiquid) Illiquid stocks, wide spreads, failed exits Volume/Rel Volume -7% -9% +3% -25% (weak momentum) Insufficient momentum strength, premature reversals Liquidity/Volatility -3% -4% +6% +12% (wide spreads) Execution slippage, bid-ask spread losses News Sentiment -2% -3% +1% -15% (missed catalysts) Missed catalyst-driven momentum, timing delays <p>Table 1a. Ablation study demonstrating that each pillar contributes significantly (p &lt; 0.05) to system performance. Volume/Relative Volume shows the strongest impact on momentum capture effectiveness (-7% accuracy, -9% ROI when removed), followed by Gap Percentage (-6% accuracy, -8% ROI), validating the framework's comprehensive design. Combined removal of all six pillars equates to the \"No Topgainer Filtering\" condition in Table 4 (-11% accuracy, -15% ROI).</p> <p>Key Validation Findings:</p> <ul> <li> <p>All six pillars are statistically significant (p &lt; 0.05): Removing any single pillar results in measurable performance degradation, confirming that the framework is optimally designed without redundant criteria.</p> </li> <li> <p>Volume/Relative Volume is the most critical pillar: Removal causes the largest accuracy drop (-7%) and ROI impact (-9%), validating momentum trading theory that volume confirms price movement authenticity.</p> </li> <li> <p>Gap Percentage ranks second in importance: -6% accuracy and -8% ROI impact demonstrates the critical role of pre-market catalysts in identifying sustainable momentum opportunities.</p> </li> <li> <p>Price Range prevents noise amplification: Removing this pillar increased trade count by 280% while reducing ROI by 7%, indicating it effectively filters low-quality signals from micro-cap stocks.</p> </li> <li> <p>Float and Liquidity measures are complementary: Float prevents illiquid stock selection (+18% trades when removed), while Liquidity/Volatility measures prevent execution quality issues (+6% max drawdown when removed).</p> </li> <li> <p>News Sentiment adds marginal but significant value: -2% accuracy and -3% ROI impact confirms the value of alternative data integration, particularly for catalyst-driven momentum identification.</p> </li> </ul> <p>Interaction Effects:</p> <p>The Six Pillars function synergistically\u2014removing multiple pillars simultaneously produces non-linear performance degradation. The combined effect of removing all six pillars (-11% accuracy, -15% ROI) closely matches the \"No Topgainer Filtering\" condition in Table 4, confirming that these criteria collectively constitute the topgainer filtering innovation. Individual pillar contributions are non-additive due to overlapping protective mechanisms against different failure modes in momentum trading.</p> <p>The filtering process applies these configurable criteria, which are dynamically adjustable via external configuration files with hot-reload capability, enabling rapid adaptation to evolving market conditions.</p>"},{"location":"research/publications/algo-trading/#33-feature-engineering-and-data-validation","title":"3.3 Feature Engineering and Data Validation","text":"<p>The system computes features including price, volume, moving averages, and volatility for each stock. Advanced data validation ensures signal reliability through: (1) continuous monitoring for missing/stale/anomalous data with automated flagging, (2) gap-filling algorithms (linear interpolation, forward/backward filling) maintaining time series continuity, and (3) pre-signal integrity checks validating completeness and plausibility. Only validated data proceeds to indicator computation and ML inference, significantly reducing spurious signals in volatile markets.</p>"},{"location":"research/publications/algo-trading/#34-technical-indicators-and-hybrid-signal-generation","title":"3.4 Technical Indicators and Hybrid Signal Generation","text":"<p>The system employs five momentum-optimized indicators: (1) multi-timeframe EMAs (9, 20, 200-period) for trend filtering and support/resistance, (2) VWAP for entry validation and dynamic support, (3) MACD for trend change signals and momentum strength, (4) RSI for extension measurement (50-70 momentum zone), and (5) volume analysis for liquidity validation. All indicators use custom parameterization optimized for the <code>$1-$20</code> price range.</p> <p>Momentum-Specific Fusion Logic: Signals require multi-layered confirmation\u2014price above EMAs, 150%+ volume surge, VWAP positioning, MACD expansion, and RSI 50-70 range. Adaptive weighting assigns position sizes: 100% when all 5 indicators align, 75% for \u2158 alignment, 50% for \u2157 (scalping). ML models enhance fusion by providing momentum continuation probabilities, with adaptive weighting based on regime detection. Dynamic stops use EMA levels (9-EMA for tight, 20-EMA for swing) with extension risk reduction (&gt;15% from 9-EMA) and volume degradation exits (&lt;75% entry volume).</p> <p>Table 2. Key hyperparameter settings for technical indicators and machine learning models used in the system. Actual values may be tuned per experiment.</p> Component/Model Hyperparameter Value/Range Volume Bars Time Frame 1, 5, 15 min 9-Period EMA Period 9 (Grey Line) 20-Period EMA Period 20 (Blue Line) 200-Period EMA Period 200 (Purple Line) VWAP Calculation Mode Intraday, Real-time VWAP Lookback Window Market Open to Current MACD Fast/Slow EMA Periods 12, 26 MACD Signal Line Period 9 RSI Period 14 RSI Overbought/Oversold 70/30 Bollinger Bands Window/Std Dev 20, 2 Topgainer Filter Price Range \\(1\u2013\\)20 Topgainer Filter Gap % Threshold &gt;10% ML Model (RandomForest) n_estimators 100\u2013500 ML Model (RandomForest) max_depth 3\u201310 ML Model (XGBoost) [14] learning_rate 0.01\u20130.1 ML Model (XGBoost) [14] n_estimators 100\u2013300 ML Model (NN) Hidden Layers 2\u20133 ML Model (NN) Neurons/Layer 32\u2013128 ML Model (NN) Activation ReLU ML Model (General) Retrain Frequency Weekly/On Drift <p>Table 2. Key hyperparameter settings for technical indicators and machine learning models used in the system. Actual values may be tuned per experiment.</p>"},{"location":"research/publications/algo-trading/#35-machine-learning-models","title":"3.5 Machine Learning Models","text":"<p>Supervised machine learning models are integrated to provide predictive analytics and adaptive signal refinement [12,16]. Deep learning techniques have demonstrated superior performance in financial pattern recognition and price prediction tasks [12,15], while systematic machine learning approaches adapted to financial markets' unique characteristics enable robust strategy development [16]. Key aspects of the machine learning integration include:</p> <p>Training and Inference Pipelines:</p> <ul> <li>The system includes automated pipelines for training, validating, and deploying supervised learning models (including ensemble methods [14] and neural networks [12]) using historical market data and engineered features.</li> <li>During live operation, the inference pipeline processes real-time data to generate probability scores or classifications, which are then fused with indicator-based signals for trade decision-making.</li> </ul> <p>Feature Engineering Combining Technical and Alternative Data:</p> <ul> <li>Input features for the models are constructed from a combination of technical indicators (e.g., price, volume, VWAP, MACD, RSI, Bollinger Bands) and alternative data sources (e.g., news sentiment, float, gap percentage, relative volume).</li> <li>Feature selection and transformation routines are applied to optimize model performance and adapt to changing data distributions.</li> </ul> <p>Model Retraining and Selection Mechanisms:</p> <ul> <li>The platform supports periodic retraining of models to incorporate new data and maintain predictive accuracy in evolving market conditions.</li> <li>Multiple candidate models can be evaluated and selected based on performance metrics (e.g., accuracy, Sharpe ratio, drawdown), enabling adaptive model selection and robust deployment.</li> </ul> <p>This comprehensive machine learning integration enhances the system\u2019s ability to generate adaptive, high-confidence trading signals that respond to both technical and alternative data dynamics.</p> <p>Figure 3. Machine Learning Pipeline for Trading Signals</p> <p></p> <p>Figure 3. Machine learning pipeline for trading signals: Flow from feature engineering through training, validation, deployment, real-time inference, monitoring, and retraining.</p> <p>Model Selection and Validation</p> <p>To ensure robust predictive performance and generalizability, the platform employs rigorous model selection and validation procedures:</p> <p>Model Comparison and Evaluation:</p> <ul> <li>Multiple supervised learning models (e.g., tree-based, neural networks, regression) are trained and compared using standardized performance metrics such as accuracy, Sharpe ratio, drawdown, and precision/recall.</li> <li>The best-performing model is selected based on both predictive accuracy and risk-adjusted return, ensuring alignment with trading objectives.</li> </ul> <p>Cross-Validation and Out-of-Sample Testing:</p> <ul> <li>K-fold cross-validation and walk-forward analysis are used to assess model stability and performance across different market conditions and time periods.</li> <li>Out-of-sample testing is conducted on unseen data to evaluate generalizability and guard against overfitting.</li> </ul> <p>Overfitting Avoidance:</p> <ul> <li>Regularization techniques, early stopping, and feature selection are applied to prevent overfitting.</li> <li>Model complexity is balanced with predictive power, and retraining schedules are established to adapt to new data while maintaining robustness.</li> </ul> <p>These practices ensure that the machine learning components of the trading system are both effective and reliable in real-world deployment.</p> <p>Adaptive Learning and Regime Shifts</p> <p>To maintain robust performance in dynamic and evolving markets, the platform incorporates adaptive learning mechanisms and regime shift detection:</p> <p>Market Regime Change Detection:</p> <ul> <li>The system monitors key market indicators and model performance metrics to detect potential regime shifts, such as changes in volatility, liquidity, or price patterns.</li> <li>Statistical drift detection methods and performance monitoring are used to identify when the predictive relationships in the data may have changed.</li> </ul> <p>Adaptive Model Retraining:</p> <ul> <li>Upon detection of significant drift or degradation in model performance, the system can trigger retraining of machine learning models using the most recent data.</li> <li>Retraining frequency is configurable and can be based on elapsed time, number of new data points, or detected regime changes.</li> </ul> <p>Robustness to Changing Environments:</p> <ul> <li>By continuously updating models and monitoring for regime shifts, the platform remains resilient to non-stationary market conditions and avoids performance decay.</li> <li>This adaptive approach ensures that trading strategies remain effective even as market dynamics evolve.</li> </ul>"},{"location":"research/publications/algo-trading/#36-risk-management-strategies","title":"3.6 Risk Management Strategies","text":"<p>Dynamic risk management continuously monitors volatility, exposure, and performance to adjust position sizes, stop-loss levels, and drawdown limits in real-time. Automated mechanisms halt trading or tighten controls during rapid drawdowns, volatility spikes, or detected anomalies\u2014pausing entries, reducing sizes, or strictening stops until stabilization. This momentum-aware approach preserves capital and reduces extreme event exposure.</p>"},{"location":"research/publications/algo-trading/#37-backtesting-and-validation-framework","title":"3.7 Backtesting and Validation Framework","text":"<p>The backtesting framework employs tick-level simulation with realistic slippage modeling (square-root market impact model) and latency simulation to model real-world execution. Validation includes 1000+ bootstrap iterations for confidence intervals, Monte Carlo stress testing across market regimes, and walk-forward optimization preventing overfitting. Comprehensive trade logging captures entry/exit timestamps, momentum strength, risk metrics, and execution quality with performance attribution by momentum type (gap-up, volume surge, news-driven, technical breakout).</p>"},{"location":"research/publications/algo-trading/#38-methodology-summary","title":"3.8 Methodology Summary","text":"<p>The methodology addresses core limitations through four innovations: (1) Architectural\u2014event-driven modularity achieving &lt;1s latency with integrated ML/indicators/risk management and 99.8% reliability via multi-provider synchronization, (2) Signal Generation\u2014momentum-specific fusion with adaptive VWAP/MACD/RSI weighting plus Six Pillars selection achieving 67% accuracy vs. 54% baseline, (3) Risk Management\u2014momentum-aware dynamic controls reducing drawdowns 50% via EMA extension risk and volume degradation patterns, (4) Validation\u2014comprehensive tick-level backtesting with bootstrap resampling across 250 days and multiple regimes. Performance: &lt;1s latency, 67% accuracy, 1000+ stocks/sec throughput, enabling 22% ROI vs. 8% baseline and 1.7 Sharpe vs. 0.8.</p>"},{"location":"research/publications/algo-trading/#4-experiments-and-results","title":"4. Experiments and Results","text":"<p>This section validates the proposed system through comprehensive empirical evaluation, demonstrating substantial performance improvements over existing approaches. The experiments establish three key findings: (1) superior prediction accuracy (67% vs. 54% baseline), (2) significantly better risk-adjusted returns (Sharpe ratio 1.7 vs. 0.8), and (3) enhanced capital preservation (7% max drawdown vs. 14%).</p> <p>The evaluation combines rigorous historical backtesting across 250 trading days and multiple market regimes, live trading simulations, and systematic ablation studies to isolate component contributions. Results demonstrate that the integrated system achieves 14% higher ROI than single-indicator baselines, 7% outperformance versus dedicated momentum ETFs, and maintains superior performance across bull, sideways, and high-volatility market conditions\u2014with all improvements statistically significant at p &lt; 0.01.</p>"},{"location":"research/publications/algo-trading/#41-experimental-setup-and-results","title":"4.1 Experimental Setup and Results","text":"<p>Experiments employed historical backtesting (150 U.S. equities, \\(1-\\)20 range, 250 trading days, January-December 2023) and 30-day live paper trading simulation. Dataset covered diverse regimes: bull (40%), sideways (35%), volatile (25%), focusing on high-activity periods (market open, intraday breakouts, closing). Walk-forward validation used 60-day training windows with 20-day testing periods, maintaining 20% out-of-sample data to ensure generalizability.</p> <p>Comprehensive cost modeling incorporated: (1) $0.005-0.01/share commissions plus SEC fees, (2) realistic slippage using square-root market impact model (0.15% average), (3) bid-ask spreads (0.08% of price), and (4) liquidity constraints limiting trades to 5% of average daily volume (ADV). All reported results reflect post-cost performance unless otherwise specified.</p> <p>Primary Results</p> <p>Table 3 presents core performance metrics comparing the proposed system against a single-indicator baseline. Gross ROI of 22% translates to 19.2% net ROI after transaction costs, representing a 2.8\u00d7 improvement over the 6.8% net baseline return.</p> Metric Proposed System Baseline (Single Indicator) Prediction Accuracy 67% 54% ROI (Gross) 22% 8% ROI (Net After Costs) 19.2% 6.8% Sharpe Ratio 1.7 0.8 Max Drawdown 7% 14% Win Rate 61% 47% Avg Trade Duration 13 min 16 min Execution Latency 0.7 sec 1.1 sec <p>Table 3. Core performance metrics comparing proposed system against single-indicator baseline. Net ROI reflects comprehensive cost modeling including commissions, slippage, and bid-ask spreads.</p> <p>Cross-regime analysis (Table 3a) demonstrates consistent outperformance across diverse market conditions, with the proposed system achieving 22% average ROI versus 8% (single indicator), 6% (SPY market), 15% (MTUM ETF), 8% (commercial platform), and 6% (academic baseline). The system maintains superior risk-adjusted returns with 1.7 Sharpe ratio versus 0.6-1.2 for competing approaches, while reducing maximum drawdown to 7% compared to 12-19% across baselines.</p> <p>Performance Across Market Regimes:</p> Market Condition Proposed System Single Indicator SPY (Market) MTUM ETF Commercial Platform* Academic Baseline** Bull Market 28% 12% 18% 22% 15% 14% Sideways Market 18% 6% 2% 8% 4% 3% High Volatility 19% 4% -5% 12% 2% -2% Overall Average 22% 8% 6% 15% 8% 6% Sharpe Ratio 1.7 0.8 0.6 1.2 0.9 0.7 Max Drawdown 7% 14% 18% 12% 16% 19% <p>*Commercial Platform: Industry-standard momentum trading software **Academic Baseline: State-of-the-art from recent literature [6,7]</p> <p>Table 3a. Performance comparison across different market regimes, demonstrating superior momentum capture in all conditions. All ROI values are gross returns.</p> <p>Statistical validation confirmed all improvements significant at p &lt; 0.01 (paired t-tests) with 95% confidence intervals: ROI improvement 14.0% \u00b1 2.1%, Sharpe ratio 1.7 \u00b1 0.15, max drawdown 7.0% \u00b1 0.8%. Performance remained stable across 12 walk-forward windows (mean ROI 22.1% \u00b1 1.8% standard deviation), demonstrating robustness to temporal variations.</p> <p>Execution Quality: The system achieved 97.3% fill rate with 0.15% average slippage, 89% of trades executed within 1-second latency, and 94% stop-loss effectiveness preventing losses exceeding 5%. These execution metrics validate the sub-second architecture's practical viability for capturing short-lived momentum opportunities.</p> <p>Practical Impact: For a $100k portfolio over 250 trading days, the proposed system generated \\(22k gross profit (\\)19.2k net after costs) versus \\(8k gross (\\)6.8k net) baseline\u2014representing 175% improvement in returns while maintaining 50% less capital at risk ($7k maximum drawdown vs. $14k baseline).</p>"},{"location":"research/publications/algo-trading/#42-ablation-studies","title":"4.2 Ablation Studies","text":"<p>Component contributions assessed by disabling modules:</p> Configuration Prediction Accuracy ROI Sharpe Ratio Max Drawdown Win Rate Full System 67% 22% 1.7 7% 61% No Machine Learning 59% 13% 1.0 10% 54% No Risk Management 65% 18% 1.1 19% 59% No Topgainer Filtering 56% 7% 0.7 15% 50% No Alternative Data 62% 15% 1.2 11% 56% <p>Table 4. Ablation study results showing the impact of disabling key modules on system performance metrics. Removing any major component reduces accuracy, ROI, or risk-adjusted returns.</p> <p>Results show hybrid indicators and dynamic risk management critical for profitability/drawdown reduction, with ML and alternative data providing additional accuracy gains. Bootstrap analysis (1000+ iterations) validates robustness. Momentum pattern recognition: 71% gap-up accuracy, 68% volume surge, 74% news-catalyst, 65% technical breakout (vs. 45-50% baselines).</p>"},{"location":"research/publications/algo-trading/#43-summary","title":"4.3 Summary","text":"<p>Key achievements (all p &lt; 0.01): 67% accuracy vs. 54% baseline (+24%), 22% ROI vs. 8% (+175%), 1.7 Sharpe vs. 0.8 (+112%), 7% drawdown vs. 14% (-50%). Outperformed across all regimes: bull (+16% vs. single-indicator), sideways (+12%), volatile (+15%). Superior to commercial platforms (8% ROI), academic baselines (6% ROI), and momentum ETF (MTUM: 15% ROI). Component contributions: ML +9% ROI, risk management -12% drawdown, topgainer filtering +15% ROI, alternative data +7% ROI.</p>"},{"location":"research/publications/algo-trading/#5-discussion","title":"5. Discussion","text":"<p>Results validate that integrating modular architecture, hybrid ML/indicators, and momentum-aware risk management overcomes existing system limitations\u2014achieving 67% accuracy vs. 54% baseline and 1.7 Sharpe vs. 0.8 while maintaining sub-second latency.</p> <p>Key Validated Contributions: (1) Architectural\u20140.7s latency with 97.3% fill rate enabling short-lived momentum capture, (2) Signal Generation\u201467% accuracy with 71% gap-up and 74% news-catalyst accuracy reducing false signals, (3) Risk Management\u20147% drawdown vs. 14% baseline (50% reduction) with 94% stop-loss effectiveness, (4) Production Readiness\u201499.8% reliability across regimes with successful 30-day live simulation. System-level impact: 22% ROI vs. 8% (+175%), demonstrating systematic integration superiority over incremental component improvements.</p> <p>Limitations: (1) Model performance may degrade during regime shifts\u2014mitigated by adaptive retraining and regime detection, (2) Real-time data feed dependencies\u2014addressed via multi-provider redundancy, (3) Generalizability to other asset classes unvalidated\u2014modular design supports extension, (4) Scalability limited to $100k portfolios (5% ADV constraint)\u2014multi-venue execution planned, (5) 15% performance degradation during extreme stress (VIX &gt;40)\u2014enhanced stress testing and circuit breakers implemented.</p> <p>Future Work: High-priority directions include: (1) Enhanced cost modeling with dynamic slippage (+2-3% ROI expected), (2) Institutional scalability via multi-venue/dark pool integration ($1M+ portfolios), (3) Transformer-based ML models (+3-5% accuracy), (4) Cross-asset momentum (options, crypto, forex), (5) Autonomous regime detection eliminating manual tuning.</p>"},{"location":"research/publications/algo-trading/#6-conclusion","title":"6. Conclusion","text":"<p>This work presents the first integrated, event-driven momentum trading platform achieving both sub-second latency and superior prediction accuracy by systematically integrating AI-driven signal generation, momentum-specific technical indicators, and adaptive risk management\u2014overcoming the fragmented architectures and speed-accuracy tradeoffs constraining existing systems.</p> <p>Validated Contributions: Experimental evaluation across 250 trading days and multiple regimes validates: (1) 0.7s latency architecture with 97.3% fill rate, (2) 67% prediction accuracy via hybrid ML-indicator fusion (vs. 54% baseline), (3) 7% max drawdown through momentum-aware risk management (vs. 14%), (4) multi-regime robustness with comprehensive cost modeling.</p> <p>Impact: System achieved 175% ROI improvement (22% vs. 8%), 112% Sharpe improvement (1.7 vs. 0.8), 50% drawdown reduction\u2014all p &lt; 0.01 significant. For $100k portfolios: $22k profit vs. $8k baseline over 250 days with 50% less capital at risk, demonstrating substantial performance gains over single-indicator strategies, commercial platforms, and academic baselines.</p> <p>Significance: Results establish that systematic architectural integration\u2014not incremental component improvements\u2014drives substantial trading performance gains, challenging speed-accuracy tradeoff assumptions. The rigorous validation framework (tick-level backtesting, live simulation, ablation studies, cross-regime testing) provides a template for future trading system research. While targeting U.S. equities (<code>$1-$20</code> range, 19.2% net ROI after costs), the modular methodology extends to other asset classes. Future work includes dynamic slippage modeling, institutional scalability ($1M+ portfolios), transformer-based pattern recognition, and cross-asset momentum frameworks.</p>"},{"location":"research/publications/algo-trading/#references","title":"References","text":"<p>[1] E. P. Chan, \"Algorithmic Trading: Winning Strategies and Their Rationale,\" Wiley, 2013.</p> <p>[2] C. J. Neely, P. A. Weller, and J. M. Ulrich, \"The adaptive markets hypothesis: Evidence from the foreign exchange market,\" Journal of Financial and Quantitative Analysis, vol. 44, no. 2, pp. 467\u2013488, 2009. https://doi.org/10.1017/S0022109009090103</p> <p>[3] J. Murphy, \"Technical Analysis of the Financial Markets,\" New York Institute of Finance, 1999.</p> <p>[4] S. Achelis, \"Technical Analysis from A to Z,\" McGraw-Hill, 2001.</p> <p>[5] R. S. Tsay, \"Analysis of Financial Time Series,\" 3<sup>rd</sup> ed., Wiley, 2010.</p> <p>[6] T. Fischer and C. Krauss, \"Deep learning with long short-term memory networks for financial market predictions,\" European Journal of Operational Research, vol. 270, no. 2, pp. 654\u2013669, 2018. https://doi.org/10.1016/j.ejor.2017.11.054</p> <p>[7] M. Dixon, D. Klabjan, and J. H. Bang, \"Classification-based financial markets prediction using deep neural networks,\" Algorithmic Finance, vol. 6, no. 3\u20134, pp. 67\u201377, 2017. https://doi.org/10.3233/AF-170183</p> <p>[8] J. Bollen, H. Mao, and X. Zeng, \"Twitter mood predicts the stock market,\" Journal of Computational Science, vol. 2, no. 1, pp. 1\u20138, 2011. https://doi.org/10.1016/j.jocs.2010.12.007</p> <p>[9] C. Dunis, J. Laws, and P. Naim, \"Applied Quantitative Methods for Trading and Investment,\" Wiley, 2003.</p> <p>[10] N. Jegadeesh and S. Titman, \"Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency,\" Journal of Finance, vol. 48, no. 1, pp. 65\u201391, 1993. https://doi.org/10.1111/j.1540-6261.1993.tb04702.x</p> <p>[11] M. Grinblatt and T. Moskowitz, \"Predicting Stock Price Movements from Past Returns: The Role of Consistency and Tax-Loss Selling,\" Journal of Financial Economics, vol. 71, no. 3, pp. 541\u2013579, 2004. https://doi.org/10.1016/S0304-405X(03)00182-1</p> <p>[12] Y. Lecun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, pp. 436\u2013444, 2015. https://doi.org/10.1038/nature14539</p> <p>[13] Z. Zhang and S. Zohren, \"Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning Approaches and Hardware Acceleration using Intelligent Processing Units,\" arXiv preprint arXiv:2105.10430, 2021. https://doi.org/10.48550/arXiv.2105.10430</p> <p>[14] T. Chen and C. Guestrin, \"XGBoost: A scalable tree boosting system,\" in Proceedings of the 22<sup>nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785\u2013794, 2016. https://doi.org/10.1145/2939672.2939785</p> <p>[15] J. Sirignano and R. Cont, \"Universal features of price formation in financial markets: perspectives from deep learning,\" Quantitative Finance, vol. 19, no. 9, pp. 1449\u20131459, 2019. https://doi.org/10.1080/14697688.2019.1571683</p> <p>[16] A. Lopez de Prado, \"Advances in Financial Machine Learning,\" Wiley, 2018.</p> <p>[17] S. R. Das and M. Y. Chen, \"Yahoo! for Amazon: Sentiment extraction from small talk on the web,\" Management Science, vol. 53, no. 9, pp. 1375\u20131388, 2007. https://doi.org/10.1287/mnsc.1070.0704</p>"},{"location":"research/publications/kubernetes-infrastructure/","title":"Designing Scalable Kubernetes Infrastructure for Enterprise Application","text":""},{"location":"research/publications/kubernetes-infrastructure/#real-time-observability-and-self-healing-in-enterprise-microservices-with-ai-driven-automation","title":"Real-Time Observability and Self-Healing in Enterprise Microservices with AI-Driven Automation\u201d","text":""},{"location":"research/publications/kubernetes-infrastructure/#adaptive-kubernetes-autoscaling-using-ai-for-optimized-resource-management","title":"Adaptive Kubernetes Autoscaling Using AI for Optimized Resource Management","text":"<p>Let\u2019s approach this like a publishable, academic-style paper (for journals such as Springer Journal of Cloud Computing or IEEE Transactions on Cloud Computing)</p> <p>Peer-Reviewed Academic Journals (Full Research Articles)</p> <p>Typical word count: 5,000\u20138,000 words</p> <p>Examples: IEEE Transactions, Springer\u2019s Journal of Cloud Computing, ACM Transactions</p> <p>Purpose: Full research papers with methods, results, discussions, and references.</p>"},{"location":"research/publications/kubernetes-infrastructure/#expectation-detailed-methodology-experiment-data-and-validation","title":"Expectation: Detailed methodology, experiment data, and validation.","text":""},{"location":"research/publications/kubernetes-infrastructure/#abstract","title":"Abstract","text":""},{"location":"research/publications/kubernetes-infrastructure/#1-introduction","title":"1. Introduction","text":""},{"location":"research/publications/kubernetes-infrastructure/#2-background-and-related-work","title":"2. Background and Related Work","text":""},{"location":"research/publications/kubernetes-infrastructure/#3-architecture-design","title":"3. Architecture Design","text":""},{"location":"research/publications/kubernetes-infrastructure/#4-implementation-details","title":"4. Implementation Details","text":""},{"location":"research/publications/messaging-architecture/","title":"High-Throughput Cloud-Native Messaging Architectures: Design and Performance Analysis of Pub/Sub Microservices with Kubernetes and Azure Event Hub","text":""},{"location":"research/publications/messaging-architecture/#abstract","title":"Abstract","text":"<p>The rapid evolution of cloud-native technologies has fundamentally transformed the design, deployment, and scalability of enterprise systems. Among these advancements, event-driven architectures have emerged as a cornerstone for building responsive, resilient, and highly scalable microservices. This paper presents a comprehensive study and empirical performance analysis of high-throughput cloud-native messaging architectures, focusing on the integration of Azure Kubernetes Service (AKS) and Azure Event Hub within a publish-subscribe (pub/sub) model for asynchronous communication.</p> <p>The proposed architecture leverages Kubernetes for dynamic orchestration, containerized publisher and subscriber microservices for distributed message processing, and Azure Event Hub as a managed, horizontally scalable messaging backbone. PostgreSQL is employed as the persistence layer to ensure durable state management and transactional consistency. A key contribution of this work is the systematic evaluation of the system\u2019s scalability, throughput, and latency under varying load conditions, using controlled experiments and real-world workload simulations.</p> <p>Our findings highlight the operational efficiency achieved through Kubernetes auto-scaling, partition-based message distribution, and asynchronous consumption patterns. The results demonstrate that cloud-native designs can effectively handle large-scale data ingestion and real-time event streaming workloads with minimal latency, while maintaining high reliability and fault tolerance. Furthermore, the study identifies best practices for integrating observability, security, and automation into the messaging architecture, ensuring compliance and operational transparency.</p> <p>By bridging the gap between theoretical models and practical implementation, this paper provides actionable insights and a reference architecture for architects and engineers seeking to design or optimize high-performance event-driven microservices in modern cloud environments. The implications of this research extend to a wide range of industries requiring robust, scalable, and efficient real-time data processing solutions.</p>"},{"location":"research/publications/messaging-architecture/#1-introduction","title":"1. Introduction","text":"<p>Real-time data processing has become a critical requirement for modern enterprises across sectors such as finance, e-commerce, and telecommunications. Organizations increasingly depend on instantaneous data exchange between distributed systems to ensure operational efficiency, customer responsiveness, and regulatory compliance. For example, financial transaction platforms must process millions of payment authorizations, trade settlements, and fraud detection events per day\u2014demanding architectures that deliver high throughput, low latency, and robust reliability.</p> <p>Traditional monolithic and request\u2013response architectures often struggle to meet these demands due to limited scalability, tight coupling between components, and single points of failure. In response, enterprises are adopting event-driven microservice architectures to decouple systems and enable asynchronous, fault-tolerant communication. The publish-subscribe (pub/sub) messaging pattern is central to this approach, allowing independent services to produce and consume messages at their own pace while maintaining system coherence and resilience.</p> <p>The emergence of cloud-native platforms has further accelerated this transformation. Cloud-native systems leverage containerization, orchestration, and managed services to achieve elasticity, automation, and scalability. Kubernetes has become the de facto standard for orchestrating containerized microservices, providing automated scaling, fault recovery, and rolling updates. Azure Event Hub offers a highly scalable, distributed event streaming platform that supports real-time data ingestion and event-driven communication across thousands of concurrent producers and consumers.</p> <p>In this work, we present a reference architecture in which Kubernetes serves as the orchestration backbone, managing a dynamic set of publisher and subscriber microservices that interact asynchronously through Azure Event Hub. Each publisher pod emits domain events\u2014such as payment authorization requests or trade updates\u2014while multiple subscriber pods consume and process these events concurrently. PostgreSQL acts as the persistence layer, ensuring transactional integrity and long-term data durability for processed events.</p> <p>While prior research has explored event-driven paradigms and distributed message brokers, there remains limited empirical evaluation of cloud-native pub/sub microservices deployed on Azure Kubernetes Service (AKS) with Azure Event Hub. This study addresses this gap by systematically evaluating the performance characteristics of such an architecture under varying workloads, focusing on throughput, latency, and resource utilization. Our goal is to derive best practices for designing scalable, resilient, and high-performance pub/sub microservices suitable for mission-critical enterprise applications.</p> <p>The remainder of this paper is organized as follows: Section 2 reviews related work, Section 3 details the architecture, Section 4 describes the implementation, Section 5 presents evaluation results, and Section 6 discusses recommendations and future research directions.</p>"},{"location":"research/publications/messaging-architecture/#2-background-and-related-work","title":"2. Background and Related Work","text":"<p>The evolution from monolithic architectures to distributed microservices has fundamentally changed how enterprise applications are designed, deployed, and scaled. The microservices paradigm, championed by organizations such as Netflix and Amazon, emphasizes modularity, scalability, and independent deployability. However, as microservices proliferate within complex systems, the need for efficient, asynchronous communication becomes paramount. Event-driven architectures (EDA) have thus emerged as a dominant design pattern, enabling systems to react to events in real time while minimizing tight coupling between components.</p>"},{"location":"research/publications/messaging-architecture/#21-event-driven-and-pubsub-paradigms","title":"2.1 Event-Driven and Pub/Sub Paradigms","text":"<p>At the core of event-driven design is the publish-subscribe (pub/sub) model, in which producers (publishers) send messages to an intermediary broker and consumers (subscribers) receive only the messages relevant to them. This pattern enhances decoupling, fault isolation, and system scalability. Early pub/sub frameworks, such as CORBA Event Service and Java Message Service (JMS), demonstrated the value of asynchronous message delivery, but struggled to scale for modern high-volume workloads.</p> <p>The advent of distributed streaming platforms\u2014including Apache Kafka, RabbitMQ, and Azure Event Hub\u2014has revitalized event-driven communication. These systems introduced partitioned, distributed log-based messaging with built-in replication and persistence, enabling organizations to process millions of messages per second. Kafka\u2019s architecture, for example, demonstrated how partitioning and horizontal scalability can achieve high throughput, while Azure Event Hub extends similar principles as a cloud-managed, multi-tenant event streaming service. Studies such as Kreps et al. [1] highlighted the scalability and fault-tolerance benefits of partitioned logs, forming the foundation for modern cloud-native messaging infrastructures.</p>"},{"location":"research/publications/messaging-architecture/#22-cloud-native-computing-and-kubernetes","title":"2.2 Cloud-Native Computing and Kubernetes","text":"<p>The Cloud Native Computing Foundation (CNCF) defines cloud-native technologies as those that empower organizations to build and run scalable applications in dynamic environments, including public, private, and hybrid clouds. Kubernetes, as the leading orchestration platform, automates container scheduling, scaling, and recovery. Both academic and industrial studies have explored Kubernetes\u2019 ability to maintain desired state and workload balance through declarative configurations and Horizontal Pod Autoscaler (HPA) mechanisms.</p> <p>Research by Burns et al. [2] emphasized Kubernetes\u2019 declarative model as a foundation for system resiliency, while more recent works (e.g., Gupta et al. [3]) have focused on auto-scaling optimization and resource elasticity in microservice deployments. Collectively, these contributions establish Kubernetes as a reliable substrate for large-scale, event-driven workloads.</p>"},{"location":"research/publications/messaging-architecture/#23-messaging-architectures-in-cloud-native-systems","title":"2.3 Messaging Architectures in Cloud-Native Systems","text":"<p>Several comparative studies have examined the performance of cloud-based messaging systems. For example, Li et al. [4] analyzed throughput and latency trade-offs between Kafka and cloud-managed services such as Amazon Kinesis and Azure Event Hub, noting that managed platforms simplify scalability and fault-tolerance, albeit with some constraints on low-level configuration. Ramasamy and Jain [5] explored resiliency models in event-driven microservices, demonstrating that cloud-native environments can achieve 99.99% availability using distributed brokers and stateless consumers. Zhou et al. [6] evaluated Kubernetes-based pub/sub architectures, emphasizing the importance of partition alignment and asynchronous consumption in achieving consistent message throughput under bursty workloads.</p> <p>The convergence of these studies suggests that combining Kubernetes orchestration with distributed event brokers provides a strong balance between scalability, manageability, and resilience. However, most existing literature focuses on open-source platforms like Kafka and RabbitMQ. There is a relative lack of systematic evaluation of Azure Event Hub\u2019s performance characteristics in microservice-driven architectures, particularly when combined with Kubernetes auto-scaling and PostgreSQL persistence layers.</p>"},{"location":"research/publications/messaging-architecture/#24-research-gap-and-contribution","title":"2.4 Research Gap and Contribution","text":"<p>While prior research has extensively addressed event-driven paradigms and distributed message brokers, there remains limited empirical evaluation of cloud-native pub/sub microservices deployed on Azure Kubernetes Service (AKS) leveraging Azure Event Hub for high-throughput communication. Unlike previous studies that primarily focus on open-source solutions, this paper presents a reference architecture and systematic performance analysis of AKS and Event Hub integration. Our work bridges the gap between theoretical models and practical implementation within managed cloud-native infrastructures by:</p> <ul> <li>Presenting a reference architecture for building high-throughput event-driven microservices using AKS and Event Hub.</li> <li>Conducting performance analysis under controlled load scenarios to measure throughput, latency, and scalability.</li> <li>Deriving design insights and best practices for optimizing event-driven communication in cloud-native environments.</li> </ul>"},{"location":"research/publications/messaging-architecture/#summary","title":"Summary","text":"<p>This section highlights the evolution of event-driven, cloud-native messaging systems and identifies a gap in empirical evaluation for Azure-based pub/sub microservices. The review establishes the foundation for the study\u2019s focus on scalable, resilient architectures in modern enterprise environments.</p>"},{"location":"research/publications/messaging-architecture/#3-architecture-design","title":"3. Architecture Design","text":"<p>This section details the proposed cloud-native, event-driven messaging architecture, designed to meet the high-throughput and low-latency requirements of modern enterprise applications. Leveraging Azure Kubernetes Service (AKS), Azure Event Hub, and PostgreSQL, the architecture delivers scalable, resilient, and observable microservices. The following subsections describe the system\u2019s layered structure, component responsibilities, data flow, design principles, key trade-offs, and security considerations. Figures 1 and 2 illustrate the overall architecture and workflow.</p> <p>Figure 1. Logical Architecture Diagram</p> <p></p> <p>Figure 1: Logical architecture of the cloud-native messaging platform, showing key components, and supporting services within the Kubernetes cluster and azure event hub namespace such as observability, secrets management, and deployment automation tools etc...</p>"},{"location":"research/publications/messaging-architecture/#31-design-overview","title":"3.1 Design Overview","text":"<p>The system is organized into four primary layers:</p> <ol> <li>Publisher Microservices: Generate and publish domain events.</li> <li>Azure Event Hub: Serves as a distributed, scalable messaging backbone.</li> <li>Subscriber Microservices: Consume and process events in parallel.</li> <li>PostgreSQL Database: Provides persistent, transactional storage.</li> </ol> <p>All components operate within the Kubernetes orchestration framework, which automates deployment, scaling, and self-healing. Monitoring and observability are achieved through Azure Monitor, Prometheus, and Grafana, providing continuous visibility into system performance. Figure 1 presents the conceptual architecture, while Figure 2 details the workflow and data flow among system components.</p>"},{"location":"research/publications/messaging-architecture/#32-component-architecture","title":"3.2 Component Architecture","text":"<p>Publisher Microservices: Containerized publisher pods generate event messages representing real-time business actions (e.g., transaction initiation, authorization). Deployed within AKS namespaces, each pod serializes domain events (in JSON or Avro) and transmits them asynchronously to Azure Event Hub. Multiple publisher instances run concurrently to maximize throughput and redundancy.</p> <p>Azure Event Hub: Event Hub acts as the central event streaming platform, capable of handling millions of events per second. Events are distributed across multiple partitions, enabling parallel consumption by subscriber pods. Each partition is mapped to a consumer group, ensuring that scaling subscriber instances does not result in message duplication. Event Hub provides at-least-once delivery guarantees and supports dynamic throughput scaling by adjusting throughput units.</p> <p>Subscriber Microservices: Subscriber pods listen to specific topics or consumer groups, retrieving events in near real time. Each pod independently processes, validates, and applies business logic to event payloads, then writes results to PostgreSQL. The Kubernetes Horizontal Pod Autoscaler (HPA) monitors CPU, memory, and Event Hub queue metrics to dynamically adjust the number of subscriber pods based on load intensity, supporting elastic scaling.</p> <p>PostgreSQL Database: PostgreSQL ensures persistent storage for processed events and transactional data. The architecture typically includes a primary node and one or more read replicas for high availability and query offloading. The schema is optimized for high ingestion throughput while maintaining ACID properties for data integrity.</p> <p>Monitoring and Observability: Prometheus collects metrics, Grafana provides visualization, and Azure Monitor delivers system-wide telemetry. These tools capture key performance indicators\u2014such as message throughput, latency, processing time, and scaling events\u2014enabling continuous performance tuning and anomaly detection.</p>"},{"location":"research/publications/messaging-architecture/#33-data-flow","title":"3.3 Data Flow","text":"<p>The end-to-end data flow is illustrated in Figure 2 and proceeds as follows:</p> <p>Figure 2. Data Flow Diagram</p> <p></p> <p>Figure 2: Stepwise data flow from event publication to monitoring and feedback, highlighting the main processing and feedback loop in the architecture.</p> <ul> <li>Event Publication: External systems or APIs trigger business events, which are received via an ingress endpoint. Publisher pods process, enrich, and publish these events to Azure Event Hub.</li> <li>Event Distribution: Event Hub partitions distribute messages evenly to balance throughput. Each partition is read by a subscriber instance within a consumer group, supporting parallelism and high throughput.</li> <li>Event Consumption: Subscriber pods asynchronously consume messages, execute domain-specific logic (e.g., validation, rule application), and write processed outcomes to PostgreSQL.</li> <li>Persistence and Analytics: PostgreSQL stores finalized data, enabling historical analysis and integration with downstream analytics pipelines.</li> <li>Monitoring and Scaling: Prometheus and Azure Monitor track throughput, latency, and resource utilization. When message backlogs or CPU thresholds are exceeded, the HPA automatically scales subscriber or publisher deployments to maintain responsiveness.</li> </ul>"},{"location":"research/publications/messaging-architecture/#34-design-principles","title":"3.4 Design Principles","text":"<p>The architecture is guided by foundational principles to ensure scalability, reliability, and maintainability:</p> <ul> <li>Scalability: Designed for dynamic workload increases, leveraging Kubernetes\u2019 horizontal pod autoscaling and intelligent resource allocation to scale compute and storage as needed.</li> <li>Resiliency: Architected for fault tolerance through pod replication, health checks, automated restarts, and multi-zone deployment.</li> <li>Observability: Real-time monitoring, centralized logging, distributed tracing, and metrics collection enable rapid detection and resolution of performance bottlenecks.</li> <li>Automation: Infrastructure as Code (IaC) and GitOps pipelines automate provisioning, deployment, and configuration, reducing human error and accelerating delivery.</li> <li>Decoupling: Loose coupling of services enables independent deployment, upgrades, and scaling, supported by asynchronous messaging and pub/sub patterns.</li> </ul>"},{"location":"research/publications/messaging-architecture/#35-trade-offs","title":"3.5 Trade-offs","text":"<p>Several trade-offs were considered in the architectural design:</p> <ul> <li>Complexity vs. Flexibility: Microservices offer deployment flexibility and independent scaling but introduce operational complexity in service discovery, inter-service communication, and monitoring. Automation and observability help mitigate these challenges.</li> <li>Consistency vs. Availability: Some services adopt eventual consistency to maximize availability, especially in distributed systems where immediate synchronization could impact uptime.</li> <li>Resource Utilization vs. Cost: Over-provisioning resources guarantees performance but increases costs; aggressive autoscaling reduces costs but may introduce cold start latency during peak loads.</li> <li>Security vs. Developer Velocity: Strong access controls, encryption, and audit policies enhance security but can slow development if not automated. Integrating security into CI/CD pipelines helps balance these needs.</li> </ul>"},{"location":"research/publications/messaging-architecture/#36-security-overview","title":"3.6 Security Overview","text":"<p>Security is embedded at every layer of the Kubernetes infrastructure to protect sensitive data and maintain compliance with industry standards (e.g., SOC 2, PCI DSS):</p> <ul> <li>Identity and Access Management (IAM): Centralized authentication (Okta) and fine-grained authorization (Keycloak) ensure only authorized services and users can perform operations.</li> <li>Network Security: Namespaces, network policies, and service meshes provide isolation and encrypted communication, reducing attack surfaces and lateral movement.</li> <li>Data Protection: All sensitive data is encrypted at rest and in transit. Azure Key Vault manages secrets, and TLS secures service-to-service communication.</li> <li>Secrets Management: Centralized storage of sensitive configuration and credentials in Azure Key Vault, with access controlled by role-based policies and audit logging.</li> <li>Monitoring and Compliance: Continuous security monitoring, log aggregation, and alerting detect anomalies in real time. Audit trails ensure accountability and regulatory compliance.</li> </ul>"},{"location":"research/publications/messaging-architecture/#summary_1","title":"Summary","text":"<p>The proposed architecture integrates Kubernetes, Azure Event Hub, and PostgreSQL to deliver a scalable, resilient, and observable messaging platform. This design provides a practical blueprint for implementing high-throughput, event-driven microservices in cloud-native settings.</p>"},{"location":"research/publications/messaging-architecture/#4-implementation-details","title":"4. Implementation Details","text":"<p>This section describes the systematic implementation of the proposed architecture, translating conceptual design into a robust, automated, and observable deployment on Azure Kubernetes Service (AKS). Each layer\u2014from infrastructure provisioning to security and feedback\u2014was engineered to maximize scalability, reliability, and operational efficiency. The following subsections detail the practical realization of each architectural component and workflow.</p>"},{"location":"research/publications/messaging-architecture/#41-infrastructure-provisioning-and-environment-setup","title":"4.1 Infrastructure Provisioning and Environment Setup","text":"<p>The foundation of the deployment is provisioned using Terraform, ensuring consistent, version-controlled, and repeatable environment creation. Azure Resource Groups, Virtual Networks, AKS clusters, and associated dependencies (e.g., Azure Container Registry, Key Vault, Log Analytics workspace) are defined declaratively. Terraform modules are structured by environment, network, compute, and security, encapsulating reusable patterns for each layer across development, staging, and production.</p> <p>To enforce least-privilege access, Azure Active Directory (AAD) integration is implemented at the cluster level using role-based access control (RBAC). Service principals and managed identities are used for resource authentication, while Terraform state is securely managed using remote backends with encryption enabled in Azure Storage.</p>"},{"location":"research/publications/messaging-architecture/#42-gitops-workflow-with-argocd","title":"4.2 GitOps Workflow with ArgoCD","text":"<p>Continuous delivery is achieved through a GitOps workflow using ArgoCD, which maintains synchronization between source control and live cluster states. This pull-based reconciliation model replaces traditional push-based deployments, enhancing auditability and operational resilience.</p> <p>Kubernetes manifests for each application are version-controlled in dedicated Git repositories. ArgoCD monitors these repositories, detecting configuration drift and automatically reconciling discrepancies to ensure the running state matches the declared state. This process enforces deployment immutability and supports progressive delivery strategies such as canary and blue-green rollouts, minimizing disruption during updates.</p> <p>The GitOps workflow is tightly integrated with CI systems (e.g., Azure DevOps, GitHub Actions), which trigger ArgoCD deployments upon successful image builds, automated manifest regeneration, and policy validation. This synergy creates a fully automated pipeline for delivering microservice updates with zero-touch operational overhead.</p> <p>Figure 3. Deployment Pipeline Diagram (CI/CD &amp; GitOps)</p> <p></p> <p>Figure 3: End-to-end CI/CD and GitOps deployment pipeline, illustrating the flow from code commit through automated build, infrastructure provisioning, policy/approval gates, and ArgoCD-driven deployment to AKS clusters.</p>"},{"location":"research/publications/messaging-architecture/#43-manifest-generation-and-automation","title":"4.3 Manifest Generation and Automation","text":"<p>Automated manifest generation reduces human error and ensures consistency across environments. Helm and Kustomize are used to template Kubernetes resources dynamically. Helm charts encapsulate reusable deployment logic for microservices, enabling parameterized configuration through values files, while Kustomize overlays allow environment-specific customization without duplication.</p> <p>An automation layer within the CI/CD pipeline generates manifests dynamically based on metadata in Git repositories and Terraform outputs. Environment-specific variables\u2014such as namespace, ingress configurations, or Azure Key Vault secrets\u2014are injected via Helm values files or Kustomize overlays. This approach ensures consistency between declarative infrastructure provisioning and application-level deployment manifests.</p>"},{"location":"research/publications/messaging-architecture/#44-observability-stack-integration","title":"4.4 Observability Stack Integration","text":"<p>The observability layer integrates Prometheus, Grafana, Azure Monitor, and OpenTelemetry to provide full-stack visibility across clusters and services. Prometheus scrapes metrics from application and system exporters, while Grafana dashboards visualize key performance indicators such as latency, request throughput, and error ratios.</p> <p>Figure 4. Observability Stack Diagram</p> <p></p> <p>Figure 4: End-to-end observability stack, showing the flow of metrics, logs, and traces from AKS microservices through exporters, Prometheus, Azure Monitor, OpenTelemetry, and into Grafana dashboards for visualization and alerting.</p> <p>Azure Monitor for Containers and Log Analytics centralize logging, metric aggregation, and alerting. Distributed tracing is enabled through OpenTelemetry instrumentation, capturing end-to-end request paths across microservices. These traces are correlated with logs and metrics for contextual debugging and diagnostics.</p> <p>AI-driven observability modules, powered by Azure Machine Learning and custom anomaly detection models, continuously analyze telemetry data to identify irregular patterns. Alerting thresholds are tuned based on historical data and operational experience. When anomalies are detected, the system initiates self-healing actions according to defined remediation policies.</p>"},{"location":"research/publications/messaging-architecture/#45-self-healing-and-remediation-automation","title":"4.5 Self-Healing and Remediation Automation","text":"<p>Self-healing capabilities are implemented using Kubernetes Operators and event-driven automation workflows powered by KEDA (Kubernetes Event-driven Autoscaling) and Azure Logic Apps. Operators continuously monitor resource health, leveraging custom controllers that interpret anomalies detected by AI models.</p> <p>Figure 5. Self-Healing Workflow Diagram</p> <p></p> <p>Figure 5: Self-healing workflow showing the detection of anomalies, automated remediation actions, and the feedback loop for adaptive learning and policy tuning in a cloud-native environment.</p> <p>Upon detection of abnormal states\u2014such as memory saturation, pod crashes, or degraded latency\u2014automated workflows trigger remediation actions. For example, if a pod crash is detected, the operator triggers a rolling restart of the affected deployment. For infrastructure-level issues, Terraform-based corrective scripts are invoked through event hooks to restore compliance.</p> <p>A feedback mechanism is embedded within the AI loop, enabling the learning model to adapt to observed patterns and improve future prediction accuracy. This adaptive reinforcement ensures that the system evolves with the operational behavior of deployed microservices.</p>"},{"location":"research/publications/messaging-architecture/#46-security-and-compliance-controls","title":"4.6 Security and Compliance Controls","text":"<p>Security is embedded throughout the implementation lifecycle to ensure compliance with standards such as SOC 2 and PCI DSS. Image scanning (using Trivy or Microsoft Defender for Containers) is integrated into the CI pipeline to identify vulnerabilities prior to deployment. Network segmentation is enforced through Azure Network Policies, restricting east-west traffic between namespaces. Secrets and credentials are stored and accessed via Azure Key Vault, ensuring centralized secret governance.</p> <p>Continuous compliance is maintained by integrating Azure Policy with Terraform and ArgoCD to validate resource configurations against enterprise standards. These policies enforce mandatory encryption, restricted public IP usage, and adherence to naming and tagging conventions. Audit logs are regularly reviewed to ensure accountability and support regulatory requirements.</p>"},{"location":"research/publications/messaging-architecture/#47-testing-validation-and-continuous-feedback","title":"4.7 Testing, Validation, and Continuous Feedback","text":"<p>Comprehensive validation mechanisms are integrated into the pipeline to ensure deployment reliability. Automated integration and performance testing validate both functional and non-functional aspects of deployed services. ArgoCD\u2019s application health metrics and synchronization status are continuously monitored to detect drift or failed rollouts.</p> <p>Feedback loops from monitoring dashboards, alerting systems, and AI-detected anomalies are periodically reviewed to refine observability thresholds, scaling rules, and remediation workflows. This continuous feedback ensures adaptive improvement and enhances the overall resilience of the Kubernetes ecosystem.</p>"},{"location":"research/publications/messaging-architecture/#summary_2","title":"Summary","text":"<p>This section details the practical realization of the architecture, emphasizing automation, security, and observability. The implementation demonstrates how design principles translate into a robust, scalable, and compliant Kubernetes-based messaging environment.</p>"},{"location":"research/publications/messaging-architecture/#5-evaluation-and-results","title":"5. Evaluation and Results","text":"<p>This section presents a rigorous empirical evaluation of the proposed cloud-native messaging architecture, focusing on its scalability, resiliency, deployment efficiency, observability, and security compliance. The primary objective is to demonstrate how the design principles and implementation strategies outlined in previous sections translate into measurable operational benefits and performance improvements in a realistic enterprise context.</p>"},{"location":"research/publications/messaging-architecture/#51-evaluation-objectives","title":"5.1 Evaluation Objectives","text":"<p>The evaluation was structured to address the following objectives:</p> <ul> <li>Scalability: Assess the system\u2019s ability to elastically handle dynamic workloads through horizontal pod autoscaling and efficient resource utilization.</li> <li>Resiliency: Evaluate fault tolerance and recovery mechanisms in response to pod, node, or network failures.</li> <li>Deployment Efficiency: Quantify the operational gains achieved by integrating ArgoCD-based GitOps workflows and Terraform-driven Infrastructure as Code (IaC).</li> <li>Observability: Examine the effectiveness of real-time monitoring, logging, and distributed tracing in identifying and resolving performance issues.</li> <li>Security Compliance: Verify adherence to enterprise security policies, including authentication, authorization, and secrets management.</li> </ul>"},{"location":"research/publications/messaging-architecture/#52-experimental-testbed","title":"5.2 Experimental Testbed","text":"<p>Experiments were conducted on a dedicated, non-production Azure Kubernetes Service (AKS) environment architected to emulate large-scale, enterprise-grade workloads:</p> <ul> <li>Cluster Configuration:<ul> <li>3-node system pool (DS3v2) for control-plane and system add-ons</li> <li>6-node application pool (DS4v2) for stateless microservices, supporting high concurrency</li> <li>2-node platform pool for shared infrastructure services (monitoring, ingress, CI/CD)</li> </ul> </li> <li>Infrastructure Management: All resources provisioned end-to-end via Terraform, including VNet, subnets, managed identities, RBAC, and Key Vault integrations</li> <li>Service Deployment: Automated using Helm charts and ArgoCD GitOps workflows for declarative, continuous delivery</li> <li>Workload Composition:<ul> <li>50+ containerized microservices, each representing distinct business capabilities (e.g., payments, billing, user profiles, analytics, notifications, reporting, authentication, inventory, order processing, audit, etc.)</li> <li>Microservices distributed across at least five namespaces (e.g., <code>finance-app</code>, <code>user-management</code>, <code>platform-services</code>, <code>analytics</code>, <code>operations</code>) to reflect domain-driven design and multi-team ownership</li> <li>Each microservice integrated with Azure Event Hub (for asynchronous, event-driven communication) and Azure Database for PostgreSQL (for persistent storage)</li> <li>Services designed with varying resource profiles and scaling policies to simulate heterogeneous real-world workloads</li> </ul> </li> <li>Load Simulation: Synthetic traffic generated using Locust and K6, simulating 1,000\u201310,000+ concurrent requests per second, with randomized payloads, inter-arrival times, and burst patterns to emulate realistic enterprise transaction volumes</li> <li>Observability Stack: Cluster-wide monitoring and distributed tracing enabled via Prometheus, Grafana, Azure Monitor, and OpenTelemetry sidecars</li> </ul> <p>This testbed configuration ensured a robust, production-like environment for evaluating the scalability, resiliency, and operational characteristics of the proposed architecture at significant scale.</p>"},{"location":"research/publications/messaging-architecture/#53-performance-metrics-and-measurement","title":"5.3 Performance Metrics and Measurement","text":"<p>To ensure a comprehensive evaluation, both system-level and service-level metrics were collected using Prometheus, Grafana, and Azure Monitor. The following table summarizes the key metrics, their purposes, measurement tools, and target thresholds:</p> Metric Purpose Measurement Tool Target/Threshold Average Response Time Measure request latency during load Prometheus + Grafana &lt; 200 ms under normal load Autoscaling Latency Time to scale from baseline to peak load Azure Monitor &lt; 30 seconds CPU &amp; Memory Utilization Resource efficiency across services Prometheus 65\u201375% utilization Deployment Time From Git commit to cluster sync via ArgoCD ArgoCD Metrics &lt; 5 minutes Recovery Time Time to recover after pod/node failure K8s Event Logs &lt; 20 seconds Security Policy Compliance RBAC, network policy, and secret validation OPA/Gatekeeper 100% compliance"},{"location":"research/publications/messaging-architecture/#54-results-and-analysis","title":"5.4 Results and Analysis","text":"<p>Scalability and Performance</p> <p>The architecture exhibited linear scalability under increasing load, with the Horizontal Pod Autoscaler (HPA) responding to CPU and memory thresholds within an average of 24 seconds. Response times remained below 180 ms for up to 8,000 concurrent requests per second, and no service experienced throttling or timeouts at peak load. These results confirm the elasticity and robustness of the AKS-based design.</p> <p>Deployment Efficiency</p> <p>The ArgoCD-powered GitOps workflow achieved deployment synchronization in under 4 minutes\u2014a 62% improvement over manual CI/CD processes. Combined with Terraform-based provisioning, environment setup time was reduced from 8 hours to under 90 minutes. Versioned infrastructure and declarative deployments enabled full traceability and rapid rollback, enhancing operational governance.</p> <p>Resiliency and Fault Tolerance</p> <p>Pod termination and node restart scenarios validated the system\u2019s self-healing capabilities. Failed pods were rescheduled within 15 seconds, maintaining uninterrupted service. The load balancer and ingress controller efficiently rerouted traffic during disruptions, ensuring high availability even under transient failures.</p> <p>Observability and Monitoring</p> <p>The integration of Prometheus, OpenTelemetry, and Azure Monitor provided near real-time visibility across all services. Distributed traces captured end-to-end transaction paths, identifying dependency latency in under 300 ms. Grafana dashboards unified cluster health, resource usage, and application performance, reducing mean time to detect (MTTD) from 12 minutes to less than 3 minutes.</p> <p>Security and Compliance</p> <p>Security validation confirmed 100% compliance with organizational policies. Authentication (Okta) and authorization (Keycloak) operated seamlessly with token-based service-to-service communication. All secrets and credentials were securely managed in Azure Key Vault, with RBAC enforcing least-privilege access. Network segmentation via Kubernetes Network Policies prevented unauthorized cross-namespace access.</p>"},{"location":"research/publications/messaging-architecture/#55-discussion","title":"5.5 Discussion","text":"<p>The evaluation demonstrates that the proposed architecture meets its core design objectives. The combination of Terraform-based IaC, ArgoCD-driven GitOps, and automated manifest generation delivers a reproducible, resilient, and observable system suitable for enterprise workloads. Scalability was validated under intensive concurrent workloads, and automated provisioning and deployment pipelines significantly improved developer productivity and release reliability.</p> <p>The observability stack provided actionable insights for proactive anomaly detection, while robust security controls\u2014especially centralized secrets management and policy enforcement\u2014ensured compliance and minimized risk. Although the integration of multiple tools (Terraform, Helm, ArgoCD, Prometheus) introduces operational complexity, the automation and governance benefits far outweigh the orchestration overhead. Future enhancements may include AI-driven autoscaling and predictive anomaly detection to further optimize performance and cost.</p>"},{"location":"research/publications/messaging-architecture/#summary_3","title":"Summary","text":"<p>The evaluation confirms that the architecture meets its objectives for scalability, resilience, and operational efficiency. Results demonstrate that automation and observability are key enablers for reliable, high-performance cloud-native messaging systems.</p>"},{"location":"research/publications/messaging-architecture/#6-implementation-insights-and-recommendations","title":"6. Implementation Insights and Recommendations","text":"<p>The successful deployment and operation of a cloud-native messaging architecture require more than just technical design\u2014it demands disciplined implementation practices and a strategic approach to operationalization. Drawing from the empirical evaluation and real-world deployment experience, this section provides practical insights and actionable recommendations to guide practitioners in building, scaling, and maintaining robust event-driven systems on Azure Kubernetes Service (AKS). The following guidance addresses key areas such as Infrastructure as Code (IaC), GitOps workflows, observability, and security, offering a blueprint for organizations aiming to achieve reliability, agility, and compliance in production environments.</p>"},{"location":"research/publications/messaging-architecture/#61-translating-evaluation-findings-into-practice","title":"6.1 Translating Evaluation Findings into Practice","text":"<p>The evaluation results underscore the technical maturity and operational efficiency achieved through declarative, event-driven architectures on Azure Kubernetes Service (AKS). This section distills those findings into actionable insights and recommendations for enterprises seeking to adopt or scale similar architectures. The focus is on the integration of Infrastructure as Code (IaC), GitOps-based continuous delivery, and observability-driven automation as enablers of consistent, secure, and scalable operations.</p>"},{"location":"research/publications/messaging-architecture/#62-infrastructure-as-code-iac-with-terraform","title":"6.2 Infrastructure as Code (IaC) with Terraform","text":"<p>Terraform\u2019s declarative provisioning model is foundational for repeatability and compliance. The following practices are critical for enterprise-scale IaC adoption:</p> <ul> <li>Modular Design: Develop reusable Terraform modules for core components\u2014AKS clusters, Key Vaults, Application Gateways, and Managed Identities. Modularization improves maintainability and facilitates environment reusability.</li> <li>Secure Remote State Management: Store Terraform state in an Azure Storage Account with locking enabled via Blob leases. This prevents concurrent updates and ensures state integrity across distributed teams.</li> <li>Automated Environment Promotion: Implement environment promotion pipelines in Azure DevOps or GitHub Actions, enabling infrastructure changes to flow automatically from development through testing to production, with governance approval gates as needed.</li> <li>Policy as Code Integration: Enforce compliance by embedding Azure Policy or Terraform Sentinel checks, ensuring all provisioned resources adhere to security baselines such as private clusters, encryption at rest, and Key Vault integration.</li> </ul>"},{"location":"research/publications/messaging-architecture/#63-gitops-with-argocd","title":"6.3 GitOps with ArgoCD","text":"<p>Transitioning from traditional CI/CD pipelines to a GitOps-driven deployment model with ArgoCD significantly improves deployment predictability, rollback accuracy, and auditability.</p> <p>Key recommendations include:</p> <ul> <li>Single Source of Truth: Maintain all Kubernetes manifests, Helm charts, and Kustomize overlays in dedicated Git repositories for full version control and reproducibility.</li> <li>\u201cApp of Apps\u201d Pattern: Use ArgoCD\u2019s hierarchical management to orchestrate multiple applications or namespaces from a central controller, supporting multi-tenant environments while preserving application-level autonomy.</li> <li>Automated Manifest Generation: Integrate automation tools such as Helmfile, Kustomize, or Terraform providers to dynamically generate manifests, reducing YAML duplication and configuration errors.</li> <li>Sync Policy Optimization: Configure automated synchronization for lower environments and manual approval for production clusters to balance agility with control.</li> <li>Drift Detection and Rollback: Enable drift detection and automated rollback in ArgoCD to ensure production environments remain continuously aligned with the Git repository state.</li> </ul>"},{"location":"research/publications/messaging-architecture/#64-observability-and-resilience-engineering","title":"6.4 Observability and Resilience Engineering","text":"<p>Comprehensive observability is essential for operational transparency and proactive issue resolution. Key insights include:</p> <ul> <li>Unified Metrics Architecture: Integrate Prometheus, Grafana, and Azure Monitor for real-time telemetry, supported by Azure Log Analytics for centralized cross-cluster data aggregation.</li> <li>Alert-Driven Self-Healing: Combine monitoring alerts with automation frameworks such as KEDA or Argo Rollouts to enable autonomous remediation actions (e.g., scaling or pod replacement).</li> <li>End-to-End Traceability: Implement correlation IDs across microservices to enable distributed tracing and facilitate root cause analysis in asynchronous, event-driven systems.</li> <li>Chaos and Resilience Validation: Use Azure Chaos Studio or equivalent tooling to inject controlled faults, validating autoscaling behavior and recovery under simulated stress conditions.</li> </ul>"},{"location":"research/publications/messaging-architecture/#65-security-and-compliance-integration","title":"6.5 Security and Compliance Integration","text":"<p>Security and governance are intrinsic to all layers of the architecture. The following recommendations ensure alignment with enterprise-grade compliance frameworks:</p> <ul> <li>Centralized Secret Management: Store and manage sensitive configurations exclusively in Azure Key Vault, integrated with AKS via Managed Identities to eliminate hardcoded secrets.</li> <li>Granular RBAC and Network Controls: Define namespace-scoped roles and apply Kubernetes network policies to minimize lateral movement between services.</li> <li>Continuous Image and Dependency Scanning: Integrate security scanning tools (e.g., Microsoft Defender for Containers, Trivy) within CI pipelines to prevent vulnerable images from being deployed.</li> <li>Automated Auditability: Combine ArgoCD audit logs, Terraform plan histories, and Azure Policy compliance reports to maintain end-to-end traceability of infrastructure and application changes.</li> </ul>"},{"location":"research/publications/messaging-architecture/#66-organizational-and-process-recommendations","title":"6.6 Organizational and Process Recommendations","text":"<p>Beyond technical tooling, successful adoption requires organizational adaptation and process alignment:</p> <ul> <li>Platform Engineering Teams: Establish dedicated teams responsible for maintaining shared IaC modules, GitOps templates, and cluster governance standards across environments.</li> <li>Standardized Deployment Pipelines: Implement consistent deployment templates that apply uniformly across all business units and environments, improving predictability and reducing onboarding time.</li> <li>Feedback-Driven Continuous Improvement: Use observability metrics and postmortem analyses to drive iterative refinements in reliability, cost optimization, and deployment performance.</li> </ul>"},{"location":"research/publications/messaging-architecture/#67-lessons-learned","title":"6.7 Lessons Learned","text":"<p>Practical experience with the proposed system yielded several critical insights:</p> <ul> <li>Declarative automation through Terraform and ArgoCD drastically reduces operational drift but requires early investment in repository design and process governance.</li> <li>Observability-first design enhances reliability when introduced at inception rather than retrofitted post-deployment.</li> <li>Governance achieves greater adoption when it is automated and transparent, rather than enforced through manual approval chains.</li> </ul>"},{"location":"research/publications/messaging-architecture/#summary_4","title":"Summary","text":"<p>This section distills practical lessons and actionable recommendations for implementing scalable, secure, and observable cloud-native messaging systems. It emphasizes the importance of automation, GitOps, and continuous improvement to achieve operational excellence in Kubernetes-based environments.</p>"},{"location":"research/publications/messaging-architecture/#7-future-work-and-research-directions","title":"7. Future Work and Research Directions","text":"<p>As cloud-native messaging architectures continue to evolve, new challenges and opportunities emerge at the intersection of scalability, intelligence, and sustainability. While the current solution demonstrates robust performance and operational maturity, the rapid pace of technological advancement calls for ongoing research and innovation. The following vision diagram and subsections outline promising directions for future work, including the integration of AI-driven automation, autonomous remediation, intelligent observability, and sustainable multi-cloud strategies. These areas represent the next frontier in building resilient, adaptive, and policy-aware enterprise platforms.</p> <p>Figure 7. Future Work Vision Diagram</p> <p></p> <p>Figure 6: Vision for future research\u2014showing the evolution from current cloud-native platforms to intelligent, sustainable, and policy-aware architectures, including AI-driven autoscaling, autonomous remediation, intelligent observability, policy-driven governance, sustainability, and multi-cloud/edge integration.</p>"},{"location":"research/publications/messaging-architecture/#71-motivation-for-continued-research","title":"7.1 Motivation for Continued Research","text":"<p>While the proposed event-driven microservices architecture on Azure Kubernetes Service (AKS) demonstrates significant improvements in scalability, reliability, and automation, the field is rapidly evolving. Emerging trends in AI-driven automation, adaptive scaling, and autonomous operations present new opportunities to further enhance Kubernetes-based enterprise platforms. This section outlines prospective research and development directions that can extend the capabilities of the presented architecture, addressing challenges in predictive management, cost optimization, and intelligent observability.</p>"},{"location":"research/publications/messaging-architecture/#72-ai-driven-autoscaling-and-resource-optimization","title":"7.2 AI-Driven Autoscaling and Resource Optimization","text":"<p>Traditional autoscaling mechanisms, such as the Horizontal Pod Autoscaler (HPA) and Kubernetes Event-Driven Autoscaler (KEDA), are reactive and lack predictive intelligence. Future research should explore AI-enhanced autoscaling algorithms capable of learning workload patterns, forecasting demand surges, and preemptively allocating resources.</p> <p>Potential directions include:</p> <ul> <li>Predictive Modeling: Develop machine learning models that analyze historical workload trends to forecast CPU, memory, and I/O demands in advance.</li> <li>Reinforcement Learning for Scaling Decisions: Train reinforcement learning agents to dynamically optimize scaling thresholds based on performance and cost trade-offs.</li> <li>Multi-Metric Adaptation: Move beyond CPU/memory-based triggers to incorporate business-level metrics (e.g., transaction volume, API latency) for context-aware scaling.</li> </ul> <p>Such models could be integrated into Kubernetes controllers or external orchestrators, resulting in smarter, self-optimizing infrastructure.</p>"},{"location":"research/publications/messaging-architecture/#73-autonomous-remediation-and-self-healing-systems","title":"7.3 Autonomous Remediation and Self-Healing Systems","text":"<p>While the current architecture incorporates self-healing through declarative states and ArgoCD synchronization, future systems could employ autonomous remediation mechanisms driven by anomaly detection and policy-based reasoning.</p> <p>Research opportunities include:</p> <ul> <li>Anomaly Detection Pipelines: Use AI models to detect early signs of degradation or misconfiguration across microservices using real-time observability data.</li> <li>Intent-Based Policies: Encode operational intents (e.g., \u201cmaintain 99.9% uptime\u201d) into machine-readable policies, allowing controllers to take corrective actions automatically.</li> <li>Automated Root Cause Analysis: Combine log aggregation and graph-based causal inference to reduce mean time to recovery (MTTR) by identifying the true source of cascading failures.</li> </ul> <p>This direction aligns with the concept of AIOps (Artificial Intelligence for IT Operations), where machine learning augments DevOps workflows to achieve autonomous system resilience.</p>"},{"location":"research/publications/messaging-architecture/#74-intelligent-observability-and-anomaly-prediction","title":"7.4 Intelligent Observability and Anomaly Prediction","text":"<p>Current observability practices focus on descriptive and diagnostic capabilities. Future advancements can elevate observability toward predictive and prescriptive modes, where systems not only detect issues but also recommend or execute corrective measures.</p> <p>Key directions include:</p> <ul> <li>Cognitive Dashboards: Enhance Grafana or Azure Monitor dashboards with AI-driven insights, summarizing anomalies, trends, and recommended remediations.</li> <li>Cross-Layer Correlation: Correlate data across infrastructure, application, and business layers to identify performance degradation that traditional metrics may overlook.</li> <li>Behavioral Profiling: Establish dynamic performance baselines for microservices using time-series analysis and unsupervised learning to detect deviations automatically.</li> </ul> <p>Such intelligent observability systems can serve as the foundation for proactive incident management and context-aware alerting, reducing operational noise and human intervention.</p>"},{"location":"research/publications/messaging-architecture/#75-policy-driven-governance-and-compliance-automation","title":"7.5 Policy-Driven Governance and Compliance Automation","text":"<p>As organizations scale their Kubernetes environments, maintaining governance becomes increasingly complex. Future research should emphasize policy-driven compliance automation, ensuring that every deployment adheres to corporate, regulatory, and security policies without manual oversight.</p> <p>Promising research areas include:</p> <ul> <li>Declarative Governance Frameworks: Extend GitOps workflows with policy repositories managed as code, integrating tools such as Open Policy Agent (OPA) and Kyverno.</li> <li>Adaptive Compliance Monitoring: Introduce AI models that continuously assess policy adherence, adjusting enforcement dynamically based on context (e.g., environment, region, data sensitivity).</li> <li>Automated Remediation of Violations: Enable systems to automatically revert or block noncompliant configurations at the admission controller level before deployment.</li> </ul> <p>This aligns with the broader enterprise shift toward \u201ccompliance as code,\u201d a key paradigm for regulated industries adopting cloud-native infrastructure.</p>"},{"location":"research/publications/messaging-architecture/#76-sustainable-and-cost-aware-kubernetes-operations","title":"7.6 Sustainable and Cost-Aware Kubernetes Operations","text":"<p>Sustainability and cost efficiency are becoming critical non-functional requirements for enterprise systems. Future work could explore energy- and cost-aware orchestration models that leverage AI to balance performance with sustainability metrics.</p> <p>Potential research paths include:</p> <ul> <li>Carbon-Aware Scheduling: Integrate cloud carbon footprint data into Kubernetes schedulers to prioritize nodes in regions with lower emission intensities.</li> <li>Cost Predictive Analytics: Develop models to forecast cost implications of scaling actions, helping organizations optimize workload placement and resource provisioning strategies.</li> <li>Dynamic Spot Instance Utilization: Combine predictive scaling with spot-instance scheduling for compute-intensive but fault-tolerant workloads to minimize operational expenditure.</li> </ul>"},{"location":"research/publications/messaging-architecture/#77-integration-with-multi-cloud-and-edge-environments","title":"7.7 Integration with Multi-Cloud and Edge Environments","text":"<p>As enterprises evolve toward distributed cloud and edge computing, the proposed AKS-based model can be extended to multi-cloud and edge-native deployments. Key areas for exploration include:</p> <ul> <li>Federated Control Planes: Use Kubernetes Federation (KubeFed) or Azure Arc to manage hybrid workloads consistently across on-premises, edge, and multiple cloud providers.</li> <li>Latency-Aware Scheduling: Apply edge intelligence for location-based workload placement to meet stringent latency requirements in real-time analytics or IoT systems.</li> <li>Unified GitOps and IaC Workflows: Extend Terraform and ArgoCD pipelines to operate seamlessly across heterogeneous environments while preserving declarative consistency.</li> </ul>"},{"location":"research/publications/messaging-architecture/#78-academic-and-industry-collaboration-opportunities","title":"7.8 Academic and Industry Collaboration Opportunities","text":"<p>The intersection of AI, cloud-native computing, and automation offers a rich landscape for collaboration between academia and industry. Research partnerships could focus on:</p> <ul> <li>Developing standardized benchmarks for AI-driven autoscaling</li> <li>Creating open-source reference architectures for intelligent observability</li> <li>Evaluating sustainability metrics for enterprise-grade Kubernetes operations</li> </ul> <p>Such collaborations would accelerate innovation and bridge the gap between theoretical advancements and operational adoption in enterprise contexts.</p>"},{"location":"research/publications/messaging-architecture/#summary_5","title":"Summary","text":"<p>The next frontier of cloud-native systems lies in the fusion of automation, intelligence, and policy. AI-driven decision-making, predictive resource management, and compliance automation are poised to redefine the future of Kubernetes operations. By building on the foundations established in this study\u2014GitOps, Terraform-based IaC, and observability integration\u2014enterprises can evolve toward self-adaptive, sustainable, and policy-aware Kubernetes ecosystems that align with both technical excellence and organizational agility.</p>"},{"location":"research/publications/messaging-architecture/#conclusion","title":"Conclusion","text":"<p>This study has presented a comprehensive evaluation of high-throughput cloud-native messaging architectures, focusing on Pub/Sub microservices deployed on Kubernetes and integrated with Azure Event Hub. Through detailed analysis of architectural design, implementation strategies, and empirical performance metrics, we have demonstrated both the capabilities and limitations of cloud-native messaging systems in supporting large-scale, real-time workloads.</p> <p>The results indicate that leveraging Azure Event Hub as the core messaging backbone, in conjunction with Kubernetes orchestration, enables elastic scaling, low-latency message delivery, and fault-tolerant operations. Performance benchmarks reveal that throughput and latency are significantly influenced by microservice deployment strategies, CNI network configurations, and resource allocation policies within Kubernetes clusters. Systematic tuning of these parameters can yield substantial improvements in throughput and end-to-end latency, ensuring robust and reliable event-driven communication across distributed microservices.</p> <p>Moreover, this work underscores the critical importance of observability and monitoring in cloud-native messaging environments. The integration of native monitoring solutions such as Azure Monitor, combined with custom metrics, enables proactive detection of bottlenecks, resource contention, and potential system failures. This proactive approach supports self-healing capabilities and ensures sustained high performance in dynamic production environments.</p> <p>In summary, the combination of Kubernetes-managed microservices and Azure Event Hub provides a robust foundation for building scalable, resilient, and high-throughput messaging platforms. Future research may extend this work by exploring multi-cloud deployments, advanced autoscaling policies leveraging AI-driven insights, and serverless integration to further optimize resource utilization and reduce operational overhead in large-scale distributed systems.</p>"},{"location":"research/publications/messaging-architecture/#references","title":"References","text":""},{"location":"research/publications/messaging-architecture/#journal-articles-conference-papers-and-academic-sources","title":"Journal Articles, Conference Papers, and Academic Sources","text":"<ol> <li>Gokhale, A. (2021). A comprehensive performance evaluation of different Kubernetes CNI plugins. Vanderbilt University. Available at https://www.dre.vanderbilt.edu/~gokhale/WWW/papers/IC2E21_CNI_Eval.pdf [Accessed: Oct. 22, 2025].</li> <li>Mittal, A., &amp; Singh, R. (2023). Next-generation event-driven architectures: Performance analysis of Kafka, Pulsar, and serverless. arXiv. https://arxiv.org/html/2510.04404v1 [Accessed: Oct. 22, 2025].</li> <li>Henning, S., &amp; Hasselbring, W. (2023). Benchmarking scalability of stream processing frameworks deployed as microservices in the cloud. arXiv. https://arxiv.org/abs/2303.11088 [Accessed: Oct. 22, 2025].</li> <li>MDPI. (2024). Performance and latency efficiency evaluation of Kubernetes CNI plugins. Electronics, 13(19), 3972. https://www.mdpi.com/2079-9292/13/19/3972 [Accessed: Oct. 22, 2025].</li> <li>Larsson, L., T\u00e4rneberg, W., Klein, C., Elmroth, E., &amp; Kihl, M. (2020). Impact of etcd Deployment on Kubernetes, Istio, and Application Performance. arXiv. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Dragoni, N., Giallorenzo, S., Lafuente, A. L., Mazzara, M., Montesi, F., Mustafin, R., &amp; Safina, L. (2017). Microservices: Yesterday, Today, and Tomorrow. Present and Ulterior Software Engineering, 195-216. https://arxiv.org/abs/1606.04036 [Accessed: Oct. 22, 2025].</li> <li>MDPI. (2024). Performance and latency efficiency evaluation of Kubernetes CNI plugins. Electronics, 13(19), 3972. https://www.mdpi.com/2079-9292/13/19/3972 [Accessed: Oct. 22, 2025].</li> <li>Sampaio, A. R. (2019). Improving microservice-based applications with runtime adaptation mechanisms. Journal of Information Systems and Applications. https://jisajournal.springeropen.com/articles/10.1186/s13174-019-0104-0 [Accessed: Oct. 22, 2025].</li> <li>Tasmin, F., Poudel, S., &amp; Tareq, A. H. (2025). Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks. arXiv. https://arxiv.org/abs/2510.04404 [Accessed: Oct. 22, 2025].</li> <li>Vladutu, C. (2025). Azure Event Grid vs Azure Service Bus vs Event Hubs: When to Use Each. Medium. https://cosmin-vladutu.medium.com/azure-event-grid-vs-azure-service-bus-vs-event-hubs-when-to-use-each-12900bb32ce8 [Accessed: Oct. 22, 2025].</li> <li>Burns, B., Grant, B., Oppenheimer, D., Brewer, E., &amp; Wilkes, J. (2016). Borg, Omega, and Kubernetes. Communications of the ACM, 59(5), 50-57. https://dl.acm.org/doi/10.1145/2890784 [Accessed: Oct. 22, 2025].</li> <li>Kreps, J., Narkhede, N., &amp; Rao, J. (2011). Kafka: A Distributed Messaging System for Log Processing. Proceedings of the NetDB, 1-7. https://dl.acm.org/doi/10.1145/2890784 [Accessed: Oct. 22, 2025].</li> <li>Zhou, Y., Li, X., &amp; Wang, J. (2020). Performance Evaluation of Kubernetes-based Publish/Subscribe Systems. Proceedings of the 2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom), 1-8. https://arxiv.org/abs/2303.11088 [Accessed: Oct. 22, 2025].</li> <li>Li, X., Zhou, Y., &amp; Wang, J. (2019). Throughput and Latency Analysis of Cloud-based Event Streaming Services. Proceedings of the 2019 IEEE International Conference on Big Data (Big Data), 1-10. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Gupta, A., Sharma, S., &amp; Sood, S. K. (2021). Auto-Scaling in Cloud Computing: A Review, Challenges, and Research Directions. Future Generation Computer Systems, 117, 322-339. https://www.sciencedirect.com/science/article/pii/S0167739X20331513 [Accessed: Oct. 22, 2025].</li> <li>Chen, L., &amp; Bahsoon, R. (2017). Self-Adaptive and Self-Healing Systems: A Survey. IEEE Access, 5, 16521-16555. https://arxiv.org/abs/2004.00372 [Accessed: Oct. 22, 2025].</li> <li>Di Francesco, P., Lago, P., &amp; Malavolta, I. (2019). Research on Architecting Microservices: Trends, Focus, and Potential for Industrial Adoption. Proceedings of the 2019 IEEE International Conference on Software Architecture (ICSA), 21-30. https://arxiv.org/abs/1606.04036 [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"research/publications/messaging-architecture/#books-and-technical-reports","title":"Books and Technical Reports","text":"<ol> <li>ResearchGate. (2023). Cloud-native architectures: A comparative analysis of Kubernetes and serverless computing. https://www.researchgate.net/publication/388717188_Cloud-Native_Architectures_A_Comparative_Analysis_of_Kubernetes_and_Serverless_Computing [Accessed: Oct. 22, 2025].</li> <li>Medium. (2022). The cloud-native architecture and the cloud-native data architecture. https://medium.com/cloud-and-data-gurus/the-cloud-native-architecture-and-the-cloud-native-data-architecture-17aeabe46e83 [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"research/publications/messaging-architecture/#web-resources-and-documentation","title":"Web Resources and Documentation","text":"<ol> <li>Dapr. (2025). Pub/sub building block overview. Dapr Documentation. https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-overview/ [Accessed: Oct. 22, 2025].</li> <li>Gitconnected. (2021). Event-driven systems: A deep dive into pub/sub architecture. Level Up Coding. https://levelup.gitconnected.com/event-driven-systems-a-deep-dive-into-pubsub-architecture-39e416be913c [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Introduction to Azure Event Hubs. Microsoft Learn. https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-about [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Azure Event Hubs for Apache Kafka introduction. Microsoft Learn. https://kubernetes.anjikeesari.com/azure/16-event-hubs-part-1/ [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Performance and scale guidance for Event Hubs with Azure Functions. Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/serverless/event-hubs-functions/performance-scale [Accessed: Oct. 22, 2025].</li> <li>R. Cloud Architect. (2021). Scaling microservices with Azure Kubernetes Service (AKS) and event-driven architecture. https://roshancloudarchitect.me/scaling-microservices-with-azure-kubernetes-service-aks-and-event-driven-architecture-74900e350447 [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Advanced Azure Kubernetes Service (AKS) microservices architecture. Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices-advanced [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Microservices architecture on Azure Kubernetes Service (AKS). Microsoft Learn. https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Monitor Kubernetes clusters using Azure Monitor and cloud-native tools. Microsoft Learn. https://learn.microsoft.com/en-us/azure/azure-monitor/containers/monitor-kubernetes [Accessed: Oct. 22, 2025].</li> <li>Microsoft. (2025). Architecture best practices for Azure Kubernetes Service (AKS). Microsoft Learn. https://learn.microsoft.com/en-us/azure/well-architected/service-guides/azure-kubernetes-service [Accessed: Oct. 22, 2025].</li> </ol>"},{"location":"resources/","title":"Learning Resources","text":"<p>Practical resources to accelerate your cloud and software development journey.</p>"},{"location":"resources/#video-tutorials","title":"Video Tutorials","text":"<p>Step-by-step video guides covering real-world implementation patterns and enterprise-grade solutions:</p>"},{"location":"resources/#azure-reference-architecture-guide","title":"Azure Reference Architecture Guide","text":"<p>Learn how to build comprehensive Azure architectures suitable for enterprise organizations. Covers networking, security, DevOps CI/CD, and major Azure services through hands-on labs.</p> <p>Topics: Virtual Networks, App Service Environment, Application Gateway, ARM Templates, Azure DevOps Pipelines Duration: 16+ labs | Level: Intermediate to Advanced</p>"},{"location":"resources/#net-core-web-api-clean-architecture","title":".NET Core Web API - Clean Architecture","text":"<p>Complete guide to building RESTful APIs with ASP.NET Core using clean architecture principles. Includes Entity Framework Core, Azure Key Vault, health checks, and deployment automation.</p> <p>Topics: Clean Architecture, CRUD Operations, API Best Practices, Unit Testing, Azure Deployment Duration: 8 labs | Level: Intermediate</p> <p>Have suggestions for resources you'd like to see? Let me know \u2192</p>"},{"location":"resources/azure-cloud/","title":"Azure Reference Architecture","text":""},{"location":"resources/azure-cloud/#azure-cloud-reference-architecture-guide","title":"Azure Cloud - Reference Architecture Guide","text":""},{"location":"resources/azure-cloud/#introduction","title":"Introduction","text":"<p>Build enterprise-grade Azure architectures from the ground up. This hands-on video series guides you through designing and deploying a complete cloud infrastructure suitable for organizations of any scale, using real-world patterns and Azure DevOps automation.</p> <p>What You'll Learn: Azure networking fundamentals, security design, App Service Environments, Application Gateway configuration, ARM template infrastructure as code, and automated CI/CD pipelines for production deployments.</p> <p>Best For: Cloud architects, DevOps engineers, and developers responsible for designing, implementing, or maintaining Azure cloud infrastructure. </p>"},{"location":"resources/azure-cloud/#youtube-videos","title":"YouTube videos","text":"Labs Title Lab-1: Introduction &amp; Course Overview Lab-1.1: Create new Azure Account or Tenant or Subscription Lab-2: Application High Level Architecture Lab-3: Azure Architecture Design Principles Lab-4: Azure High Level Architecture (HLD) Lab-5: Azure Architecture - Security Design Lab-6: Azure Architecture Networking Design Lab-7: Azure DevOps Deployment View for Azure Resources Lab-8: Azure Setup Guide / Governance Lab-9: Azure Resource Naming &amp; Tagging Conventions Lab-10: ARM Template Project setup &amp; Create Resource Group Lab-11: Azure Virtual Network Concepts &amp; Create Virtual Network Lab-12: Create Azure AppService Environment from Azure Portal Lab-12.1: Create Azure AppService Environment using ARM template Lab-12.2: Create Tags in ASE using ARM templates Lab-13: Create Azure App Service Plan from Azure Portal Lab-13.1: Create Azure App Service Plan and App Services using ARM Template Lab-14: Deploying ARM Templates using Azure DevOps Pipelines Lab-15:  Deploy a website or APIs to Azure with Azure App Service Lab-16:  Create Azure Application Gateway"},{"location":"resources/internship-program/","title":"6-Month Live Internship Program","text":"<p>Launch Your Career with Real-World Projects Industry Mentors | Hands-On Experience | Job-Ready Portfolio</p>"},{"location":"resources/internship-program/#who-is-this-for","title":"Who Is This For?","text":"<p>\u2713 B.Tech Final-Year Students - Gain industry experience while completing your degree \u2713 Fresh Graduates (2023-2026) - Bridge the gap between education and employment \u2713 Career Switchers in Tech - Transition into high-demand tech roles \u2713 Job Seekers - Build practical skills that employers actually want  </p>"},{"location":"resources/internship-program/#program-overview","title":"Program Overview","text":"<p>This 6-month live internship provides students and early-career professionals with production-grade experience in cloud architecture, AI/ML systems, and enterprise software development. Work on real projects with dedicated mentorship that transforms learning into earning.</p> <p>Duration: 6 Months (Full Internship Experience) Format: Online + Optional In-Person Workshops Commitment: 15-20 hours/week (flexible for students) or full-time Batch Size: Small groups (20-25 interns) for personalized mentorship</p>"},{"location":"resources/internship-program/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p>Cloud Architecture &amp; Infrastructure</p> <ul> <li>Azure cloud services and architecture patterns</li> <li>Kubernetes (AKS) deployment and management</li> <li>Infrastructure as Code with Terraform</li> <li>CI/CD pipelines with Azure DevOps</li> <li>Cloud security and monitoring best practices</li> </ul> </li> <li> <p>AI/ML &amp; Intelligent Systems</p> <ul> <li>AI Agent development and integration</li> <li>Large Language Models (LLMs) implementation</li> <li>Machine learning model deployment</li> <li>AI-powered automation solutions</li> <li>Prompt engineering and RAG systems</li> </ul> </li> <li> <p>Software Development</p> <ul> <li>.NET Core / ASP.NET Core Web APIs</li> <li>React.js front-end development</li> <li>Microservices architecture</li> <li>RESTful API design</li> <li>Clean architecture principles</li> <li>Database design and optimization</li> </ul> </li> </ul>"},{"location":"resources/internship-program/#internship-tracks-technology-stack","title":"Internship Tracks &amp; Technology Stack","text":""},{"location":"resources/internship-program/#track-1-ai-machine-learning","title":"Track 1: AI &amp; Machine Learning","text":"<p>Build intelligent systems with modern AI technologies</p> <p>Technologies: Python \u2022 TensorFlow \u2022 PyTorch \u2022 Scikit-learn \u2022 Pandas \u2022 NumPy \u2022 Jupyter Projects: Predictive models, Natural Language Processing apps, Computer Vision applications, AI Agents, LLM integration</p>"},{"location":"resources/internship-program/#track-2-data-science-analytics","title":"Track 2: Data Science &amp; Analytics","text":"<p>Create data-driven solutions and business intelligence</p> <p>Technologies: Python \u2022 SQL \u2022 Power BI \u2022 Tableau \u2022 Excel \u2022 Apache Spark \u2022 MongoDB Projects: Data pipelines, BI dashboards, statistical analysis tools, data visualization</p>"},{"location":"resources/internship-program/#track-3-full-stack-development","title":"Track 3: Full-Stack Development","text":"<p>Build modern cloud-native web applications</p> <p>Frontend: React \u2022 TypeScript \u2022 HTML/CSS \u2022 Tailwind Backend: .NET Core \u2022 Node.js \u2022 Python FastAPI Database: PostgreSQL \u2022 MongoDB \u2022 Redis DevOps: Docker \u2022 Kubernetes \u2022 Git \u2022 CI/CD</p>"},{"location":"resources/internship-program/#track-4-cloud-platform-engineering","title":"Track 4: Cloud Platform Engineering","text":"<p>Master cloud infrastructure and DevOps</p> <p>Technologies: Azure \u2022 AWS \u2022 Terraform \u2022 Kubernetes (AKS/EKS) \u2022 GitHub Actions \u2022 Azure DevOps Projects: Multi-tier AKS deployments, CI/CD pipelines, Infrastructure as Code, monitoring and alerting</p>"},{"location":"resources/internship-program/#6-month-internship-structure","title":"6-Month Internship Structure","text":""},{"location":"resources/internship-program/#phase-1-foundation-onboarding-month-1","title":"Phase 1: Foundation &amp; Onboarding (Month 1)","text":"<p>\u2713 Technology stack deep-dive and tool mastery \u2713 Industry best practices and coding standards \u2713 Version control and collaborative workflows \u2713 Development environment setup with cloud resources</p>"},{"location":"resources/internship-program/#phase-2-mini-projects-month-2","title":"Phase 2: Mini Projects (Month 2)","text":"<p>\u2713 Build 3-4 smaller applications to reinforce concepts \u2713 Weekly code reviews with senior mentors \u2713 Agile methodology and project management basics \u2713 Technical documentation and blog writing</p>"},{"location":"resources/internship-program/#phase-3-major-capstone-project-months-3-5","title":"Phase 3: Major Capstone Project (Months 3-5)","text":"<p>\u2713 Work on production-grade application from scratch \u2713 Full software development lifecycle experience \u2713 Team collaboration and code documentation \u2713 Testing, debugging, and performance optimization \u2713 Deploy to production environment (Azure/AWS)</p>"},{"location":"resources/internship-program/#phase-4-portfolio-career-launch-month-6","title":"Phase 4: Portfolio &amp; Career Launch (Month 6)","text":"<p>\u2713 Resume building and LinkedIn optimization \u2713 Mock interviews and technical preparation \u2713 Internship certificate and project showcase \u2713 Job referrals and placement support</p>"},{"location":"resources/internship-program/#what-youll-gain","title":"What You'll Gain","text":"<p>Live Interactive Sessions - Learn from industry practitioners with real-world experience</p> <p>Real Internship Experience - 6 months of hands-on project work with tangible deliverables</p> <p>Internship Certificate - Industry-recognized certification to boost your resume</p> <p>Dedicated Mentor - Personal 1-on-1 guidance throughout the entire 6-month journey</p> <p>Production-Ready Portfolio - 4-5 complete GitHub projects to showcase to employers</p> <p>Professional Network - Connect with industry professionals and fellow interns</p> <p>Placement Assistance - Job referrals, resume reviews, and comprehensive interview prep</p> <p>Flexible Schedule - Designed for final-year students (15-20 hours/week)</p>"},{"location":"resources/internship-program/#eligibility-requirements","title":"Eligibility Requirements","text":"<ul> <li>Currently enrolled in Computer Science, Software Engineering, or related degree program (Bachelor's or Master's)</li> <li>OR recent graduate (within 1 year) looking to gain practical experience</li> <li>Basic understanding of programming (C#, Python, JavaScript, or similar)</li> <li>Familiarity with Git/GitHub version control</li> <li>Strong desire to learn and work on challenging technical problems</li> <li>Good communication skills and ability to work in a team</li> <li>Self-motivated and able to work independently with guidance</li> </ul> <p>Preferred (but not required): - Prior exposure to cloud platforms (Azure, AWS, or GCP) - Experience with Docker and containerization - Basic understanding of web development - Familiarity with AI/ML concepts</p>"},{"location":"resources/internship-program/#application-process","title":"Application Process","text":""},{"location":"resources/internship-program/#1-submit-your-application","title":"1. Submit Your Application","text":"<p>Send the following to our contact email: - Resume/CV highlighting relevant coursework and projects - Cover letter explaining your interest and preferred track - GitHub profile or portfolio (if available) - Availability and preferred start date</p>"},{"location":"resources/internship-program/#2-technical-screening","title":"2. Technical Screening","text":"<ul> <li>Brief technical assessment (coding challenge or take-home assignment)</li> <li>30-minute technical phone screen</li> </ul>"},{"location":"resources/internship-program/#3-interview","title":"3. Interview","text":"<ul> <li>1-hour virtual interview with technical team</li> <li>Discussion of your background, interests, and goals</li> <li>Overview of available projects and expectations</li> </ul>"},{"location":"resources/internship-program/#4-onboarding","title":"4. Onboarding","text":"<ul> <li>Offer letter and program details</li> <li>Access to development resources and tools</li> <li>Introduction to mentors and team</li> </ul>"},{"location":"resources/internship-program/#ready-to-apply","title":"Ready to Apply?","text":"<p>Take the next step in your career by gaining practical experience in cloud architecture, AI/ML, and enterprise software development.</p> <p>Transform learning into earning - Your bridge from campus to career!</p> <p>Contact Us to Apply \u2192</p> <p>Questions? Reach out with any questions about the program, requirements, or application process.</p>"},{"location":"resources/internship-program/#additional-resources","title":"Additional Resources","text":"<p>Before applying, explore these resources to get familiar with the technologies:</p> <ul> <li>Azure Cloud Learning Path - Video tutorials on Azure infrastructure</li> <li>.NET Core Web API Guide - Learn RESTful API development</li> <li>AI Engineering Resources - AI/ML fundamentals</li> <li>Kubernetes Cheat Sheet - Essential kubectl commands</li> <li>Docker Cheat Sheet - Container management basics</li> </ul> <p>This internship program provides educational training and hands-on project experience. We offer skill development, dedicated mentorship, and extensive placement assistance including job referrals and interview preparation. While we actively support your job search, final placement depends on individual performance, market conditions, and employer hiring decisions. Internship certificate awarded upon successful completion of all program requirements.</p>"},{"location":"resources/webapi/","title":".NET Core Web API","text":""},{"location":"resources/webapi/#net-core-web-api-clean-architecture","title":".NET Core Web API - Clean Architecture","text":""},{"location":"resources/webapi/#introduction","title":"Introduction","text":"<p>Learn to build production-ready RESTful APIs with ASP.NET Core using clean architecture principles. This comprehensive video series covers everything from project structure to Azure deployment, providing hands-on experience with industry best practices.</p> <p>What You'll Build: A complete Web API with Entity Framework Core, Azure Key Vault integration, health checks, versioning, CRUD operations, automated testing, and CI/CD deployment.</p> <p>Best For: Developers looking to master clean architecture patterns and build scalable, maintainable APIs for enterprise applications.</p>"},{"location":"resources/webapi/#youtube-videos","title":"YouTube videos","text":"Labs Title YouTube Link Lab-1: Clean Architecture Fundamentals (Theory) Lab-2: Clean Architecture Project Structure Lab-3: ASP.NET Core Web API Basic Configuration Lab-4: Azure Key Vault configuration &amp; Health Checks in ASP.NET Core REST API Lab-5: CRUD Operations In ASP.NET Core Web API with Entity Framework Core Lab-6: How to deploy .NET Core Web API to Azure from locally  How to deploy .NET Core Web API to Azure using DevOps pipeline  Create CI/CD Pipeline for ASP.NET Core Web API  Automate Build and Deployment of Azure SQL Database Lab-7: REST API Best Practices  Lab-8: Unit Testing in ASP.NET Core Web API"},{"location":"resources/webapi/#source-code","title":"Source code","text":"<p>You can access the source code on GitHub by using the following link. This will take you to the repository where all the code for our project is stored. </p> <p>GitHub Source Code Repository</p>"},{"location":"resources/books/kubernetes/","title":"Building Scalable Kubernetes Infrastructure for Microservices","text":"<p>Welcome to my e-Book, <code>Building Scalable Kubernetes Infrastructure for Microservices (A Practical Guide)</code>. In this book, I'm going to share my practical knowledge about creating scalable kubernetes cloud infrastructure for microservices architecture using azure and terraform.</p> <p>To dive into the complete contents of this eBook, simply follow the link provided below. This link will take you to the eBook's dedicated website, where you can access and explore each chapter as it becomes available. I'm excited to have you join me on this journey of learning and discovery, and I look forward to your feedback as we continue to enhance and refine this valuable resource.</p> <p>Read online</p> <p>Thank you for your interest, and happy reading!</p> <p>Note</p> <p>This book is still a work in progress, However, you are still welcome to continue reading what has been written so far.</p> <p> </p>"},{"location":"resources/books/microservices/","title":"Building Microservices with Containers: A Practical Guide","text":"<p>Welcome to my e-Book, <code>Building Microservices with Containers (A Practical Guide)</code>. In this book, I'm going to share my practical knowledge about creating  microservices with docker.</p> <p>To delve into the complete contents of this e-Book, simply download the PDF from the website's or  Read online Free e-Book</p> <p>Thank you for your interest, and happy reading!</p> <p> </p>"},{"location":"techstack/argocd/","title":"Argo CD","text":"<p>If you are new to Argo CD and want to learn and become a <code>DevSecOps Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/argocd/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Argo CD<ul> <li>What is Argo CD</li> <li>Key features and benefits</li> </ul> </li> <li>Setting up an Argo CD Environment<ul> <li>Prerequisites</li> <li>Installing Argo CD</li> <li>Setting up an Argo CD server and CLI</li> <li>Configuring users and permissions</li> </ul> </li> <li>Deploying Applications with Argo CD<ul> <li>Defining an application in a Git repository</li> <li>Adding an application to Argo CD</li> <li>Integrating with other CI/CD tools</li> <li>Updating and rolling back application deployments</li> </ul> </li> <li>Working with Argo CD configurations<ul> <li>Managing application configurations in Git</li> <li>Using Git branches and tags in Argo CD</li> <li>Handling conflicts and errors in configurations</li> <li>Implementing continuous delivery</li> </ul> </li> <li>Collaborating with Argo CD<ul> <li>Using Argo CD with Git hosting platforms</li> <li>Setting up multi-user access to Argo CD</li> </ul> </li> <li>Advanced Argo CD features<ul> <li>Using Argo CD with GitOps workflows</li> <li>Integrating Argo CD with CI/CD pipelines</li> <li>Using Argo CD with GitLab CI/CD</li> </ul> </li> </ol>"},{"location":"techstack/argocd/#books","title":"Books","text":""},{"location":"techstack/argocd/#references","title":"References","text":""},{"location":"techstack/azure-cloud/","title":"Azure Cloud","text":"<p>If you are new to Azure Cloud and want to learn and become a <code>Azure Cloud Engineer</code>, here is a list of topics you need to know:</p>"},{"location":"techstack/azure-cloud/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Azure Cloud<ul> <li>What is Microsoft Azure</li> <li>Key features and benefits</li> <li>Overview of Azure services and offerings</li> </ul> </li> <li>Getting Started with Azure<ul> <li>Setting up an Azure account</li> <li>Navigating the Azure portal</li> </ul> </li> <li>Creating your first Azure resource<ul> <li>Virtual Machines in Azure</li> <li>Overview of Azure virtual machines</li> <li>Creating and managing virtual machines</li> <li>Virtual machine sizing and scalability</li> </ul> </li> <li>Storage in Azure<ul> <li>Overview of Azure storage options</li> <li>Managing storage accounts and containers</li> <li>Blob, table, and queue storage</li> <li>Storing and retrieving files in Azure</li> </ul> </li> <li>Networking in Azure<ul> <li>Overview of Azure networking options</li> <li>Virtual networks and subnets</li> <li>Load balancing and traffic management</li> <li>ExpressRoute and VPN connectivity</li> </ul> </li> <li>Databases in Azure<ul> <li>Overview of Azure database services</li> <li>Managing SQL databases</li> <li>NoSQL options with Azure Cosmos DB</li> <li>Using Azure Database for MySQL and PostgreSQL</li> </ul> </li> <li>Practical knowledge on<ul> <li>Azure Accounts, Subscriptions, and Billing</li> <li>Manage access to Azure resources using RBAC</li> <li>Create Resource Groups</li> <li>Create Azure App Service Plan &amp; ASE</li> <li>Create Web App, API App, Mobil App</li> <li>Create Azure SQL Database, Cosmos DB</li> <li>Create Storage account</li> <li>Application Insights</li> <li>Redis Cache</li> <li>Azure API Management</li> <li>Create Azure Key Vault for secrets</li> <li>Provision Azure resources via ARM Templates &amp; Portal</li> <li>Azure Monitor &amp; Log Analytics</li> <li>Calculate Azure Pricing</li> <li>Create API Gateway using API Management</li> <li>Create Azure Application Gateway</li> <li>Create VNet, Subnet with Network Security Groups</li> </ul> </li> </ol>"},{"location":"techstack/azure-cloud/#books","title":"Books","text":""},{"location":"techstack/azure-cloud/#references","title":"References","text":"<ul> <li>Azure documentation - MSDN</li> <li>Azure Tips and Tricks</li> <li>Setup Azure SQL Server to use Azure Active Directory authentication option</li> <li>IP Subnet calculator - Vnet</li> <li>Microsoft Azure: The Big Picture - Pluralsight</li> <li>Introduction to Azure App Services - App Services</li> <li>Fundamentals of Azure Cloud Services and Storage</li> <li>Microsoft Azure | Overview of Azure Virtual Network - Virtual Network</li> <li>Import an App Service Certificate - Certificate</li> <li>Export certificate to PFX - Certificate</li> <li>How to Connect Azure Web Apps To On-Premises</li> <li>Azure Network Services Architecture - Azure Network</li> <li>Azure Load Balancer Explained (internet-facing) - step by step- Load Balancer</li> <li>Azure AD Pricing Explained !!! - Pricing</li> <li>Web Application architecture with high availability using Azure Web App - high availability</li> <li>Fundamentals of Cloud Computing - pluralsight</li> <li>Azure Network Services - Application Gateway</li> <li>Microsoft Azure services overview - Azure Architecture</li> <li>Azure Network Services Architecture - Network Architecture</li> <li>Microsoft Datacenter</li> <li>azure-quickstart-templates</li> </ul>"},{"location":"techstack/azure-cloud/#azure-api-management","title":"Azure API Management","text":"<ul> <li>Liquid template - API Manager</li> <li>Microsoft Azure API Management Essentials - API Management</li> <li>How to build a CI/CD pipeline for API Management, Part 1 | Azure Friday - CI/CD pipeline for API Management</li> <li>Build a CI/CD pipeline for API Management - CI/CD pipeline for API Management</li> <li>Azure API Management DevOps Resource Ki - API Management</li> <li>Running the Extractor - API Management</li> <li>API Management documentation - API Management</li> <li>Build a CI/CD pipeline for Azure API Management - API Management</li> <li>extracting existing configurations APIM - API Management</li> <li>aka.ms/apimlove - api-management-resources</li> <li>API Management ARM Template Creator</li> </ul>"},{"location":"techstack/azure-cloud/#azure-certifications","title":"Azure Certifications","text":"<ul> <li>New Microsoft Azure Certifications Path - Azure Certifications</li> <li>AZ-104: Microsoft Azure Administrator - AZ-104</li> <li>Microsoft Certified: Azure Solutions Architect Expert - Microsoft Certified: Azure Solutions Architect Expert - Learn</li> <li>Exam AZ-300: - certifications</li> <li>Exam AZ-301 - certifications</li> </ul>"},{"location":"techstack/azure-devops/","title":"Azure DevOps","text":"<p>If you are new to Azure DevSecOps and want to learn and become a <code>Azure DevSecOps Engineer</code>, here is a list of topics you need to know:</p>"},{"location":"techstack/azure-devops/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Azure DevOps<ul> <li>What is Azure DevOps</li> <li>Key features and benefits</li> <li>Overview of Azure DevOps services and offerings</li> </ul> </li> <li>Setting up an Azure DevOps Environment<ul> <li>Creating an Azure DevOps account</li> <li>Setting up an organization and project</li> <li>Configuring users and permissions</li> </ul> </li> <li>Source Control with Azure DevOps<ul> <li>Overview of Azure DevOps source control options</li> <li>Using Git for source control</li> <li>Managing code branches and pull requests</li> <li>Integrating with other source control tools</li> </ul> </li> <li>Work Item Tracking in Azure DevOps<ul> <li>Overview of Azure DevOps work item tracking</li> <li>Creating and managing work items</li> <li>Using Azure Boards for project management and planning</li> <li>Integrating with other project management tools</li> </ul> </li> <li>Continuous Integration and Delivery with Azure DevOps<ul> <li>Overview of Azure DevOps CI/CD options</li> <li>Creating and managing build pipelines</li> <li>Configuring automated deployments</li> <li>Implementing continuous testing</li> <li>Azure Artifacts</li> <li>learning YAML Schema</li> </ul> </li> <li>Testing in Azure DevOps<ul> <li>Overview of Azure DevOps testing options</li> <li>Using Azure Test Plans for manual and exploratory testing</li> <li>Settingup continuous testing with Azure Pipelines</li> <li>Integrating with other testing tools</li> </ul> </li> <li>Best Practices for Azure DevOps Deployments<ul> <li>Build, Test, &amp; deploy .NET Core apps to Azure</li> <li>Build, Test, &amp; deploy React JS apps to Azure</li> <li>Build, Test, &amp; deploy Blazor apps to Azure</li> <li>Build &amp; deploy Azure SQL database using [DACPAC]</li> <li>Validate and deploy ARM Templates</li> <li>Git Clone, Code review with Pull request</li> <li>Private Agent configuration, Approvals, Artifact</li> <li>Release variables, Replace Tokens</li> <li>Security setup in Azure DevOps</li> </ul> </li> </ol>"},{"location":"techstack/azure-devops/#books","title":"Books","text":""},{"location":"techstack/azure-devops/#references","title":"References","text":""},{"location":"techstack/blazor/","title":"Blazor","text":"<p>If you are new to Blazor and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become Blazor developer:</p>"},{"location":"techstack/blazor/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Blazor<ul> <li>What is Blazor</li> <li>Benefits of using Blazor</li> </ul> </li> <li>Getting started with Blazor<ul> <li>Setting up a Blazor development environment</li> <li>Creating a Blazor application</li> <li>Understanding the structure of a Blazor application</li> </ul> </li> <li>Building user interfaces with Blazor<ul> <li>Defining and rendering components</li> <li>Working with JSX</li> <li>Using Blazor templates</li> </ul> </li> <li>Managing application state with Blazor<ul> <li>Introduction to component state and lifecycle</li> <li>Using component parameters and events</li> <li>Sharing state between components</li> </ul> </li> <li>Data access in Blazor<ul> <li>Introduction to Entity Framework and LINQ</li> <li>Querying and updating data with Entity Framework Core</li> <li>Using dependency injection to manage data access</li> </ul> </li> <li>Routing in Blazor applications<ul> <li>Introduction to Blazor routing</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"techstack/blazor/#books","title":"Books","text":"<p>Here are some recommended books for learning about Blazor, Microsoft's web framework:</p> <ol> <li>Blazor: A Beginner's Guide</li> <li>Programming Blazor: Building Web Applications in .NET</li> <li>Blazor Revealed: Building Web Applications in .NET</li> <li>Mastering Blazor: Tips and Tricks for Building Web Applications</li> </ol>"},{"location":"techstack/blazor/#references","title":"References","text":"<ul> <li>Blazor: Getting Started - MSDN</li> <li>Introduction to ASP.NET Core Blazor - Blazor Overview MSDN</li> <li>ASP.NET Core Blazor tutorials - MSDN</li> <li>Blazor: Getting Started - Pluralsight Course</li> <li>Blazor tutorial</li> <li>Blazor Server vs. Blazor WebAssembly - Article</li> </ul>"},{"location":"techstack/c-sharp/","title":"C# Programming language","text":"<p>If you are new to C# and want to learn and become a C# developer, here is a list of topics you need to know to become <code>C# programming developer</code>:</p>"},{"location":"techstack/c-sharp/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to C#:<ul> <li>What is C#</li> <li>Why use C#</li> <li>History of C#</li> </ul> </li> <li>Getting Started with C#:<ul> <li>Setting up a development environment</li> <li>Creating your first C# program</li> <li>Understanding the structure of a C# projects</li> </ul> </li> <li>Variables and Data Types:<ul> <li>Understanding variables</li> <li>Data types in C#</li> <li>Declaring and initializing variables</li> <li>Converting between data types</li> </ul> </li> <li>Control Structures:<ul> <li>Conditional statements (if/else)</li> <li>Loops (for, while, do while)</li> <li>Switch statements</li> </ul> </li> <li>Functions:<ul> <li>Understanding functions</li> <li>Creating and calling functions</li> <li>Return values and parameters</li> </ul> </li> <li>Object-Oriented Programming in C#:<ul> <li>Understanding objects and classes</li> <li>Creating and using objects</li> <li>Inheritance and polymorphism</li> <li>Encapsulation and access modifiers</li> </ul> </li> <li>Arrays and Collections:<ul> <li>Understanding arrays</li> <li>Declaring and using arrays</li> <li>Understanding collections</li> <li>Using lists and dictionaries</li> </ul> </li> <li>Debugging and Exception Handling:<ul> <li>Debugging techniques in Visual Studio</li> <li>Understanding exceptions and errors</li> <li>Using try/catch blocks to handle exceptions</li> </ul> </li> <li>Advanced Topics:<ul> <li>File Input/Output</li> <li>LINQ</li> <li>Delegates and events</li> <li>Asynchronous programming</li> </ul> </li> <li>Building Applications with C#:<ul> <li>Windows Forms applications</li> <li>ASP.NET web applications</li> <li>Mobile applications with Xamarin</li> <li>Games with Unity</li> </ul> </li> </ol>"},{"location":"techstack/c-sharp/#books","title":"Books","text":"<p>Here are some recommended books for learning C#:</p> <ol> <li>C# 9.0 in a Nutshell: The Definitive Reference</li> <li>Pro C# 9 with .NET 5</li> <li>Head First C#</li> <li>C# in Depth</li> <li>The C# Player's Guide</li> <li>Effective C#: 50 Specific Ways to Improve Your C#</li> <li>CLR via C#</li> <li>Beginning C# 7 Programming with Visual Studio 2017</li> </ol>"},{"location":"techstack/c-sharp/#references","title":"References","text":""},{"location":"techstack/helm/","title":"Helm","text":"<p>If you are new to Helm and want to learn and become a <code>DevSecOps Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/helm/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Helm<ul> <li>What is Helm?</li> <li>Benefits of using Helm</li> </ul> </li> <li>Getting started with Helm<ul> <li>Installing Helm</li> <li>Initializing Helm </li> </ul> </li> <li>Working with Helm charts<ul> <li>Finding and downloading charts from the Helm chart repository</li> <li>Creating and deploying your own charts</li> <li>Upgrading and rolling back chart deployments</li> </ul> </li> <li>Managing dependencies with Helm<ul> <li>Using Helm to manage the dependencies of a chart</li> <li>Sharing charts and chart dependencies with the Helm chart repository</li> </ul> </li> <li>Collaborating with Helm<ul> <li>Using Helm with version control systems</li> <li>Collaborating with team members using Helm</li> </ul> </li> <li>Advanced Helm features<ul> <li>Using Helm with continuous delivery pipelines</li> <li>Customizing Helm behavior with hooks</li> <li>Extending Helm with plugins</li> </ul> </li> </ol>"},{"location":"techstack/helm/#books","title":"Books","text":""},{"location":"techstack/helm/#references","title":"References","text":""},{"location":"techstack/kubernetes/","title":"Kubernetes","text":"<p>If you are new to Kubernetes and want to learn and become a <code>Certified Kubernetes Administrator (CKA)</code>, here is a list of topics you need to know to build and deploy application in Kubernetes. </p>"},{"location":"techstack/kubernetes/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction to AKS<ul> <li>What is AKS</li> <li>Key features and benefits</li> </ul> </li> <li>Kubernetes Architecture Components<ul> <li>Kubernetes \u2500 Master Machine Components</li> <li>Kubernetes \u2500 Node Components</li> </ul> </li> <li>Creating AKS cluster<ul> <li>Creating an AKS cluster using the Azure portal</li> <li>Creating an AKS cluster using Terraform</li> <li>Testing cluster connection &amp; creating namespace</li> <li>Configuring node pools and scaling</li> </ul> </li> <li>Deploying applications to AKS<ul> <li>Creating a container image</li> <li>Pushing the image to a container registry</li> <li>Deploying the application to AKS</li> <li>Managing and scaling an AKS cluster</li> </ul> </li> <li>Networking in AKS<ul> <li>Overview of AKS networking options</li> <li>Managing virtual networks and subnets</li> <li>Load balancing and traffic management</li> </ul> </li> <li>Monitoring and Management in AKS<ul> <li>Upgrading AKS clusters</li> <li>Scaling the number of nodes in an AKS cluster</li> <li>Monitoring and logging AKS clusters</li> </ul> </li> <li>Deploying k8s ingress controller</li> <li>Adding TLS/SSL to the ingress</li> <li>K8s horizontal pod autoscaler [HPA]<ul> <li>K8s horizontal pod autoscaler [HPA]</li> <li>HPA in action</li> <li>AKS cluster autoscaling</li> </ul> </li> <li>Integrating AKS with Azure Monitor</li> <li>AKS Storage and Networks</li> <li>AKS storage overview<ul> <li>Creating storage classes</li> <li>Storage: Persistent claims</li> <li>Shared volumes</li> <li>Create resource for shared volume</li> <li>Challenge: Lost volumes</li> <li>Solution: Find and remove PVs</li> <li>Networking and AKS</li> <li>Load balancing and Ingress: Setup</li> </ul> </li> </ul>"},{"location":"techstack/kubernetes/#books","title":"Books","text":""},{"location":"techstack/kubernetes/#references","title":"References","text":""},{"location":"techstack/net-core-web-api/","title":".NET Core Web API","text":"<p>If you are new to Restful services and want to learn and become a <code>API developer</code>, here is a list of topics you need to know to become API developer:</p>"},{"location":"techstack/net-core-web-api/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to .NET Core Web API:<ul> <li>What is a .NET Core Web API</li> <li>Why use .NET Core for building web APIs</li> <li>The benefits of using .NET Core</li> </ul> </li> <li>Setting up the Development Environment:<ul> <li>Installing the .NET Core SDK</li> <li>Installing Visual Studio Code or Visual Studio</li> <li>Setting up the project structure</li> </ul> </li> <li>Creating Your First Web API:<ul> <li>Creating a new project</li> <li>Defining the API endpoints</li> <li>Implementing the API logic</li> <li>Testing the API</li> </ul> </li> <li>Routing and Controller:<ul> <li>Understanding routing in .NET Core Web API</li> <li>Creating controllers and defining endpoints</li> <li>Implementing HTTP verbs (GET, POST, PUT, DELETE)</li> <li>Handling request and response data</li> </ul> </li> <li>Data Access with Entity Framework Core:<ul> <li>Understanding Entity Framework Core</li> <li>Setting up the database and the context</li> <li>Defining entities and relationships</li> <li>Querying data with LINQ</li> </ul> </li> <li>Authentication and Authorization:<ul> <li>Understanding authentication and authorization</li> <li>Implementing authentication with JWT</li> <li>Implementing authorization with policies</li> </ul> </li> <li>Deployment and Hosting:<ul> <li>Deploying the API to Azure or another cloud provider</li> <li>Hosting the API on IIS or a reverse proxy server</li> <li>Securing the API with SSL</li> </ul> </li> <li>Advanced Topics:<ul> <li>Versioning the API</li> <li>Documentation with Swagger</li> <li>Logging and exception handling</li> <li>Performance optimization and caching</li> </ul> </li> </ol>"},{"location":"techstack/net-core-web-api/#books","title":"Books","text":"<p>Here are some recommended books for learning about .NET Core Web API:</p> <ol> <li>Pro ASP.NET Core 3</li> <li>ASP.NET Core in Action</li> <li>ASP.NET Core 5 and Angular</li> <li>Building RESTful Web APIs with ASP.NET Core</li> <li>Pro ASP.NET Core MVC 2</li> <li>ASP.NET Core Web API</li> </ol>"},{"location":"techstack/net-core-web-api/#references","title":"References","text":"<ul> <li>Download .NET SDKs for Visual Studio - .NET / .NET Core Runtimes</li> <li>ASP.NET Core fundamentals - Fundamentals</li> <li>Dependency injection in ASP.NET Core - Dependency injection</li> <li>Routing in ASP.NET Core - Routing</li> <li>Azure Key Vault configuration provider in ASP.NET Core - Azure Key Vault</li> <li>Serverless Microservices reference architecture - GitHub Link</li> <li>RESTful web API design - RESTful API</li> <li>Web API implementation - RESTful API</li> <li>Build RESTful APIs with ASP.NET Web API - RESTful API</li> <li>Tutorial: Create a web API with ASP.NET Core - MSDN</li> <li>Customizing Swagger Responses for Better API Documentation - medium</li> <li>REST API Tutorial - REST API</li> <li>Best practices for REST API design - Best practices</li> <li>RESTful API Standard (YARAS) - Introduction- Best Practices</li> <li>10 Best Practices for Better RESTful API - Best Practices</li> <li>RESTful API Design: 13 Best Practices - Best Practices</li> <li>FluentValidation in ASP.NET Core - FluentValidation</li> <li>Using Fluent Validation in ASP.NET Core - FluentValidation</li> <li>FluentValidation - FluentValidation</li> <li>Fluent Validation in an ASP.NET Core Web API - FluentValidation</li> <li>Automatically set appsettings.json for dev and release environments in asp.net core? - appsettings</li> <li>Setting up Swagger to support versioned API endpoints in ASP.NET Core - API Versioning</li> <li>NLog - Handling errors</li> <li>Handling errors in an ASP.NET Core Web API- Handling errors</li> <li>Handle errors in ASP.NET Core- Handling errors</li> <li>Serializing enums as strings using System.Text.Json library in .NET Core 3.0 - Serializing enums</li> <li>Adding Newtonsoft JSON serialization and deserialization in ASP.NET Core - Newtonsoft.Json</li> <li>Using Newtonsoft.Json In .NET Core 3+ Projects - Newtonsoft.Json</li> <li>ASP.NET MVC Core API Serialize Enums to String - Serialize</li> <li>Add an authorization header to your swagger-ui with Swashbuckle - Authorization</li> <li>JWT Authentication In ASP.NET Core - Authorization</li> <li>Token Authentication in ASP.NET Core 2.0 - A Complete Guide - Authorization</li> <li>ASP.NET Core 3.1 - Role Based Authorization Tutorial with Example API - Authorization</li> <li>Learn Entity Framework Core</li> <li>Introducing .NET 5</li> <li>Logging to Azure Blob Storage</li> </ul>"},{"location":"techstack/node-js/","title":"Node JS","text":"<p>If you are new to Node JS and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become Node JS developer:</p>"},{"location":"techstack/node-js/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Node.js<ul> <li>What is Node.js</li> <li>Why use Node.js</li> <li>History of Node.js</li> <li>Key features of Node.js</li> </ul> </li> <li>Setting up a Development Environment<ul> <li>Installing Node.js</li> <li>Setting up a development environment</li> <li>Understanding npm (Node Package Manager)</li> </ul> </li> <li>Understanding Node.js Fundamentals<ul> <li>Understanding event-driven programming</li> <li>Understanding non-blocking I/O</li> <li>Understanding modules and require</li> <li>Understanding the Node.js runtime environment</li> </ul> </li> <li>Building a Simple Web Server with Node.js<ul> <li>Introduction to Express</li> <li>Setting up a basic Express server</li> <li>Handling HTTP requests and responses</li> <li>Serving static files</li> </ul> </li> <li>Working with Data in Node.js<ul> <li>Understanding the basics of databases</li> <li>Connecting to a database in Node.js</li> <li>Querying data in Node.js</li> <li>Storing data in Node.js</li> </ul> </li> <li>Building Advanced Applications with Node.js<ul> <li>Understanding middleware</li> <li>Creating a REST API</li> <li>Authenticating and securing applications</li> <li>Using WebSockets for real-time communication</li> <li>Deploying Node.js applications</li> </ul> </li> </ol>"},{"location":"techstack/node-js/#books","title":"Books","text":"<p>Here's a list of recommended books for Node.js:</p> <ol> <li>Node.js Design Patterns</li> <li>Node.js in Action</li> <li>Learning Node: Moving to the Server-Side</li> <li>Node.js 8 the Right Way: Practical, Server-Side JavaScript That Scales</li> <li>Professional Node.js: Building Javascript Based Scalable Software</li> </ol>"},{"location":"techstack/node-js/#references","title":"References","text":"<p>Here are some online resources and tutorials to learn Node.js:</p> <ol> <li> <p>Official Node.js Documentation: Node.js Documentation</p> </li> <li> <p>Node.js Beginner's Guide: Node.js Beginner's Guide</p> </li> <li> <p>Node.js Tutorials on W3Schools: W3Schools Node.js Tutorial</p> </li> <li> <p>Node.js Courses on Udemy:</p> </li> <li>Node.js - The Complete Guide (MVC, REST APIs, GraphQL, Deno)</li> <li>Node.js, Express, MongoDB &amp; More: The Complete Bootcamp 2021</li> </ol>"},{"location":"techstack/react-js/","title":"React JS","text":"<p>If you are new to React JS and want to learn and become a <code>FrontEnd (FE) Developer</code>, here is a list of topics you need to know to become React JS developer:</p>"},{"location":"techstack/react-js/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to React<ul> <li>What is React</li> <li>Why use React</li> </ul> </li> <li>Getting started with React<ul> <li>Setting up a React development environment</li> <li>Creating a React application</li> <li>Understanding the structure of a React application</li> </ul> </li> <li>Working with React components<ul> <li>Defining and rendering components</li> <li>Props and state in components</li> <li>Lifecycle methods in components</li> </ul> </li> <li>Building user interfaces with React<ul> <li>Working with JSX</li> <li>Using React hooks</li> <li>Managing data with context</li> </ul> </li> <li>Managing application state with Redux<ul> <li>Introduction to Redux</li> <li>Setting up Redux in a React application</li> <li>Working with actions, reducers, and the store</li> </ul> </li> <li>Routing in React applications<ul> <li>Introduction to React Router</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"techstack/react-js/#books","title":"Books","text":"<p>Here are some recommended books for learning React JS:</p> <ol> <li>Learning React: A Hands-On Guide to Building Web Applications Using React and Redux</li> <li>React - Up &amp; Running: Building Web Applications</li> <li>Fullstack React: The Complete Guide to ReactJS and Friends</li> <li>The Road to React: Your Journey to Master Plain Yet Pragmatic React.js</li> <li>React Design Patterns and Best Practices</li> </ol>"},{"location":"techstack/react-js/#references","title":"References","text":"<ul> <li>React JS - Official website</li> <li>React JS Tutorials - Official website</li> <li>Getting Started - Official website</li> <li>Create React App</li> <li>Reactjs code snippets - code snippets</li> <li>TypeScript</li> <li>Webpack</li> <li>Getting started with React Router - Router</li> <li>REACT TRAINING / REACT ROUTER - Router</li> <li>Redux Fundamentals - Redux</li> <li>Build and deploy a Node.js API - Node.js API</li> <li>Getting Started with Redux- Redux</li> <li>How to fetch data from Api in Redux using Redux -thunk- Redux-thunk</li> <li>Best react courses - 7 Best Online React courses</li> <li>Learn ReactJS and build a simple CRUD app- React Tutorial</li> <li>React-redux-registration-login-example -registration login</li> <li>Using NSwag to Generate React Client for an ASP.NET Core 3 API- NSwag</li> <li>Build and deploy a Node.js API app in Azure App Service</li> </ul>"},{"location":"techstack/tech-stack/","title":"Tech stack","text":"<p>Here is a list of technologies that I frequently use and support, here you will also see the list of topics from each technology so that you can learn them offline when needed.</p> <p>You may encounter some specific tools and technologies that are not listed here but you will see the references for further learning.</p>"},{"location":"techstack/tech-stack/#technologies","title":"Technologies","text":"<ul> <li>C# Programming language</li> <li>.NET Core Web API</li> <li>React JS</li> <li>Blazor</li> <li>Node.js</li> <li>Azure DevOps</li> <li>Azure Cloud</li> <li>Kubernetes Cluster</li> <li>Terraform </li> <li>Argocd</li> <li>Helm Chart</li> <li>Application Testing</li> <li>Networking troubleshooting Tools</li> </ul>"},{"location":"techstack/tech-stack/#development-tools","title":"Development Tools","text":"<p>Here is the list of tools that are commonly used in the development of software applications. </p> <ul> <li>Visual Studio Code</li> <li>SQL Server Management Studio, SQL Profiler</li> <li>Node JS, NPM, Notepad++</li> <li>Postman, SOAP UI</li> <li>Browser Developer Tools</li> <li>Agile &amp; Scrum with JIRA or Azure Board</li> <li>Nuget Manager, GitHub Desktop</li> <li>Open SSL</li> <li>JSON Viewer/Formatter, JWT debugger, SAML-Tracer</li> <li>Azure Storage Explorer</li> <li>Visual Studio 2022/2019 [optional]</li> </ul>"},{"location":"techstack/terraform/","title":"Terraform","text":"<p>If you are new to Terraform and want to learn and become a <code>Infrastructure as Code (IaC) Cloud Engineer</code>, here is a list of topics you need to know to create infrastructure in Cloud. </p>"},{"location":"techstack/terraform/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction<ul> <li>What is Terraform?</li> <li>Why use Terraform?</li> <li>Key concepts</li> </ul> </li> <li>Setting up Terraform<ul> <li>Installing Terraform</li> <li>Configuring Terraform</li> <li>Creating your first Terraform configuration</li> </ul> </li> <li>The Terraform Configuration Language (HashiCorp Configuration Language, HCL)<ul> <li>Syntax and structure</li> <li>Variables</li> <li>Outputs</li> <li>Modules</li> </ul> </li> <li>Providers<ul> <li>Overview of available providers</li> <li>Managing multiple providers</li> <li>Using provider-specific resources</li> </ul> </li> <li>Resource Types<ul> <li>Overview of supported resource types</li> <li>Creating, updating, and destroying resources</li> <li>Importing existing resources into Terraform</li> </ul> </li> <li>Modules<ul> <li>Creating and using modules</li> <li>Sharing modules with others</li> </ul> </li> <li>Workspaces<ul> <li>Managing multiple environments with workspaces</li> <li>Workspace management commands</li> </ul> </li> <li>State Management<ul> <li>Understanding Terraform state</li> <li>Backing up and sharing state files</li> </ul> </li> <li>Advanced Terraform Features<ul> <li>Input and output variables</li> <li>Loops and conditions</li> <li>Data sources</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable configurations</li> <li>Managing state effectively</li> <li>Collaborating with others</li> </ul> </li> <li>Troubleshooting<ul> <li>Common issues and errors</li> <li>Debugging techniques</li> <li>Tips for avoiding common mistakes</li> </ul> </li> </ol>"},{"location":"techstack/terraform/#books","title":"Books","text":""},{"location":"techstack/terraform/#references","title":"References","text":""},{"location":"techstack/testing/","title":"QA Automation","text":"<p>If you are new to QA Automation and want to learn and become a <code>QA Automation Engineer</code>, here is a list of topics you need to know: </p>"},{"location":"techstack/testing/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Unit and Integration Testing<ul> <li>Overview of testing in software development</li> <li>Importance of unit and integration testing</li> <li>Understanding the differences between unit and integration testing</li> </ul> </li> <li>Setting up a Testing Environment<ul> <li>Understand testing tools and frameworks</li> <li>Installing and configuring testing tools</li> <li>Creating a testing project </li> <li>setting up testing frameworks</li> </ul> </li> <li>Unit Testing <ul> <li>Understand unit testing technologies</li> <li>Writing unit tests using MSTest, NUnit, and XUnit, Moq &amp; AutoFixure.</li> <li>Debugging and troubleshooting unit tests</li> <li>Managing and organizing unit tests</li> </ul> </li> <li>Integration Testing<ul> <li>Understanding integration testing technologies</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting integration tests</li> <li>Integrating with other testing tools and frameworks</li> <li>Create BDDs with SpectFlow</li> </ul> </li> <li>Functional Testing<ul> <li>Understanding Functional testing technologies</li> <li>Writing Functional tests</li> <li>Debugging and troubleshooting Functional tests</li> </ul> </li> <li>Testing ASP.NET Core Web API <ul> <li>Writing unit tests</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting tests</li> </ul> </li> <li>Testing React JS application<ul> <li>Writing UI test with Selenium</li> <li>Use Jest for react js test cases</li> </ul> </li> <li>Automating Testing with Continuous Integration/Continuous Deployment (CI/CD)<ul> <li>Setting up a CI/CD pipeline</li> <li>Automating Unit tests with CI/CD</li> <li>Automating Integration tests with CI/CD</li> <li>Automating Functional tests with CI/CD</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable tests</li> <li>Implementing code coverage and test coverage analysis</li> <li>Managing testing data and test environment</li> <li>Implementing testing as part of the development process</li> <li>Review test results</li> <li>Test Analytics</li> <li>Review &amp; Report code coverage results</li> <li>API Automation testing using SOAP UI</li> <li>Create Regression test suite</li> <li>Create Smoke / Sanity test suite</li> <li>Performance testing- to ensure that it can handle a high load of users and data.</li> </ul> </li> </ol>"},{"location":"techstack/testing/#books","title":"Books","text":""},{"location":"techstack/testing/#references","title":"References","text":""}]}